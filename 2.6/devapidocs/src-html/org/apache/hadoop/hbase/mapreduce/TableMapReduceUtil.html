<!DOCTYPE HTML>
<html lang="en">
<head>
<!-- Generated by javadoc (17) -->
<title>Source code</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="source: package: org.apache.hadoop.hbase.mapreduce, class: TableMapReduceUtil">
<meta name="generator" content="javadoc/SourceToHTMLConverter">
<link rel="stylesheet" type="text/css" href="../../../../../../stylesheet.css" title="Style">
</head>
<body class="source-page">
<main role="main">
<div class="source-container">
<pre><span class="source-line-no">001</span><span id="line-1">/*</span>
<span class="source-line-no">002</span><span id="line-2"> * Licensed to the Apache Software Foundation (ASF) under one</span>
<span class="source-line-no">003</span><span id="line-3"> * or more contributor license agreements.  See the NOTICE file</span>
<span class="source-line-no">004</span><span id="line-4"> * distributed with this work for additional information</span>
<span class="source-line-no">005</span><span id="line-5"> * regarding copyright ownership.  The ASF licenses this file</span>
<span class="source-line-no">006</span><span id="line-6"> * to you under the Apache License, Version 2.0 (the</span>
<span class="source-line-no">007</span><span id="line-7"> * "License"); you may not use this file except in compliance</span>
<span class="source-line-no">008</span><span id="line-8"> * with the License.  You may obtain a copy of the License at</span>
<span class="source-line-no">009</span><span id="line-9"> *</span>
<span class="source-line-no">010</span><span id="line-10"> *     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="source-line-no">011</span><span id="line-11"> *</span>
<span class="source-line-no">012</span><span id="line-12"> * Unless required by applicable law or agreed to in writing, software</span>
<span class="source-line-no">013</span><span id="line-13"> * distributed under the License is distributed on an "AS IS" BASIS,</span>
<span class="source-line-no">014</span><span id="line-14"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="source-line-no">015</span><span id="line-15"> * See the License for the specific language governing permissions and</span>
<span class="source-line-no">016</span><span id="line-16"> * limitations under the License.</span>
<span class="source-line-no">017</span><span id="line-17"> */</span>
<span class="source-line-no">018</span><span id="line-18">package org.apache.hadoop.hbase.mapreduce;</span>
<span class="source-line-no">019</span><span id="line-19"></span>
<span class="source-line-no">020</span><span id="line-20">import com.codahale.metrics.MetricRegistry;</span>
<span class="source-line-no">021</span><span id="line-21">import java.io.File;</span>
<span class="source-line-no">022</span><span id="line-22">import java.io.IOException;</span>
<span class="source-line-no">023</span><span id="line-23">import java.net.URL;</span>
<span class="source-line-no">024</span><span id="line-24">import java.net.URLDecoder;</span>
<span class="source-line-no">025</span><span id="line-25">import java.util.ArrayList;</span>
<span class="source-line-no">026</span><span id="line-26">import java.util.Base64;</span>
<span class="source-line-no">027</span><span id="line-27">import java.util.Collection;</span>
<span class="source-line-no">028</span><span id="line-28">import java.util.Enumeration;</span>
<span class="source-line-no">029</span><span id="line-29">import java.util.HashMap;</span>
<span class="source-line-no">030</span><span id="line-30">import java.util.HashSet;</span>
<span class="source-line-no">031</span><span id="line-31">import java.util.List;</span>
<span class="source-line-no">032</span><span id="line-32">import java.util.Map;</span>
<span class="source-line-no">033</span><span id="line-33">import java.util.Set;</span>
<span class="source-line-no">034</span><span id="line-34">import java.util.zip.ZipEntry;</span>
<span class="source-line-no">035</span><span id="line-35">import java.util.zip.ZipFile;</span>
<span class="source-line-no">036</span><span id="line-36">import org.apache.hadoop.conf.Configuration;</span>
<span class="source-line-no">037</span><span id="line-37">import org.apache.hadoop.fs.FileSystem;</span>
<span class="source-line-no">038</span><span id="line-38">import org.apache.hadoop.fs.Path;</span>
<span class="source-line-no">039</span><span id="line-39">import org.apache.hadoop.hbase.HBaseConfiguration;</span>
<span class="source-line-no">040</span><span id="line-40">import org.apache.hadoop.hbase.HConstants;</span>
<span class="source-line-no">041</span><span id="line-41">import org.apache.hadoop.hbase.TableName;</span>
<span class="source-line-no">042</span><span id="line-42">import org.apache.hadoop.hbase.client.Connection;</span>
<span class="source-line-no">043</span><span id="line-43">import org.apache.hadoop.hbase.client.ConnectionFactory;</span>
<span class="source-line-no">044</span><span id="line-44">import org.apache.hadoop.hbase.client.Put;</span>
<span class="source-line-no">045</span><span id="line-45">import org.apache.hadoop.hbase.client.RegionLocator;</span>
<span class="source-line-no">046</span><span id="line-46">import org.apache.hadoop.hbase.client.Scan;</span>
<span class="source-line-no">047</span><span id="line-47">import org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span>
<span class="source-line-no">048</span><span id="line-48">import org.apache.hadoop.hbase.security.User;</span>
<span class="source-line-no">049</span><span id="line-49">import org.apache.hadoop.hbase.security.UserProvider;</span>
<span class="source-line-no">050</span><span id="line-50">import org.apache.hadoop.hbase.security.token.TokenUtil;</span>
<span class="source-line-no">051</span><span id="line-51">import org.apache.hadoop.hbase.util.Bytes;</span>
<span class="source-line-no">052</span><span id="line-52">import org.apache.hadoop.hbase.util.RegionSplitter;</span>
<span class="source-line-no">053</span><span id="line-53">import org.apache.hadoop.hbase.zookeeper.ZKConfig;</span>
<span class="source-line-no">054</span><span id="line-54">import org.apache.hadoop.io.Writable;</span>
<span class="source-line-no">055</span><span id="line-55">import org.apache.hadoop.mapreduce.InputFormat;</span>
<span class="source-line-no">056</span><span id="line-56">import org.apache.hadoop.mapreduce.Job;</span>
<span class="source-line-no">057</span><span id="line-57">import org.apache.hadoop.util.StringUtils;</span>
<span class="source-line-no">058</span><span id="line-58">import org.apache.yetus.audience.InterfaceAudience;</span>
<span class="source-line-no">059</span><span id="line-59">import org.slf4j.Logger;</span>
<span class="source-line-no">060</span><span id="line-60">import org.slf4j.LoggerFactory;</span>
<span class="source-line-no">061</span><span id="line-61"></span>
<span class="source-line-no">062</span><span id="line-62">import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;</span>
<span class="source-line-no">063</span><span id="line-63">import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos;</span>
<span class="source-line-no">064</span><span id="line-64"></span>
<span class="source-line-no">065</span><span id="line-65">/**</span>
<span class="source-line-no">066</span><span id="line-66"> * Utility for {@link TableMapper} and {@link TableReducer}</span>
<span class="source-line-no">067</span><span id="line-67"> */</span>
<span class="source-line-no">068</span><span id="line-68">@SuppressWarnings({ "rawtypes", "unchecked" })</span>
<span class="source-line-no">069</span><span id="line-69">@InterfaceAudience.Public</span>
<span class="source-line-no">070</span><span id="line-70">public class TableMapReduceUtil {</span>
<span class="source-line-no">071</span><span id="line-71">  private static final Logger LOG = LoggerFactory.getLogger(TableMapReduceUtil.class);</span>
<span class="source-line-no">072</span><span id="line-72">  public static final String TABLE_INPUT_CLASS_KEY = "hbase.table.input.class";</span>
<span class="source-line-no">073</span><span id="line-73"></span>
<span class="source-line-no">074</span><span id="line-74">  /**</span>
<span class="source-line-no">075</span><span id="line-75">   * Use this before submitting a TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">076</span><span id="line-76">   * @param table            The table name to read from.</span>
<span class="source-line-no">077</span><span id="line-77">   * @param scan             The scan instance with the columns, time range etc.</span>
<span class="source-line-no">078</span><span id="line-78">   * @param mapper           The mapper class to use.</span>
<span class="source-line-no">079</span><span id="line-79">   * @param outputKeyClass   The class of the output key.</span>
<span class="source-line-no">080</span><span id="line-80">   * @param outputValueClass The class of the output value.</span>
<span class="source-line-no">081</span><span id="line-81">   * @param job              The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">082</span><span id="line-82">   *                         necessary HBase configuration.</span>
<span class="source-line-no">083</span><span id="line-83">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">084</span><span id="line-84">   */</span>
<span class="source-line-no">085</span><span id="line-85">  public static void initTableMapperJob(String table, Scan scan,</span>
<span class="source-line-no">086</span><span id="line-86">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">087</span><span id="line-87">    Job job) throws IOException {</span>
<span class="source-line-no">088</span><span id="line-88">    initTableMapperJob(table, scan, mapper, outputKeyClass, outputValueClass, job, true);</span>
<span class="source-line-no">089</span><span id="line-89">  }</span>
<span class="source-line-no">090</span><span id="line-90"></span>
<span class="source-line-no">091</span><span id="line-91">  /**</span>
<span class="source-line-no">092</span><span id="line-92">   * Use this before submitting a TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">093</span><span id="line-93">   * @param table            The table name to read from.</span>
<span class="source-line-no">094</span><span id="line-94">   * @param scan             The scan instance with the columns, time range etc.</span>
<span class="source-line-no">095</span><span id="line-95">   * @param mapper           The mapper class to use.</span>
<span class="source-line-no">096</span><span id="line-96">   * @param outputKeyClass   The class of the output key.</span>
<span class="source-line-no">097</span><span id="line-97">   * @param outputValueClass The class of the output value.</span>
<span class="source-line-no">098</span><span id="line-98">   * @param job              The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">099</span><span id="line-99">   *                         necessary HBase configuration.</span>
<span class="source-line-no">100</span><span id="line-100">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">101</span><span id="line-101">   */</span>
<span class="source-line-no">102</span><span id="line-102">  public static void initTableMapperJob(TableName table, Scan scan,</span>
<span class="source-line-no">103</span><span id="line-103">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">104</span><span id="line-104">    Job job) throws IOException {</span>
<span class="source-line-no">105</span><span id="line-105">    initTableMapperJob(table.getNameAsString(), scan, mapper, outputKeyClass, outputValueClass, job,</span>
<span class="source-line-no">106</span><span id="line-106">      true);</span>
<span class="source-line-no">107</span><span id="line-107">  }</span>
<span class="source-line-no">108</span><span id="line-108"></span>
<span class="source-line-no">109</span><span id="line-109">  /**</span>
<span class="source-line-no">110</span><span id="line-110">   * Use this before submitting a TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">111</span><span id="line-111">   * @param table            Binary representation of the table name to read from.</span>
<span class="source-line-no">112</span><span id="line-112">   * @param scan             The scan instance with the columns, time range etc.</span>
<span class="source-line-no">113</span><span id="line-113">   * @param mapper           The mapper class to use.</span>
<span class="source-line-no">114</span><span id="line-114">   * @param outputKeyClass   The class of the output key.</span>
<span class="source-line-no">115</span><span id="line-115">   * @param outputValueClass The class of the output value.</span>
<span class="source-line-no">116</span><span id="line-116">   * @param job              The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">117</span><span id="line-117">   *                         necessary HBase configuration.</span>
<span class="source-line-no">118</span><span id="line-118">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">119</span><span id="line-119">   */</span>
<span class="source-line-no">120</span><span id="line-120">  public static void initTableMapperJob(byte[] table, Scan scan,</span>
<span class="source-line-no">121</span><span id="line-121">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">122</span><span id="line-122">    Job job) throws IOException {</span>
<span class="source-line-no">123</span><span id="line-123">    initTableMapperJob(Bytes.toString(table), scan, mapper, outputKeyClass, outputValueClass, job,</span>
<span class="source-line-no">124</span><span id="line-124">      true);</span>
<span class="source-line-no">125</span><span id="line-125">  }</span>
<span class="source-line-no">126</span><span id="line-126"></span>
<span class="source-line-no">127</span><span id="line-127">  /**</span>
<span class="source-line-no">128</span><span id="line-128">   * Use this before submitting a TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">129</span><span id="line-129">   * @param table             The table name to read from.</span>
<span class="source-line-no">130</span><span id="line-130">   * @param scan              The scan instance with the columns, time range etc.</span>
<span class="source-line-no">131</span><span id="line-131">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">132</span><span id="line-132">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">133</span><span id="line-133">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">134</span><span id="line-134">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">135</span><span id="line-135">   *                          necessary HBase configuration.</span>
<span class="source-line-no">136</span><span id="line-136">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">137</span><span id="line-137">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">138</span><span id="line-138">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">139</span><span id="line-139">   */</span>
<span class="source-line-no">140</span><span id="line-140">  public static void initTableMapperJob(String table, Scan scan,</span>
<span class="source-line-no">141</span><span id="line-141">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">142</span><span id="line-142">    Job job, boolean addDependencyJars, Class&lt;? extends InputFormat&gt; inputFormatClass)</span>
<span class="source-line-no">143</span><span id="line-143">    throws IOException {</span>
<span class="source-line-no">144</span><span id="line-144">    initTableMapperJob(table, scan, mapper, outputKeyClass, outputValueClass, job,</span>
<span class="source-line-no">145</span><span id="line-145">      addDependencyJars, true, inputFormatClass);</span>
<span class="source-line-no">146</span><span id="line-146">  }</span>
<span class="source-line-no">147</span><span id="line-147"></span>
<span class="source-line-no">148</span><span id="line-148">  /**</span>
<span class="source-line-no">149</span><span id="line-149">   * Use this before submitting a TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">150</span><span id="line-150">   * @param table             The table name to read from.</span>
<span class="source-line-no">151</span><span id="line-151">   * @param scan              The scan instance with the columns, time range etc.</span>
<span class="source-line-no">152</span><span id="line-152">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">153</span><span id="line-153">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">154</span><span id="line-154">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">155</span><span id="line-155">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">156</span><span id="line-156">   *                          necessary HBase configuration.</span>
<span class="source-line-no">157</span><span id="line-157">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">158</span><span id="line-158">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">159</span><span id="line-159">   * @param initCredentials   whether to initialize hbase auth credentials for the job</span>
<span class="source-line-no">160</span><span id="line-160">   * @param inputFormatClass  the input format</span>
<span class="source-line-no">161</span><span id="line-161">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">162</span><span id="line-162">   */</span>
<span class="source-line-no">163</span><span id="line-163">  public static void initTableMapperJob(String table, Scan scan,</span>
<span class="source-line-no">164</span><span id="line-164">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">165</span><span id="line-165">    Job job, boolean addDependencyJars, boolean initCredentials,</span>
<span class="source-line-no">166</span><span id="line-166">    Class&lt;? extends InputFormat&gt; inputFormatClass) throws IOException {</span>
<span class="source-line-no">167</span><span id="line-167">    job.setInputFormatClass(inputFormatClass);</span>
<span class="source-line-no">168</span><span id="line-168">    if (outputValueClass != null) job.setMapOutputValueClass(outputValueClass);</span>
<span class="source-line-no">169</span><span id="line-169">    if (outputKeyClass != null) job.setMapOutputKeyClass(outputKeyClass);</span>
<span class="source-line-no">170</span><span id="line-170">    job.setMapperClass(mapper);</span>
<span class="source-line-no">171</span><span id="line-171">    if (Put.class.equals(outputValueClass)) {</span>
<span class="source-line-no">172</span><span id="line-172">      job.setCombinerClass(PutCombiner.class);</span>
<span class="source-line-no">173</span><span id="line-173">    }</span>
<span class="source-line-no">174</span><span id="line-174">    Configuration conf = job.getConfiguration();</span>
<span class="source-line-no">175</span><span id="line-175">    HBaseConfiguration.merge(conf, HBaseConfiguration.create(conf));</span>
<span class="source-line-no">176</span><span id="line-176">    conf.set(TableInputFormat.INPUT_TABLE, table);</span>
<span class="source-line-no">177</span><span id="line-177">    conf.set(TableInputFormat.SCAN, convertScanToString(scan));</span>
<span class="source-line-no">178</span><span id="line-178">    conf.setStrings("io.serializations", conf.get("io.serializations"),</span>
<span class="source-line-no">179</span><span id="line-179">      MutationSerialization.class.getName(), ResultSerialization.class.getName(),</span>
<span class="source-line-no">180</span><span id="line-180">      CellSerialization.class.getName());</span>
<span class="source-line-no">181</span><span id="line-181">    if (addDependencyJars) {</span>
<span class="source-line-no">182</span><span id="line-182">      addDependencyJars(job);</span>
<span class="source-line-no">183</span><span id="line-183">    }</span>
<span class="source-line-no">184</span><span id="line-184">    if (initCredentials) {</span>
<span class="source-line-no">185</span><span id="line-185">      initCredentials(job);</span>
<span class="source-line-no">186</span><span id="line-186">    }</span>
<span class="source-line-no">187</span><span id="line-187">  }</span>
<span class="source-line-no">188</span><span id="line-188"></span>
<span class="source-line-no">189</span><span id="line-189">  /**</span>
<span class="source-line-no">190</span><span id="line-190">   * Use this before submitting a TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">191</span><span id="line-191">   * @param table             Binary representation of the table name to read from.</span>
<span class="source-line-no">192</span><span id="line-192">   * @param scan              The scan instance with the columns, time range etc.</span>
<span class="source-line-no">193</span><span id="line-193">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">194</span><span id="line-194">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">195</span><span id="line-195">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">196</span><span id="line-196">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">197</span><span id="line-197">   *                          necessary HBase configuration.</span>
<span class="source-line-no">198</span><span id="line-198">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">199</span><span id="line-199">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">200</span><span id="line-200">   * @param inputFormatClass  The class of the input format</span>
<span class="source-line-no">201</span><span id="line-201">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">202</span><span id="line-202">   */</span>
<span class="source-line-no">203</span><span id="line-203">  public static void initTableMapperJob(byte[] table, Scan scan,</span>
<span class="source-line-no">204</span><span id="line-204">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">205</span><span id="line-205">    Job job, boolean addDependencyJars, Class&lt;? extends InputFormat&gt; inputFormatClass)</span>
<span class="source-line-no">206</span><span id="line-206">    throws IOException {</span>
<span class="source-line-no">207</span><span id="line-207">    initTableMapperJob(Bytes.toString(table), scan, mapper, outputKeyClass, outputValueClass, job,</span>
<span class="source-line-no">208</span><span id="line-208">      addDependencyJars, inputFormatClass);</span>
<span class="source-line-no">209</span><span id="line-209">  }</span>
<span class="source-line-no">210</span><span id="line-210"></span>
<span class="source-line-no">211</span><span id="line-211">  /**</span>
<span class="source-line-no">212</span><span id="line-212">   * Use this before submitting a TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">213</span><span id="line-213">   * @param table             Binary representation of the table name to read from.</span>
<span class="source-line-no">214</span><span id="line-214">   * @param scan              The scan instance with the columns, time range etc.</span>
<span class="source-line-no">215</span><span id="line-215">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">216</span><span id="line-216">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">217</span><span id="line-217">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">218</span><span id="line-218">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">219</span><span id="line-219">   *                          necessary HBase configuration.</span>
<span class="source-line-no">220</span><span id="line-220">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">221</span><span id="line-221">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">222</span><span id="line-222">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">223</span><span id="line-223">   */</span>
<span class="source-line-no">224</span><span id="line-224">  public static void initTableMapperJob(byte[] table, Scan scan,</span>
<span class="source-line-no">225</span><span id="line-225">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">226</span><span id="line-226">    Job job, boolean addDependencyJars) throws IOException {</span>
<span class="source-line-no">227</span><span id="line-227">    initTableMapperJob(Bytes.toString(table), scan, mapper, outputKeyClass, outputValueClass, job,</span>
<span class="source-line-no">228</span><span id="line-228">      addDependencyJars, getConfiguredInputFormat(job));</span>
<span class="source-line-no">229</span><span id="line-229">  }</span>
<span class="source-line-no">230</span><span id="line-230"></span>
<span class="source-line-no">231</span><span id="line-231">  /**</span>
<span class="source-line-no">232</span><span id="line-232">   * @return {@link TableInputFormat} .class unless Configuration has something else at</span>
<span class="source-line-no">233</span><span id="line-233">   *         {@link #TABLE_INPUT_CLASS_KEY}.</span>
<span class="source-line-no">234</span><span id="line-234">   */</span>
<span class="source-line-no">235</span><span id="line-235">  private static Class&lt;? extends InputFormat&gt; getConfiguredInputFormat(Job job) {</span>
<span class="source-line-no">236</span><span id="line-236">    return (Class&lt;? extends InputFormat&gt;) job.getConfiguration().getClass(TABLE_INPUT_CLASS_KEY,</span>
<span class="source-line-no">237</span><span id="line-237">      TableInputFormat.class);</span>
<span class="source-line-no">238</span><span id="line-238">  }</span>
<span class="source-line-no">239</span><span id="line-239"></span>
<span class="source-line-no">240</span><span id="line-240">  /**</span>
<span class="source-line-no">241</span><span id="line-241">   * Use this before submitting a TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">242</span><span id="line-242">   * @param table             The table name to read from.</span>
<span class="source-line-no">243</span><span id="line-243">   * @param scan              The scan instance with the columns, time range etc.</span>
<span class="source-line-no">244</span><span id="line-244">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">245</span><span id="line-245">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">246</span><span id="line-246">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">247</span><span id="line-247">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">248</span><span id="line-248">   *                          necessary HBase configuration.</span>
<span class="source-line-no">249</span><span id="line-249">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">250</span><span id="line-250">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">251</span><span id="line-251">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">252</span><span id="line-252">   */</span>
<span class="source-line-no">253</span><span id="line-253">  public static void initTableMapperJob(String table, Scan scan,</span>
<span class="source-line-no">254</span><span id="line-254">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">255</span><span id="line-255">    Job job, boolean addDependencyJars) throws IOException {</span>
<span class="source-line-no">256</span><span id="line-256">    initTableMapperJob(table, scan, mapper, outputKeyClass, outputValueClass, job,</span>
<span class="source-line-no">257</span><span id="line-257">      addDependencyJars, getConfiguredInputFormat(job));</span>
<span class="source-line-no">258</span><span id="line-258">  }</span>
<span class="source-line-no">259</span><span id="line-259"></span>
<span class="source-line-no">260</span><span id="line-260">  /**</span>
<span class="source-line-no">261</span><span id="line-261">   * Enable a basic on-heap cache for these jobs. Any BlockCache implementation based on direct</span>
<span class="source-line-no">262</span><span id="line-262">   * memory will likely cause the map tasks to OOM when opening the region. This is done here</span>
<span class="source-line-no">263</span><span id="line-263">   * instead of in TableSnapshotRegionRecordReader in case an advanced user wants to override this</span>
<span class="source-line-no">264</span><span id="line-264">   * behavior in their job.</span>
<span class="source-line-no">265</span><span id="line-265">   */</span>
<span class="source-line-no">266</span><span id="line-266">  public static void resetCacheConfig(Configuration conf) {</span>
<span class="source-line-no">267</span><span id="line-267">    conf.setFloat(HConstants.HFILE_BLOCK_CACHE_SIZE_KEY, HConstants.HFILE_BLOCK_CACHE_SIZE_DEFAULT);</span>
<span class="source-line-no">268</span><span id="line-268">    conf.setFloat(HConstants.BUCKET_CACHE_SIZE_KEY, 0f);</span>
<span class="source-line-no">269</span><span id="line-269">    conf.unset(HConstants.BUCKET_CACHE_IOENGINE_KEY);</span>
<span class="source-line-no">270</span><span id="line-270">  }</span>
<span class="source-line-no">271</span><span id="line-271"></span>
<span class="source-line-no">272</span><span id="line-272">  /**</span>
<span class="source-line-no">273</span><span id="line-273">   * Sets up the job for reading from one or more table snapshots, with one or more scans per</span>
<span class="source-line-no">274</span><span id="line-274">   * snapshot. It bypasses hbase servers and read directly from snapshot files.</span>
<span class="source-line-no">275</span><span id="line-275">   * @param snapshotScans     map of snapshot name to scans on that snapshot.</span>
<span class="source-line-no">276</span><span id="line-276">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">277</span><span id="line-277">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">278</span><span id="line-278">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">279</span><span id="line-279">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">280</span><span id="line-280">   *                          necessary HBase configuration.</span>
<span class="source-line-no">281</span><span id="line-281">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">282</span><span id="line-282">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">283</span><span id="line-283">   */</span>
<span class="source-line-no">284</span><span id="line-284">  public static void initMultiTableSnapshotMapperJob(Map&lt;String, Collection&lt;Scan&gt;&gt; snapshotScans,</span>
<span class="source-line-no">285</span><span id="line-285">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">286</span><span id="line-286">    Job job, boolean addDependencyJars, Path tmpRestoreDir) throws IOException {</span>
<span class="source-line-no">287</span><span id="line-287">    MultiTableSnapshotInputFormat.setInput(job.getConfiguration(), snapshotScans, tmpRestoreDir);</span>
<span class="source-line-no">288</span><span id="line-288"></span>
<span class="source-line-no">289</span><span id="line-289">    job.setInputFormatClass(MultiTableSnapshotInputFormat.class);</span>
<span class="source-line-no">290</span><span id="line-290">    if (outputValueClass != null) {</span>
<span class="source-line-no">291</span><span id="line-291">      job.setMapOutputValueClass(outputValueClass);</span>
<span class="source-line-no">292</span><span id="line-292">    }</span>
<span class="source-line-no">293</span><span id="line-293">    if (outputKeyClass != null) {</span>
<span class="source-line-no">294</span><span id="line-294">      job.setMapOutputKeyClass(outputKeyClass);</span>
<span class="source-line-no">295</span><span id="line-295">    }</span>
<span class="source-line-no">296</span><span id="line-296">    job.setMapperClass(mapper);</span>
<span class="source-line-no">297</span><span id="line-297">    Configuration conf = job.getConfiguration();</span>
<span class="source-line-no">298</span><span id="line-298">    HBaseConfiguration.merge(conf, HBaseConfiguration.create(conf));</span>
<span class="source-line-no">299</span><span id="line-299"></span>
<span class="source-line-no">300</span><span id="line-300">    if (addDependencyJars) {</span>
<span class="source-line-no">301</span><span id="line-301">      addDependencyJars(job);</span>
<span class="source-line-no">302</span><span id="line-302">      addDependencyJarsForClasses(job.getConfiguration(), MetricRegistry.class);</span>
<span class="source-line-no">303</span><span id="line-303">    }</span>
<span class="source-line-no">304</span><span id="line-304"></span>
<span class="source-line-no">305</span><span id="line-305">    resetCacheConfig(job.getConfiguration());</span>
<span class="source-line-no">306</span><span id="line-306">  }</span>
<span class="source-line-no">307</span><span id="line-307"></span>
<span class="source-line-no">308</span><span id="line-308">  /**</span>
<span class="source-line-no">309</span><span id="line-309">   * Sets up the job for reading from a table snapshot. It bypasses hbase servers and read directly</span>
<span class="source-line-no">310</span><span id="line-310">   * from snapshot files.</span>
<span class="source-line-no">311</span><span id="line-311">   * @param snapshotName      The name of the snapshot (of a table) to read from.</span>
<span class="source-line-no">312</span><span id="line-312">   * @param scan              The scan instance with the columns, time range etc.</span>
<span class="source-line-no">313</span><span id="line-313">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">314</span><span id="line-314">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">315</span><span id="line-315">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">316</span><span id="line-316">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">317</span><span id="line-317">   *                          necessary HBase configuration.</span>
<span class="source-line-no">318</span><span id="line-318">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">319</span><span id="line-319">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">320</span><span id="line-320">   * @param tmpRestoreDir     a temporary directory to copy the snapshot files into. Current user</span>
<span class="source-line-no">321</span><span id="line-321">   *                          should have write permissions to this directory, and this should not</span>
<span class="source-line-no">322</span><span id="line-322">   *                          be a subdirectory of rootdir. After the job is finished, restore</span>
<span class="source-line-no">323</span><span id="line-323">   *                          directory can be deleted.</span>
<span class="source-line-no">324</span><span id="line-324">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">325</span><span id="line-325">   * @see TableSnapshotInputFormat</span>
<span class="source-line-no">326</span><span id="line-326">   */</span>
<span class="source-line-no">327</span><span id="line-327">  public static void initTableSnapshotMapperJob(String snapshotName, Scan scan,</span>
<span class="source-line-no">328</span><span id="line-328">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">329</span><span id="line-329">    Job job, boolean addDependencyJars, Path tmpRestoreDir) throws IOException {</span>
<span class="source-line-no">330</span><span id="line-330">    TableSnapshotInputFormat.setInput(job, snapshotName, tmpRestoreDir);</span>
<span class="source-line-no">331</span><span id="line-331">    initTableMapperJob(snapshotName, scan, mapper, outputKeyClass, outputValueClass, job,</span>
<span class="source-line-no">332</span><span id="line-332">      addDependencyJars, false, TableSnapshotInputFormat.class);</span>
<span class="source-line-no">333</span><span id="line-333">    resetCacheConfig(job.getConfiguration());</span>
<span class="source-line-no">334</span><span id="line-334">  }</span>
<span class="source-line-no">335</span><span id="line-335"></span>
<span class="source-line-no">336</span><span id="line-336">  /**</span>
<span class="source-line-no">337</span><span id="line-337">   * Sets up the job for reading from a table snapshot. It bypasses hbase servers and read directly</span>
<span class="source-line-no">338</span><span id="line-338">   * from snapshot files.</span>
<span class="source-line-no">339</span><span id="line-339">   * @param snapshotName       The name of the snapshot (of a table) to read from.</span>
<span class="source-line-no">340</span><span id="line-340">   * @param scan               The scan instance with the columns, time range etc.</span>
<span class="source-line-no">341</span><span id="line-341">   * @param mapper             The mapper class to use.</span>
<span class="source-line-no">342</span><span id="line-342">   * @param outputKeyClass     The class of the output key.</span>
<span class="source-line-no">343</span><span id="line-343">   * @param outputValueClass   The class of the output value.</span>
<span class="source-line-no">344</span><span id="line-344">   * @param job                The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">345</span><span id="line-345">   *                           necessary HBase configuration.</span>
<span class="source-line-no">346</span><span id="line-346">   * @param addDependencyJars  upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">347</span><span id="line-347">   *                           the distributed cache (tmpjars).</span>
<span class="source-line-no">348</span><span id="line-348">   * @param tmpRestoreDir      a temporary directory to copy the snapshot files into. Current user</span>
<span class="source-line-no">349</span><span id="line-349">   *                           should have write permissions to this directory, and this should not</span>
<span class="source-line-no">350</span><span id="line-350">   *                           be a subdirectory of rootdir. After the job is finished, restore</span>
<span class="source-line-no">351</span><span id="line-351">   *                           directory can be deleted.</span>
<span class="source-line-no">352</span><span id="line-352">   * @param splitAlgo          algorithm to split</span>
<span class="source-line-no">353</span><span id="line-353">   * @param numSplitsPerRegion how many input splits to generate per one region</span>
<span class="source-line-no">354</span><span id="line-354">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">355</span><span id="line-355">   * @see TableSnapshotInputFormat</span>
<span class="source-line-no">356</span><span id="line-356">   */</span>
<span class="source-line-no">357</span><span id="line-357">  public static void initTableSnapshotMapperJob(String snapshotName, Scan scan,</span>
<span class="source-line-no">358</span><span id="line-358">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">359</span><span id="line-359">    Job job, boolean addDependencyJars, Path tmpRestoreDir, RegionSplitter.SplitAlgorithm splitAlgo,</span>
<span class="source-line-no">360</span><span id="line-360">    int numSplitsPerRegion) throws IOException {</span>
<span class="source-line-no">361</span><span id="line-361">    TableSnapshotInputFormat.setInput(job, snapshotName, tmpRestoreDir, splitAlgo,</span>
<span class="source-line-no">362</span><span id="line-362">      numSplitsPerRegion);</span>
<span class="source-line-no">363</span><span id="line-363">    initTableMapperJob(snapshotName, scan, mapper, outputKeyClass, outputValueClass, job,</span>
<span class="source-line-no">364</span><span id="line-364">      addDependencyJars, false, TableSnapshotInputFormat.class);</span>
<span class="source-line-no">365</span><span id="line-365">    resetCacheConfig(job.getConfiguration());</span>
<span class="source-line-no">366</span><span id="line-366">  }</span>
<span class="source-line-no">367</span><span id="line-367"></span>
<span class="source-line-no">368</span><span id="line-368">  /**</span>
<span class="source-line-no">369</span><span id="line-369">   * Use this before submitting a Multi TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">370</span><span id="line-370">   * @param scans            The list of {@link Scan} objects to read from.</span>
<span class="source-line-no">371</span><span id="line-371">   * @param mapper           The mapper class to use.</span>
<span class="source-line-no">372</span><span id="line-372">   * @param outputKeyClass   The class of the output key.</span>
<span class="source-line-no">373</span><span id="line-373">   * @param outputValueClass The class of the output value.</span>
<span class="source-line-no">374</span><span id="line-374">   * @param job              The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">375</span><span id="line-375">   *                         necessary HBase configuration.</span>
<span class="source-line-no">376</span><span id="line-376">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">377</span><span id="line-377">   */</span>
<span class="source-line-no">378</span><span id="line-378">  public static void initTableMapperJob(List&lt;Scan&gt; scans, Class&lt;? extends TableMapper&gt; mapper,</span>
<span class="source-line-no">379</span><span id="line-379">    Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass, Job job) throws IOException {</span>
<span class="source-line-no">380</span><span id="line-380">    initTableMapperJob(scans, mapper, outputKeyClass, outputValueClass, job, true);</span>
<span class="source-line-no">381</span><span id="line-381">  }</span>
<span class="source-line-no">382</span><span id="line-382"></span>
<span class="source-line-no">383</span><span id="line-383">  /**</span>
<span class="source-line-no">384</span><span id="line-384">   * Use this before submitting a Multi TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">385</span><span id="line-385">   * @param scans             The list of {@link Scan} objects to read from.</span>
<span class="source-line-no">386</span><span id="line-386">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">387</span><span id="line-387">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">388</span><span id="line-388">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">389</span><span id="line-389">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">390</span><span id="line-390">   *                          necessary HBase configuration.</span>
<span class="source-line-no">391</span><span id="line-391">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">392</span><span id="line-392">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">393</span><span id="line-393">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">394</span><span id="line-394">   */</span>
<span class="source-line-no">395</span><span id="line-395">  public static void initTableMapperJob(List&lt;Scan&gt; scans, Class&lt;? extends TableMapper&gt; mapper,</span>
<span class="source-line-no">396</span><span id="line-396">    Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass, Job job, boolean addDependencyJars)</span>
<span class="source-line-no">397</span><span id="line-397">    throws IOException {</span>
<span class="source-line-no">398</span><span id="line-398">    initTableMapperJob(scans, mapper, outputKeyClass, outputValueClass, job, addDependencyJars,</span>
<span class="source-line-no">399</span><span id="line-399">      true);</span>
<span class="source-line-no">400</span><span id="line-400">  }</span>
<span class="source-line-no">401</span><span id="line-401"></span>
<span class="source-line-no">402</span><span id="line-402">  /**</span>
<span class="source-line-no">403</span><span id="line-403">   * Use this before submitting a Multi TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">404</span><span id="line-404">   * @param scans             The list of {@link Scan} objects to read from.</span>
<span class="source-line-no">405</span><span id="line-405">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">406</span><span id="line-406">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">407</span><span id="line-407">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">408</span><span id="line-408">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">409</span><span id="line-409">   *                          necessary HBase configuration.</span>
<span class="source-line-no">410</span><span id="line-410">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">411</span><span id="line-411">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">412</span><span id="line-412">   * @param initCredentials   whether to initialize hbase auth credentials for the job</span>
<span class="source-line-no">413</span><span id="line-413">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">414</span><span id="line-414">   */</span>
<span class="source-line-no">415</span><span id="line-415">  public static void initTableMapperJob(List&lt;Scan&gt; scans, Class&lt;? extends TableMapper&gt; mapper,</span>
<span class="source-line-no">416</span><span id="line-416">    Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass, Job job, boolean addDependencyJars,</span>
<span class="source-line-no">417</span><span id="line-417">    boolean initCredentials) throws IOException {</span>
<span class="source-line-no">418</span><span id="line-418">    job.setInputFormatClass(MultiTableInputFormat.class);</span>
<span class="source-line-no">419</span><span id="line-419">    if (outputValueClass != null) {</span>
<span class="source-line-no">420</span><span id="line-420">      job.setMapOutputValueClass(outputValueClass);</span>
<span class="source-line-no">421</span><span id="line-421">    }</span>
<span class="source-line-no">422</span><span id="line-422">    if (outputKeyClass != null) {</span>
<span class="source-line-no">423</span><span id="line-423">      job.setMapOutputKeyClass(outputKeyClass);</span>
<span class="source-line-no">424</span><span id="line-424">    }</span>
<span class="source-line-no">425</span><span id="line-425">    job.setMapperClass(mapper);</span>
<span class="source-line-no">426</span><span id="line-426">    Configuration conf = job.getConfiguration();</span>
<span class="source-line-no">427</span><span id="line-427">    HBaseConfiguration.merge(conf, HBaseConfiguration.create(conf));</span>
<span class="source-line-no">428</span><span id="line-428">    List&lt;String&gt; scanStrings = new ArrayList&lt;&gt;();</span>
<span class="source-line-no">429</span><span id="line-429"></span>
<span class="source-line-no">430</span><span id="line-430">    for (Scan scan : scans) {</span>
<span class="source-line-no">431</span><span id="line-431">      scanStrings.add(convertScanToString(scan));</span>
<span class="source-line-no">432</span><span id="line-432">    }</span>
<span class="source-line-no">433</span><span id="line-433">    job.getConfiguration().setStrings(MultiTableInputFormat.SCANS,</span>
<span class="source-line-no">434</span><span id="line-434">      scanStrings.toArray(new String[scanStrings.size()]));</span>
<span class="source-line-no">435</span><span id="line-435"></span>
<span class="source-line-no">436</span><span id="line-436">    if (addDependencyJars) {</span>
<span class="source-line-no">437</span><span id="line-437">      addDependencyJars(job);</span>
<span class="source-line-no">438</span><span id="line-438">    }</span>
<span class="source-line-no">439</span><span id="line-439"></span>
<span class="source-line-no">440</span><span id="line-440">    if (initCredentials) {</span>
<span class="source-line-no">441</span><span id="line-441">      initCredentials(job);</span>
<span class="source-line-no">442</span><span id="line-442">    }</span>
<span class="source-line-no">443</span><span id="line-443">  }</span>
<span class="source-line-no">444</span><span id="line-444"></span>
<span class="source-line-no">445</span><span id="line-445">  public static void initCredentials(Job job) throws IOException {</span>
<span class="source-line-no">446</span><span id="line-446">    UserProvider userProvider = UserProvider.instantiate(job.getConfiguration());</span>
<span class="source-line-no">447</span><span id="line-447">    if (userProvider.isHadoopSecurityEnabled()) {</span>
<span class="source-line-no">448</span><span id="line-448">      // propagate delegation related props from launcher job to MR job</span>
<span class="source-line-no">449</span><span id="line-449">      if (System.getenv("HADOOP_TOKEN_FILE_LOCATION") != null) {</span>
<span class="source-line-no">450</span><span id="line-450">        job.getConfiguration().set("mapreduce.job.credentials.binary",</span>
<span class="source-line-no">451</span><span id="line-451">          System.getenv("HADOOP_TOKEN_FILE_LOCATION"));</span>
<span class="source-line-no">452</span><span id="line-452">      }</span>
<span class="source-line-no">453</span><span id="line-453">    }</span>
<span class="source-line-no">454</span><span id="line-454"></span>
<span class="source-line-no">455</span><span id="line-455">    if (userProvider.isHBaseSecurityEnabled()) {</span>
<span class="source-line-no">456</span><span id="line-456">      try {</span>
<span class="source-line-no">457</span><span id="line-457">        // init credentials for remote cluster</span>
<span class="source-line-no">458</span><span id="line-458">        String quorumAddress = job.getConfiguration().get(TableOutputFormat.QUORUM_ADDRESS);</span>
<span class="source-line-no">459</span><span id="line-459">        User user = userProvider.getCurrent();</span>
<span class="source-line-no">460</span><span id="line-460">        if (quorumAddress != null) {</span>
<span class="source-line-no">461</span><span id="line-461">          Configuration peerConf = HBaseConfiguration.createClusterConf(job.getConfiguration(),</span>
<span class="source-line-no">462</span><span id="line-462">            quorumAddress, TableOutputFormat.OUTPUT_CONF_PREFIX);</span>
<span class="source-line-no">463</span><span id="line-463">          Connection peerConn = ConnectionFactory.createConnection(peerConf);</span>
<span class="source-line-no">464</span><span id="line-464">          try {</span>
<span class="source-line-no">465</span><span id="line-465">            TokenUtil.addTokenForJob(peerConn, user, job);</span>
<span class="source-line-no">466</span><span id="line-466">          } finally {</span>
<span class="source-line-no">467</span><span id="line-467">            peerConn.close();</span>
<span class="source-line-no">468</span><span id="line-468">          }</span>
<span class="source-line-no">469</span><span id="line-469">        }</span>
<span class="source-line-no">470</span><span id="line-470"></span>
<span class="source-line-no">471</span><span id="line-471">        Connection conn = ConnectionFactory.createConnection(job.getConfiguration());</span>
<span class="source-line-no">472</span><span id="line-472">        try {</span>
<span class="source-line-no">473</span><span id="line-473">          TokenUtil.addTokenForJob(conn, user, job);</span>
<span class="source-line-no">474</span><span id="line-474">        } finally {</span>
<span class="source-line-no">475</span><span id="line-475">          conn.close();</span>
<span class="source-line-no">476</span><span id="line-476">        }</span>
<span class="source-line-no">477</span><span id="line-477">      } catch (InterruptedException ie) {</span>
<span class="source-line-no">478</span><span id="line-478">        LOG.info("Interrupted obtaining user authentication token");</span>
<span class="source-line-no">479</span><span id="line-479">        Thread.currentThread().interrupt();</span>
<span class="source-line-no">480</span><span id="line-480">      }</span>
<span class="source-line-no">481</span><span id="line-481">    }</span>
<span class="source-line-no">482</span><span id="line-482">  }</span>
<span class="source-line-no">483</span><span id="line-483"></span>
<span class="source-line-no">484</span><span id="line-484">  /**</span>
<span class="source-line-no">485</span><span id="line-485">   * Obtain an authentication token, for the specified cluster, on behalf of the current user and</span>
<span class="source-line-no">486</span><span id="line-486">   * add it to the credentials for the given map reduce job. The quorumAddress is the key to the ZK</span>
<span class="source-line-no">487</span><span id="line-487">   * ensemble, which contains: hbase.zookeeper.quorum, hbase.zookeeper.client.port and</span>
<span class="source-line-no">488</span><span id="line-488">   * zookeeper.znode.parent</span>
<span class="source-line-no">489</span><span id="line-489">   * @param job           The job that requires the permission.</span>
<span class="source-line-no">490</span><span id="line-490">   * @param quorumAddress string that contains the 3 required configuratins</span>
<span class="source-line-no">491</span><span id="line-491">   * @throws IOException When the authentication token cannot be obtained.</span>
<span class="source-line-no">492</span><span id="line-492">   * @deprecated Since 1.2.0 and will be removed in 3.0.0. Use</span>
<span class="source-line-no">493</span><span id="line-493">   *             {@link #initCredentialsForCluster(Job, Configuration)} instead.</span>
<span class="source-line-no">494</span><span id="line-494">   * @see #initCredentialsForCluster(Job, Configuration)</span>
<span class="source-line-no">495</span><span id="line-495">   * @see &lt;a href="https://issues.apache.org/jira/browse/HBASE-14886"&gt;HBASE-14886&lt;/a&gt;</span>
<span class="source-line-no">496</span><span id="line-496">   */</span>
<span class="source-line-no">497</span><span id="line-497">  @Deprecated</span>
<span class="source-line-no">498</span><span id="line-498">  public static void initCredentialsForCluster(Job job, String quorumAddress) throws IOException {</span>
<span class="source-line-no">499</span><span id="line-499">    Configuration peerConf =</span>
<span class="source-line-no">500</span><span id="line-500">      HBaseConfiguration.createClusterConf(job.getConfiguration(), quorumAddress);</span>
<span class="source-line-no">501</span><span id="line-501">    initCredentialsForCluster(job, peerConf);</span>
<span class="source-line-no">502</span><span id="line-502">  }</span>
<span class="source-line-no">503</span><span id="line-503"></span>
<span class="source-line-no">504</span><span id="line-504">  /**</span>
<span class="source-line-no">505</span><span id="line-505">   * Obtain an authentication token, for the specified cluster, on behalf of the current user and</span>
<span class="source-line-no">506</span><span id="line-506">   * add it to the credentials for the given map reduce job.</span>
<span class="source-line-no">507</span><span id="line-507">   * @param job  The job that requires the permission.</span>
<span class="source-line-no">508</span><span id="line-508">   * @param conf The configuration to use in connecting to the peer cluster</span>
<span class="source-line-no">509</span><span id="line-509">   * @throws IOException When the authentication token cannot be obtained.</span>
<span class="source-line-no">510</span><span id="line-510">   */</span>
<span class="source-line-no">511</span><span id="line-511">  public static void initCredentialsForCluster(Job job, Configuration conf) throws IOException {</span>
<span class="source-line-no">512</span><span id="line-512">    UserProvider userProvider = UserProvider.instantiate(conf);</span>
<span class="source-line-no">513</span><span id="line-513">    if (userProvider.isHBaseSecurityEnabled()) {</span>
<span class="source-line-no">514</span><span id="line-514">      try {</span>
<span class="source-line-no">515</span><span id="line-515">        Connection peerConn = ConnectionFactory.createConnection(conf);</span>
<span class="source-line-no">516</span><span id="line-516">        try {</span>
<span class="source-line-no">517</span><span id="line-517">          TokenUtil.addTokenForJob(peerConn, userProvider.getCurrent(), job);</span>
<span class="source-line-no">518</span><span id="line-518">        } finally {</span>
<span class="source-line-no">519</span><span id="line-519">          peerConn.close();</span>
<span class="source-line-no">520</span><span id="line-520">        }</span>
<span class="source-line-no">521</span><span id="line-521">      } catch (InterruptedException e) {</span>
<span class="source-line-no">522</span><span id="line-522">        LOG.info("Interrupted obtaining user authentication token");</span>
<span class="source-line-no">523</span><span id="line-523">        Thread.interrupted();</span>
<span class="source-line-no">524</span><span id="line-524">      }</span>
<span class="source-line-no">525</span><span id="line-525">    }</span>
<span class="source-line-no">526</span><span id="line-526">  }</span>
<span class="source-line-no">527</span><span id="line-527"></span>
<span class="source-line-no">528</span><span id="line-528">  /**</span>
<span class="source-line-no">529</span><span id="line-529">   * Writes the given scan into a Base64 encoded string.</span>
<span class="source-line-no">530</span><span id="line-530">   * @param scan The scan to write out.</span>
<span class="source-line-no">531</span><span id="line-531">   * @return The scan saved in a Base64 encoded string.</span>
<span class="source-line-no">532</span><span id="line-532">   * @throws IOException When writing the scan fails.</span>
<span class="source-line-no">533</span><span id="line-533">   */</span>
<span class="source-line-no">534</span><span id="line-534">  public static String convertScanToString(Scan scan) throws IOException {</span>
<span class="source-line-no">535</span><span id="line-535">    ClientProtos.Scan proto = ProtobufUtil.toScan(scan);</span>
<span class="source-line-no">536</span><span id="line-536">    return Bytes.toString(Base64.getEncoder().encode(proto.toByteArray()));</span>
<span class="source-line-no">537</span><span id="line-537">  }</span>
<span class="source-line-no">538</span><span id="line-538"></span>
<span class="source-line-no">539</span><span id="line-539">  /**</span>
<span class="source-line-no">540</span><span id="line-540">   * Converts the given Base64 string back into a Scan instance.</span>
<span class="source-line-no">541</span><span id="line-541">   * @param base64 The scan details.</span>
<span class="source-line-no">542</span><span id="line-542">   * @return The newly created Scan instance.</span>
<span class="source-line-no">543</span><span id="line-543">   * @throws IOException When reading the scan instance fails.</span>
<span class="source-line-no">544</span><span id="line-544">   */</span>
<span class="source-line-no">545</span><span id="line-545">  public static Scan convertStringToScan(String base64) throws IOException {</span>
<span class="source-line-no">546</span><span id="line-546">    byte[] decoded = Base64.getDecoder().decode(base64);</span>
<span class="source-line-no">547</span><span id="line-547">    return ProtobufUtil.toScan(ClientProtos.Scan.parseFrom(decoded));</span>
<span class="source-line-no">548</span><span id="line-548">  }</span>
<span class="source-line-no">549</span><span id="line-549"></span>
<span class="source-line-no">550</span><span id="line-550">  /**</span>
<span class="source-line-no">551</span><span id="line-551">   * Use this before submitting a TableReduce job. It will appropriately set up the JobConf.</span>
<span class="source-line-no">552</span><span id="line-552">   * @param table   The output table.</span>
<span class="source-line-no">553</span><span id="line-553">   * @param reducer The reducer class to use.</span>
<span class="source-line-no">554</span><span id="line-554">   * @param job     The current job to adjust.</span>
<span class="source-line-no">555</span><span id="line-555">   * @throws IOException When determining the region count fails.</span>
<span class="source-line-no">556</span><span id="line-556">   */</span>
<span class="source-line-no">557</span><span id="line-557">  public static void initTableReducerJob(String table, Class&lt;? extends TableReducer&gt; reducer,</span>
<span class="source-line-no">558</span><span id="line-558">    Job job) throws IOException {</span>
<span class="source-line-no">559</span><span id="line-559">    initTableReducerJob(table, reducer, job, null);</span>
<span class="source-line-no">560</span><span id="line-560">  }</span>
<span class="source-line-no">561</span><span id="line-561"></span>
<span class="source-line-no">562</span><span id="line-562">  /**</span>
<span class="source-line-no">563</span><span id="line-563">   * Use this before submitting a TableReduce job. It will appropriately set up the JobConf.</span>
<span class="source-line-no">564</span><span id="line-564">   * @param table       The output table.</span>
<span class="source-line-no">565</span><span id="line-565">   * @param reducer     The reducer class to use.</span>
<span class="source-line-no">566</span><span id="line-566">   * @param job         The current job to adjust.</span>
<span class="source-line-no">567</span><span id="line-567">   * @param partitioner Partitioner to use. Pass &lt;code&gt;null&lt;/code&gt; to use default partitioner.</span>
<span class="source-line-no">568</span><span id="line-568">   * @throws IOException When determining the region count fails.</span>
<span class="source-line-no">569</span><span id="line-569">   */</span>
<span class="source-line-no">570</span><span id="line-570">  public static void initTableReducerJob(String table, Class&lt;? extends TableReducer&gt; reducer,</span>
<span class="source-line-no">571</span><span id="line-571">    Job job, Class partitioner) throws IOException {</span>
<span class="source-line-no">572</span><span id="line-572">    initTableReducerJob(table, reducer, job, partitioner, null, null, null);</span>
<span class="source-line-no">573</span><span id="line-573">  }</span>
<span class="source-line-no">574</span><span id="line-574"></span>
<span class="source-line-no">575</span><span id="line-575">  /**</span>
<span class="source-line-no">576</span><span id="line-576">   * Use this before submitting a TableReduce job. It will appropriately set up the JobConf.</span>
<span class="source-line-no">577</span><span id="line-577">   * @param table         The output table.</span>
<span class="source-line-no">578</span><span id="line-578">   * @param reducer       The reducer class to use.</span>
<span class="source-line-no">579</span><span id="line-579">   * @param job           The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">580</span><span id="line-580">   *                      necessary HBase configuration.</span>
<span class="source-line-no">581</span><span id="line-581">   * @param partitioner   Partitioner to use. Pass &lt;code&gt;null&lt;/code&gt; to use default partitioner.</span>
<span class="source-line-no">582</span><span id="line-582">   * @param quorumAddress Distant cluster to write to; default is null for output to the cluster</span>
<span class="source-line-no">583</span><span id="line-583">   *                      that is designated in &lt;code&gt;hbase-site.xml&lt;/code&gt;. Set this String to the</span>
<span class="source-line-no">584</span><span id="line-584">   *                      zookeeper ensemble of an alternate remote cluster when you would have the</span>
<span class="source-line-no">585</span><span id="line-585">   *                      reduce write a cluster that is other than the default; e.g. copying tables</span>
<span class="source-line-no">586</span><span id="line-586">   *                      between clusters, the source would be designated by</span>
<span class="source-line-no">587</span><span id="line-587">   *                      &lt;code&gt;hbase-site.xml&lt;/code&gt; and this param would have the ensemble address</span>
<span class="source-line-no">588</span><span id="line-588">   *                      of the remote cluster. The format to pass is particular. Pass</span>
<span class="source-line-no">589</span><span id="line-589">   *                      &lt;code&gt; &amp;lt;hbase.zookeeper.quorum&amp;gt;:&amp;lt;</span>
<span class="source-line-no">590</span><span id="line-590">   *             hbase.zookeeper.client.port&amp;gt;:&amp;lt;zookeeper.znode.parent&amp;gt;</span>
<span class="source-line-no">591</span><span id="line-591">   * &lt;/code&gt;           such as &lt;code&gt;server,server2,server3:2181:/hbase&lt;/code&gt;.</span>
<span class="source-line-no">592</span><span id="line-592">   * @param serverClass   redefined hbase.regionserver.class</span>
<span class="source-line-no">593</span><span id="line-593">   * @param serverImpl    redefined hbase.regionserver.impl</span>
<span class="source-line-no">594</span><span id="line-594">   * @throws IOException When determining the region count fails.</span>
<span class="source-line-no">595</span><span id="line-595">   */</span>
<span class="source-line-no">596</span><span id="line-596">  public static void initTableReducerJob(String table, Class&lt;? extends TableReducer&gt; reducer,</span>
<span class="source-line-no">597</span><span id="line-597">    Job job, Class partitioner, String quorumAddress, String serverClass, String serverImpl)</span>
<span class="source-line-no">598</span><span id="line-598">    throws IOException {</span>
<span class="source-line-no">599</span><span id="line-599">    initTableReducerJob(table, reducer, job, partitioner, quorumAddress, serverClass, serverImpl,</span>
<span class="source-line-no">600</span><span id="line-600">      true);</span>
<span class="source-line-no">601</span><span id="line-601">  }</span>
<span class="source-line-no">602</span><span id="line-602"></span>
<span class="source-line-no">603</span><span id="line-603">  /**</span>
<span class="source-line-no">604</span><span id="line-604">   * Use this before submitting a TableReduce job. It will appropriately set up the JobConf.</span>
<span class="source-line-no">605</span><span id="line-605">   * @param table             The output table.</span>
<span class="source-line-no">606</span><span id="line-606">   * @param reducer           The reducer class to use.</span>
<span class="source-line-no">607</span><span id="line-607">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">608</span><span id="line-608">   *                          necessary HBase configuration.</span>
<span class="source-line-no">609</span><span id="line-609">   * @param partitioner       Partitioner to use. Pass &lt;code&gt;null&lt;/code&gt; to use default partitioner.</span>
<span class="source-line-no">610</span><span id="line-610">   * @param quorumAddress     Distant cluster to write to; default is null for output to the cluster</span>
<span class="source-line-no">611</span><span id="line-611">   *                          that is designated in &lt;code&gt;hbase-site.xml&lt;/code&gt;. Set this String to</span>
<span class="source-line-no">612</span><span id="line-612">   *                          the zookeeper ensemble of an alternate remote cluster when you would</span>
<span class="source-line-no">613</span><span id="line-613">   *                          have the reduce write a cluster that is other than the default; e.g.</span>
<span class="source-line-no">614</span><span id="line-614">   *                          copying tables between clusters, the source would be designated by</span>
<span class="source-line-no">615</span><span id="line-615">   *                          &lt;code&gt;hbase-site.xml&lt;/code&gt; and this param would have the ensemble</span>
<span class="source-line-no">616</span><span id="line-616">   *                          address of the remote cluster. The format to pass is particular. Pass</span>
<span class="source-line-no">617</span><span id="line-617">   *                          &lt;code&gt; &amp;lt;hbase.zookeeper.quorum&amp;gt;:&amp;lt;</span>
<span class="source-line-no">618</span><span id="line-618">   *             hbase.zookeeper.client.port&amp;gt;:&amp;lt;zookeeper.znode.parent&amp;gt;</span>
<span class="source-line-no">619</span><span id="line-619">   * &lt;/code&gt;               such as &lt;code&gt;server,server2,server3:2181:/hbase&lt;/code&gt;.</span>
<span class="source-line-no">620</span><span id="line-620">   * @param serverClass       redefined hbase.regionserver.class</span>
<span class="source-line-no">621</span><span id="line-621">   * @param serverImpl        redefined hbase.regionserver.impl</span>
<span class="source-line-no">622</span><span id="line-622">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">623</span><span id="line-623">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">624</span><span id="line-624">   * @throws IOException When determining the region count fails.</span>
<span class="source-line-no">625</span><span id="line-625">   */</span>
<span class="source-line-no">626</span><span id="line-626">  public static void initTableReducerJob(String table, Class&lt;? extends TableReducer&gt; reducer,</span>
<span class="source-line-no">627</span><span id="line-627">    Job job, Class partitioner, String quorumAddress, String serverClass, String serverImpl,</span>
<span class="source-line-no">628</span><span id="line-628">    boolean addDependencyJars) throws IOException {</span>
<span class="source-line-no">629</span><span id="line-629"></span>
<span class="source-line-no">630</span><span id="line-630">    Configuration conf = job.getConfiguration();</span>
<span class="source-line-no">631</span><span id="line-631">    HBaseConfiguration.merge(conf, HBaseConfiguration.create(conf));</span>
<span class="source-line-no">632</span><span id="line-632">    job.setOutputFormatClass(TableOutputFormat.class);</span>
<span class="source-line-no">633</span><span id="line-633">    if (reducer != null) job.setReducerClass(reducer);</span>
<span class="source-line-no">634</span><span id="line-634">    conf.set(TableOutputFormat.OUTPUT_TABLE, table);</span>
<span class="source-line-no">635</span><span id="line-635">    conf.setStrings("io.serializations", conf.get("io.serializations"),</span>
<span class="source-line-no">636</span><span id="line-636">      MutationSerialization.class.getName(), ResultSerialization.class.getName());</span>
<span class="source-line-no">637</span><span id="line-637">    // If passed a quorum/ensemble address, pass it on to TableOutputFormat.</span>
<span class="source-line-no">638</span><span id="line-638">    if (quorumAddress != null) {</span>
<span class="source-line-no">639</span><span id="line-639">      // Calling this will validate the format</span>
<span class="source-line-no">640</span><span id="line-640">      ZKConfig.validateClusterKey(quorumAddress);</span>
<span class="source-line-no">641</span><span id="line-641">      conf.set(TableOutputFormat.QUORUM_ADDRESS, quorumAddress);</span>
<span class="source-line-no">642</span><span id="line-642">    }</span>
<span class="source-line-no">643</span><span id="line-643">    if (serverClass != null &amp;&amp; serverImpl != null) {</span>
<span class="source-line-no">644</span><span id="line-644">      conf.set(TableOutputFormat.REGION_SERVER_CLASS, serverClass);</span>
<span class="source-line-no">645</span><span id="line-645">      conf.set(TableOutputFormat.REGION_SERVER_IMPL, serverImpl);</span>
<span class="source-line-no">646</span><span id="line-646">    }</span>
<span class="source-line-no">647</span><span id="line-647">    job.setOutputKeyClass(ImmutableBytesWritable.class);</span>
<span class="source-line-no">648</span><span id="line-648">    job.setOutputValueClass(Writable.class);</span>
<span class="source-line-no">649</span><span id="line-649">    if (partitioner == HRegionPartitioner.class) {</span>
<span class="source-line-no">650</span><span id="line-650">      job.setPartitionerClass(HRegionPartitioner.class);</span>
<span class="source-line-no">651</span><span id="line-651">      int regions = getRegionCount(conf, TableName.valueOf(table));</span>
<span class="source-line-no">652</span><span id="line-652">      if (job.getNumReduceTasks() &gt; regions) {</span>
<span class="source-line-no">653</span><span id="line-653">        job.setNumReduceTasks(regions);</span>
<span class="source-line-no">654</span><span id="line-654">      }</span>
<span class="source-line-no">655</span><span id="line-655">    } else if (partitioner != null) {</span>
<span class="source-line-no">656</span><span id="line-656">      job.setPartitionerClass(partitioner);</span>
<span class="source-line-no">657</span><span id="line-657">    }</span>
<span class="source-line-no">658</span><span id="line-658"></span>
<span class="source-line-no">659</span><span id="line-659">    if (addDependencyJars) {</span>
<span class="source-line-no">660</span><span id="line-660">      addDependencyJars(job);</span>
<span class="source-line-no">661</span><span id="line-661">    }</span>
<span class="source-line-no">662</span><span id="line-662"></span>
<span class="source-line-no">663</span><span id="line-663">    initCredentials(job);</span>
<span class="source-line-no">664</span><span id="line-664">  }</span>
<span class="source-line-no">665</span><span id="line-665"></span>
<span class="source-line-no">666</span><span id="line-666">  /**</span>
<span class="source-line-no">667</span><span id="line-667">   * Ensures that the given number of reduce tasks for the given job configuration does not exceed</span>
<span class="source-line-no">668</span><span id="line-668">   * the number of regions for the given table.</span>
<span class="source-line-no">669</span><span id="line-669">   * @param table The table to get the region count for.</span>
<span class="source-line-no">670</span><span id="line-670">   * @param job   The current job to adjust.</span>
<span class="source-line-no">671</span><span id="line-671">   * @throws IOException When retrieving the table details fails.</span>
<span class="source-line-no">672</span><span id="line-672">   */</span>
<span class="source-line-no">673</span><span id="line-673">  public static void limitNumReduceTasks(String table, Job job) throws IOException {</span>
<span class="source-line-no">674</span><span id="line-674">    int regions = getRegionCount(job.getConfiguration(), TableName.valueOf(table));</span>
<span class="source-line-no">675</span><span id="line-675">    if (job.getNumReduceTasks() &gt; regions) {</span>
<span class="source-line-no">676</span><span id="line-676">      job.setNumReduceTasks(regions);</span>
<span class="source-line-no">677</span><span id="line-677">    }</span>
<span class="source-line-no">678</span><span id="line-678">  }</span>
<span class="source-line-no">679</span><span id="line-679"></span>
<span class="source-line-no">680</span><span id="line-680">  /**</span>
<span class="source-line-no">681</span><span id="line-681">   * Sets the number of reduce tasks for the given job configuration to the number of regions the</span>
<span class="source-line-no">682</span><span id="line-682">   * given table has.</span>
<span class="source-line-no">683</span><span id="line-683">   * @param table The table to get the region count for.</span>
<span class="source-line-no">684</span><span id="line-684">   * @param job   The current job to adjust.</span>
<span class="source-line-no">685</span><span id="line-685">   * @throws IOException When retrieving the table details fails.</span>
<span class="source-line-no">686</span><span id="line-686">   */</span>
<span class="source-line-no">687</span><span id="line-687">  public static void setNumReduceTasks(String table, Job job) throws IOException {</span>
<span class="source-line-no">688</span><span id="line-688">    job.setNumReduceTasks(getRegionCount(job.getConfiguration(), TableName.valueOf(table)));</span>
<span class="source-line-no">689</span><span id="line-689">  }</span>
<span class="source-line-no">690</span><span id="line-690"></span>
<span class="source-line-no">691</span><span id="line-691">  /**</span>
<span class="source-line-no">692</span><span id="line-692">   * Sets the number of rows to return and cache with each scanner iteration. Higher caching values</span>
<span class="source-line-no">693</span><span id="line-693">   * will enable faster mapreduce jobs at the expense of requiring more heap to contain the cached</span>
<span class="source-line-no">694</span><span id="line-694">   * rows.</span>
<span class="source-line-no">695</span><span id="line-695">   * @param job       The current job to adjust.</span>
<span class="source-line-no">696</span><span id="line-696">   * @param batchSize The number of rows to return in batch with each scanner iteration.</span>
<span class="source-line-no">697</span><span id="line-697">   */</span>
<span class="source-line-no">698</span><span id="line-698">  public static void setScannerCaching(Job job, int batchSize) {</span>
<span class="source-line-no">699</span><span id="line-699">    job.getConfiguration().setInt("hbase.client.scanner.caching", batchSize);</span>
<span class="source-line-no">700</span><span id="line-700">  }</span>
<span class="source-line-no">701</span><span id="line-701"></span>
<span class="source-line-no">702</span><span id="line-702">  /**</span>
<span class="source-line-no">703</span><span id="line-703">   * Add HBase and its dependencies (only) to the job configuration.</span>
<span class="source-line-no">704</span><span id="line-704">   * &lt;p&gt;</span>
<span class="source-line-no">705</span><span id="line-705">   * This is intended as a low-level API, facilitating code reuse between this class and its mapred</span>
<span class="source-line-no">706</span><span id="line-706">   * counterpart. It also of use to external tools that need to build a MapReduce job that interacts</span>
<span class="source-line-no">707</span><span id="line-707">   * with HBase but want fine-grained control over the jars shipped to the cluster.</span>
<span class="source-line-no">708</span><span id="line-708">   * &lt;/p&gt;</span>
<span class="source-line-no">709</span><span id="line-709">   * @param conf The Configuration object to extend with dependencies.</span>
<span class="source-line-no">710</span><span id="line-710">   * @see org.apache.hadoop.hbase.mapred.TableMapReduceUtil</span>
<span class="source-line-no">711</span><span id="line-711">   * @see &lt;a href="https://issues.apache.org/jira/browse/PIG-3285"&gt;PIG-3285&lt;/a&gt;</span>
<span class="source-line-no">712</span><span id="line-712">   */</span>
<span class="source-line-no">713</span><span id="line-713">  public static void addHBaseDependencyJars(Configuration conf) throws IOException {</span>
<span class="source-line-no">714</span><span id="line-714">    addDependencyJarsForClasses(conf,</span>
<span class="source-line-no">715</span><span id="line-715">      // explicitly pull a class from each module</span>
<span class="source-line-no">716</span><span id="line-716">      org.apache.hadoop.hbase.HConstants.class, // hbase-common</span>
<span class="source-line-no">717</span><span id="line-717">      org.apache.hadoop.hbase.protobuf.generated.ClientProtos.class, // hbase-protocol</span>
<span class="source-line-no">718</span><span id="line-718">      org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.class, // hbase-protocol-shaded</span>
<span class="source-line-no">719</span><span id="line-719">      org.apache.hadoop.hbase.client.Put.class, // hbase-client</span>
<span class="source-line-no">720</span><span id="line-720">      org.apache.hadoop.hbase.ipc.RpcServer.class, // hbase-server</span>
<span class="source-line-no">721</span><span id="line-721">      org.apache.hadoop.hbase.CompatibilityFactory.class, // hbase-hadoop-compat</span>
<span class="source-line-no">722</span><span id="line-722">      org.apache.hadoop.hbase.mapreduce.JobUtil.class, // hbase-hadoop2-compat</span>
<span class="source-line-no">723</span><span id="line-723">      org.apache.hadoop.hbase.mapreduce.TableMapper.class, // hbase-mapreduce</span>
<span class="source-line-no">724</span><span id="line-724">      org.apache.hadoop.hbase.metrics.impl.FastLongHistogram.class, // hbase-metrics</span>
<span class="source-line-no">725</span><span id="line-725">      org.apache.hadoop.hbase.metrics.Snapshot.class, // hbase-metrics-api</span>
<span class="source-line-no">726</span><span id="line-726">      org.apache.hadoop.hbase.replication.ReplicationUtils.class, // hbase-replication</span>
<span class="source-line-no">727</span><span id="line-727">      org.apache.hadoop.hbase.http.HttpServer.class, // hbase-http</span>
<span class="source-line-no">728</span><span id="line-728">      org.apache.hadoop.hbase.procedure2.Procedure.class, // hbase-procedure</span>
<span class="source-line-no">729</span><span id="line-729">      org.apache.hadoop.hbase.zookeeper.ZKWatcher.class, // hbase-zookeeper</span>
<span class="source-line-no">730</span><span id="line-730">      org.apache.hbase.thirdparty.com.google.common.collect.Lists.class, // hb-shaded-miscellaneous</span>
<span class="source-line-no">731</span><span id="line-731">      org.apache.hbase.thirdparty.com.google.gson.GsonBuilder.class, // hbase-shaded-gson</span>
<span class="source-line-no">732</span><span id="line-732">      org.apache.hbase.thirdparty.com.google.protobuf.UnsafeByteOperations.class, // hb-sh-protobuf</span>
<span class="source-line-no">733</span><span id="line-733">      org.apache.hbase.thirdparty.io.netty.channel.Channel.class, // hbase-shaded-netty</span>
<span class="source-line-no">734</span><span id="line-734">      org.apache.hadoop.hbase.unsafe.HBasePlatformDependent.class, // hbase-unsafe</span>
<span class="source-line-no">735</span><span id="line-735">      org.apache.zookeeper.ZooKeeper.class, // zookeeper</span>
<span class="source-line-no">736</span><span id="line-736">      com.google.protobuf.Message.class, // protobuf</span>
<span class="source-line-no">737</span><span id="line-737">      com.codahale.metrics.MetricRegistry.class, // metrics-core</span>
<span class="source-line-no">738</span><span id="line-738">      org.apache.commons.lang3.ArrayUtils.class, // commons-lang</span>
<span class="source-line-no">739</span><span id="line-739">      io.opentelemetry.api.trace.Span.class, // opentelemetry-api</span>
<span class="source-line-no">740</span><span id="line-740">      io.opentelemetry.semconv.trace.attributes.SemanticAttributes.class, // opentelemetry-semconv</span>
<span class="source-line-no">741</span><span id="line-741">      io.opentelemetry.context.Context.class); // opentelemetry-context</span>
<span class="source-line-no">742</span><span id="line-742">  }</span>
<span class="source-line-no">743</span><span id="line-743"></span>
<span class="source-line-no">744</span><span id="line-744">  /**</span>
<span class="source-line-no">745</span><span id="line-745">   * Returns a classpath string built from the content of the "tmpjars" value in {@code conf}. Also</span>
<span class="source-line-no">746</span><span id="line-746">   * exposed to shell scripts via `bin/hbase mapredcp`.</span>
<span class="source-line-no">747</span><span id="line-747">   */</span>
<span class="source-line-no">748</span><span id="line-748">  public static String buildDependencyClasspath(Configuration conf) {</span>
<span class="source-line-no">749</span><span id="line-749">    if (conf == null) {</span>
<span class="source-line-no">750</span><span id="line-750">      throw new IllegalArgumentException("Must provide a configuration object.");</span>
<span class="source-line-no">751</span><span id="line-751">    }</span>
<span class="source-line-no">752</span><span id="line-752">    Set&lt;String&gt; paths = new HashSet&lt;&gt;(conf.getStringCollection("tmpjars"));</span>
<span class="source-line-no">753</span><span id="line-753">    if (paths.isEmpty()) {</span>
<span class="source-line-no">754</span><span id="line-754">      throw new IllegalArgumentException("Configuration contains no tmpjars.");</span>
<span class="source-line-no">755</span><span id="line-755">    }</span>
<span class="source-line-no">756</span><span id="line-756">    StringBuilder sb = new StringBuilder();</span>
<span class="source-line-no">757</span><span id="line-757">    for (String s : paths) {</span>
<span class="source-line-no">758</span><span id="line-758">      // entries can take the form 'file:/path/to/file.jar'.</span>
<span class="source-line-no">759</span><span id="line-759">      int idx = s.indexOf(":");</span>
<span class="source-line-no">760</span><span id="line-760">      if (idx != -1) s = s.substring(idx + 1);</span>
<span class="source-line-no">761</span><span id="line-761">      if (sb.length() &gt; 0) sb.append(File.pathSeparator);</span>
<span class="source-line-no">762</span><span id="line-762">      sb.append(s);</span>
<span class="source-line-no">763</span><span id="line-763">    }</span>
<span class="source-line-no">764</span><span id="line-764">    return sb.toString();</span>
<span class="source-line-no">765</span><span id="line-765">  }</span>
<span class="source-line-no">766</span><span id="line-766"></span>
<span class="source-line-no">767</span><span id="line-767">  /**</span>
<span class="source-line-no">768</span><span id="line-768">   * Add the HBase dependency jars as well as jars for any of the configured job classes to the job</span>
<span class="source-line-no">769</span><span id="line-769">   * configuration, so that JobClient will ship them to the cluster and add them to the</span>
<span class="source-line-no">770</span><span id="line-770">   * DistributedCache.</span>
<span class="source-line-no">771</span><span id="line-771">   */</span>
<span class="source-line-no">772</span><span id="line-772">  public static void addDependencyJars(Job job) throws IOException {</span>
<span class="source-line-no">773</span><span id="line-773">    addHBaseDependencyJars(job.getConfiguration());</span>
<span class="source-line-no">774</span><span id="line-774">    try {</span>
<span class="source-line-no">775</span><span id="line-775">      addDependencyJarsForClasses(job.getConfiguration(),</span>
<span class="source-line-no">776</span><span id="line-776">        // when making changes here, consider also mapred.TableMapReduceUtil</span>
<span class="source-line-no">777</span><span id="line-777">        // pull job classes</span>
<span class="source-line-no">778</span><span id="line-778">        job.getMapOutputKeyClass(), job.getMapOutputValueClass(), job.getInputFormatClass(),</span>
<span class="source-line-no">779</span><span id="line-779">        job.getOutputKeyClass(), job.getOutputValueClass(), job.getOutputFormatClass(),</span>
<span class="source-line-no">780</span><span id="line-780">        job.getPartitionerClass(), job.getCombinerClass());</span>
<span class="source-line-no">781</span><span id="line-781">    } catch (ClassNotFoundException e) {</span>
<span class="source-line-no">782</span><span id="line-782">      throw new IOException(e);</span>
<span class="source-line-no">783</span><span id="line-783">    }</span>
<span class="source-line-no">784</span><span id="line-784">  }</span>
<span class="source-line-no">785</span><span id="line-785"></span>
<span class="source-line-no">786</span><span id="line-786">  /**</span>
<span class="source-line-no">787</span><span id="line-787">   * Add the jars containing the given classes to the job's configuration such that JobClient will</span>
<span class="source-line-no">788</span><span id="line-788">   * ship them to the cluster and add them to the DistributedCache.</span>
<span class="source-line-no">789</span><span id="line-789">   * @deprecated since 1.3.0 and will be removed in 3.0.0. Use {@link #addDependencyJars(Job)}</span>
<span class="source-line-no">790</span><span id="line-790">   *             instead.</span>
<span class="source-line-no">791</span><span id="line-791">   * @see #addDependencyJars(Job)</span>
<span class="source-line-no">792</span><span id="line-792">   * @see &lt;a href="https://issues.apache.org/jira/browse/HBASE-8386"&gt;HBASE-8386&lt;/a&gt;</span>
<span class="source-line-no">793</span><span id="line-793">   */</span>
<span class="source-line-no">794</span><span id="line-794">  @Deprecated</span>
<span class="source-line-no">795</span><span id="line-795">  public static void addDependencyJars(Configuration conf, Class&lt;?&gt;... classes) throws IOException {</span>
<span class="source-line-no">796</span><span id="line-796">    LOG.warn("The addDependencyJars(Configuration, Class&lt;?&gt;...) method has been deprecated since it"</span>
<span class="source-line-no">797</span><span id="line-797">      + " is easy to use incorrectly. Most users should rely on addDependencyJars(Job) "</span>
<span class="source-line-no">798</span><span id="line-798">      + "instead. See HBASE-8386 for more details.");</span>
<span class="source-line-no">799</span><span id="line-799">    addDependencyJarsForClasses(conf, classes);</span>
<span class="source-line-no">800</span><span id="line-800">  }</span>
<span class="source-line-no">801</span><span id="line-801"></span>
<span class="source-line-no">802</span><span id="line-802">  /**</span>
<span class="source-line-no">803</span><span id="line-803">   * Add the jars containing the given classes to the job's configuration such that JobClient will</span>
<span class="source-line-no">804</span><span id="line-804">   * ship them to the cluster and add them to the DistributedCache. N.B. that this method at most</span>
<span class="source-line-no">805</span><span id="line-805">   * adds one jar per class given. If there is more than one jar available containing a class with</span>
<span class="source-line-no">806</span><span id="line-806">   * the same name as a given class, we don't define which of those jars might be chosen.</span>
<span class="source-line-no">807</span><span id="line-807">   * @param conf    The Hadoop Configuration to modify</span>
<span class="source-line-no">808</span><span id="line-808">   * @param classes will add just those dependencies needed to find the given classes</span>
<span class="source-line-no">809</span><span id="line-809">   * @throws IOException if an underlying library call fails.</span>
<span class="source-line-no">810</span><span id="line-810">   */</span>
<span class="source-line-no">811</span><span id="line-811">  @InterfaceAudience.Private</span>
<span class="source-line-no">812</span><span id="line-812">  public static void addDependencyJarsForClasses(Configuration conf, Class&lt;?&gt;... classes)</span>
<span class="source-line-no">813</span><span id="line-813">    throws IOException {</span>
<span class="source-line-no">814</span><span id="line-814"></span>
<span class="source-line-no">815</span><span id="line-815">    FileSystem localFs = FileSystem.getLocal(conf);</span>
<span class="source-line-no">816</span><span id="line-816">    Set&lt;String&gt; jars = new HashSet&lt;&gt;();</span>
<span class="source-line-no">817</span><span id="line-817">    // Add jars that are already in the tmpjars variable</span>
<span class="source-line-no">818</span><span id="line-818">    jars.addAll(conf.getStringCollection("tmpjars"));</span>
<span class="source-line-no">819</span><span id="line-819"></span>
<span class="source-line-no">820</span><span id="line-820">    // add jars as we find them to a map of contents jar name so that we can avoid</span>
<span class="source-line-no">821</span><span id="line-821">    // creating new jars for classes that have already been packaged.</span>
<span class="source-line-no">822</span><span id="line-822">    Map&lt;String, String&gt; packagedClasses = new HashMap&lt;&gt;();</span>
<span class="source-line-no">823</span><span id="line-823"></span>
<span class="source-line-no">824</span><span id="line-824">    // Add jars containing the specified classes</span>
<span class="source-line-no">825</span><span id="line-825">    for (Class&lt;?&gt; clazz : classes) {</span>
<span class="source-line-no">826</span><span id="line-826">      if (clazz == null) continue;</span>
<span class="source-line-no">827</span><span id="line-827"></span>
<span class="source-line-no">828</span><span id="line-828">      Path path = findOrCreateJar(clazz, localFs, packagedClasses);</span>
<span class="source-line-no">829</span><span id="line-829">      if (path == null) {</span>
<span class="source-line-no">830</span><span id="line-830">        LOG.warn("Could not find jar for class " + clazz + " in order to ship it to the cluster.");</span>
<span class="source-line-no">831</span><span id="line-831">        continue;</span>
<span class="source-line-no">832</span><span id="line-832">      }</span>
<span class="source-line-no">833</span><span id="line-833">      if (!localFs.exists(path)) {</span>
<span class="source-line-no">834</span><span id="line-834">        LOG.warn("Could not validate jar file " + path + " for class " + clazz);</span>
<span class="source-line-no">835</span><span id="line-835">        continue;</span>
<span class="source-line-no">836</span><span id="line-836">      }</span>
<span class="source-line-no">837</span><span id="line-837">      jars.add(path.toString());</span>
<span class="source-line-no">838</span><span id="line-838">    }</span>
<span class="source-line-no">839</span><span id="line-839">    if (jars.isEmpty()) return;</span>
<span class="source-line-no">840</span><span id="line-840"></span>
<span class="source-line-no">841</span><span id="line-841">    conf.set("tmpjars", StringUtils.arrayToString(jars.toArray(new String[jars.size()])));</span>
<span class="source-line-no">842</span><span id="line-842">  }</span>
<span class="source-line-no">843</span><span id="line-843"></span>
<span class="source-line-no">844</span><span id="line-844">  /**</span>
<span class="source-line-no">845</span><span id="line-845">   * Finds the Jar for a class or creates it if it doesn't exist. If the class is in a directory in</span>
<span class="source-line-no">846</span><span id="line-846">   * the classpath, it creates a Jar on the fly with the contents of the directory and returns the</span>
<span class="source-line-no">847</span><span id="line-847">   * path to that Jar. If a Jar is created, it is created in the system temporary directory.</span>
<span class="source-line-no">848</span><span id="line-848">   * Otherwise, returns an existing jar that contains a class of the same name. Maintains a mapping</span>
<span class="source-line-no">849</span><span id="line-849">   * from jar contents to the tmp jar created.</span>
<span class="source-line-no">850</span><span id="line-850">   * @param my_class        the class to find.</span>
<span class="source-line-no">851</span><span id="line-851">   * @param fs              the FileSystem with which to qualify the returned path.</span>
<span class="source-line-no">852</span><span id="line-852">   * @param packagedClasses a map of class name to path.</span>
<span class="source-line-no">853</span><span id="line-853">   * @return a jar file that contains the class.</span>
<span class="source-line-no">854</span><span id="line-854">   */</span>
<span class="source-line-no">855</span><span id="line-855">  private static Path findOrCreateJar(Class&lt;?&gt; my_class, FileSystem fs,</span>
<span class="source-line-no">856</span><span id="line-856">    Map&lt;String, String&gt; packagedClasses) throws IOException {</span>
<span class="source-line-no">857</span><span id="line-857">    // attempt to locate an existing jar for the class.</span>
<span class="source-line-no">858</span><span id="line-858">    String jar = findContainingJar(my_class, packagedClasses);</span>
<span class="source-line-no">859</span><span id="line-859">    if (null == jar || jar.isEmpty()) {</span>
<span class="source-line-no">860</span><span id="line-860">      jar = getJar(my_class);</span>
<span class="source-line-no">861</span><span id="line-861">      updateMap(jar, packagedClasses);</span>
<span class="source-line-no">862</span><span id="line-862">    }</span>
<span class="source-line-no">863</span><span id="line-863"></span>
<span class="source-line-no">864</span><span id="line-864">    if (null == jar || jar.isEmpty()) {</span>
<span class="source-line-no">865</span><span id="line-865">      return null;</span>
<span class="source-line-no">866</span><span id="line-866">    }</span>
<span class="source-line-no">867</span><span id="line-867"></span>
<span class="source-line-no">868</span><span id="line-868">    LOG.debug(String.format("For class %s, using jar %s", my_class.getName(), jar));</span>
<span class="source-line-no">869</span><span id="line-869">    return new Path(jar).makeQualified(fs.getUri(), fs.getWorkingDirectory());</span>
<span class="source-line-no">870</span><span id="line-870">  }</span>
<span class="source-line-no">871</span><span id="line-871"></span>
<span class="source-line-no">872</span><span id="line-872">  /**</span>
<span class="source-line-no">873</span><span id="line-873">   * Add entries to &lt;code&gt;packagedClasses&lt;/code&gt; corresponding to class files contained in</span>
<span class="source-line-no">874</span><span id="line-874">   * &lt;code&gt;jar&lt;/code&gt;.</span>
<span class="source-line-no">875</span><span id="line-875">   * @param jar             The jar who's content to list.</span>
<span class="source-line-no">876</span><span id="line-876">   * @param packagedClasses map[class -&gt; jar]</span>
<span class="source-line-no">877</span><span id="line-877">   */</span>
<span class="source-line-no">878</span><span id="line-878">  private static void updateMap(String jar, Map&lt;String, String&gt; packagedClasses)</span>
<span class="source-line-no">879</span><span id="line-879">    throws IOException {</span>
<span class="source-line-no">880</span><span id="line-880">    if (null == jar || jar.isEmpty()) {</span>
<span class="source-line-no">881</span><span id="line-881">      return;</span>
<span class="source-line-no">882</span><span id="line-882">    }</span>
<span class="source-line-no">883</span><span id="line-883">    ZipFile zip = null;</span>
<span class="source-line-no">884</span><span id="line-884">    try {</span>
<span class="source-line-no">885</span><span id="line-885">      zip = new ZipFile(jar);</span>
<span class="source-line-no">886</span><span id="line-886">      for (Enumeration&lt;? extends ZipEntry&gt; iter = zip.entries(); iter.hasMoreElements();) {</span>
<span class="source-line-no">887</span><span id="line-887">        ZipEntry entry = iter.nextElement();</span>
<span class="source-line-no">888</span><span id="line-888">        if (entry.getName().endsWith("class")) {</span>
<span class="source-line-no">889</span><span id="line-889">          packagedClasses.put(entry.getName(), jar);</span>
<span class="source-line-no">890</span><span id="line-890">        }</span>
<span class="source-line-no">891</span><span id="line-891">      }</span>
<span class="source-line-no">892</span><span id="line-892">    } finally {</span>
<span class="source-line-no">893</span><span id="line-893">      if (null != zip) zip.close();</span>
<span class="source-line-no">894</span><span id="line-894">    }</span>
<span class="source-line-no">895</span><span id="line-895">  }</span>
<span class="source-line-no">896</span><span id="line-896"></span>
<span class="source-line-no">897</span><span id="line-897">  /**</span>
<span class="source-line-no">898</span><span id="line-898">   * Find a jar that contains a class of the same name, if any. It will return a jar file, even if</span>
<span class="source-line-no">899</span><span id="line-899">   * that is not the first thing on the class path that has a class with the same name. Looks first</span>
<span class="source-line-no">900</span><span id="line-900">   * on the classpath and then in the &lt;code&gt;packagedClasses&lt;/code&gt; map.</span>
<span class="source-line-no">901</span><span id="line-901">   * @param my_class the class to find.</span>
<span class="source-line-no">902</span><span id="line-902">   * @return a jar file that contains the class, or null.</span>
<span class="source-line-no">903</span><span id="line-903">   */</span>
<span class="source-line-no">904</span><span id="line-904">  private static String findContainingJar(Class&lt;?&gt; my_class, Map&lt;String, String&gt; packagedClasses)</span>
<span class="source-line-no">905</span><span id="line-905">    throws IOException {</span>
<span class="source-line-no">906</span><span id="line-906">    ClassLoader loader = my_class.getClassLoader();</span>
<span class="source-line-no">907</span><span id="line-907"></span>
<span class="source-line-no">908</span><span id="line-908">    String class_file = my_class.getName().replaceAll("\\.", "/") + ".class";</span>
<span class="source-line-no">909</span><span id="line-909"></span>
<span class="source-line-no">910</span><span id="line-910">    if (loader != null) {</span>
<span class="source-line-no">911</span><span id="line-911">      // first search the classpath</span>
<span class="source-line-no">912</span><span id="line-912">      for (Enumeration&lt;URL&gt; itr = loader.getResources(class_file); itr.hasMoreElements();) {</span>
<span class="source-line-no">913</span><span id="line-913">        URL url = itr.nextElement();</span>
<span class="source-line-no">914</span><span id="line-914">        if ("jar".equals(url.getProtocol())) {</span>
<span class="source-line-no">915</span><span id="line-915">          String toReturn = url.getPath();</span>
<span class="source-line-no">916</span><span id="line-916">          if (toReturn.startsWith("file:")) {</span>
<span class="source-line-no">917</span><span id="line-917">            toReturn = toReturn.substring("file:".length());</span>
<span class="source-line-no">918</span><span id="line-918">          }</span>
<span class="source-line-no">919</span><span id="line-919">          // URLDecoder is a misnamed class, since it actually decodes</span>
<span class="source-line-no">920</span><span id="line-920">          // x-www-form-urlencoded MIME type rather than actual</span>
<span class="source-line-no">921</span><span id="line-921">          // URL encoding (which the file path has). Therefore it would</span>
<span class="source-line-no">922</span><span id="line-922">          // decode +s to ' 's which is incorrect (spaces are actually</span>
<span class="source-line-no">923</span><span id="line-923">          // either unencoded or encoded as "%20"). Replace +s first, so</span>
<span class="source-line-no">924</span><span id="line-924">          // that they are kept sacred during the decoding process.</span>
<span class="source-line-no">925</span><span id="line-925">          toReturn = toReturn.replaceAll("\\+", "%2B");</span>
<span class="source-line-no">926</span><span id="line-926">          toReturn = URLDecoder.decode(toReturn, "UTF-8");</span>
<span class="source-line-no">927</span><span id="line-927">          return toReturn.replaceAll("!.*$", "");</span>
<span class="source-line-no">928</span><span id="line-928">        }</span>
<span class="source-line-no">929</span><span id="line-929">      }</span>
<span class="source-line-no">930</span><span id="line-930">    }</span>
<span class="source-line-no">931</span><span id="line-931"></span>
<span class="source-line-no">932</span><span id="line-932">    // now look in any jars we've packaged using JarFinder. Returns null when</span>
<span class="source-line-no">933</span><span id="line-933">    // no jar is found.</span>
<span class="source-line-no">934</span><span id="line-934">    return packagedClasses.get(class_file);</span>
<span class="source-line-no">935</span><span id="line-935">  }</span>
<span class="source-line-no">936</span><span id="line-936"></span>
<span class="source-line-no">937</span><span id="line-937">  /**</span>
<span class="source-line-no">938</span><span id="line-938">   * Invoke 'getJar' on a custom JarFinder implementation. Useful for some job configuration</span>
<span class="source-line-no">939</span><span id="line-939">   * contexts (HBASE-8140) and also for testing on MRv2. check if we have HADOOP-9426.</span>
<span class="source-line-no">940</span><span id="line-940">   * @param my_class the class to find.</span>
<span class="source-line-no">941</span><span id="line-941">   * @return a jar file that contains the class, or null.</span>
<span class="source-line-no">942</span><span id="line-942">   */</span>
<span class="source-line-no">943</span><span id="line-943">  private static String getJar(Class&lt;?&gt; my_class) {</span>
<span class="source-line-no">944</span><span id="line-944">    String ret = null;</span>
<span class="source-line-no">945</span><span id="line-945">    try {</span>
<span class="source-line-no">946</span><span id="line-946">      ret = JarFinder.getJar(my_class);</span>
<span class="source-line-no">947</span><span id="line-947">    } catch (Exception e) {</span>
<span class="source-line-no">948</span><span id="line-948">      // toss all other exceptions, related to reflection failure</span>
<span class="source-line-no">949</span><span id="line-949">      throw new RuntimeException("getJar invocation failed.", e);</span>
<span class="source-line-no">950</span><span id="line-950">    }</span>
<span class="source-line-no">951</span><span id="line-951"></span>
<span class="source-line-no">952</span><span id="line-952">    return ret;</span>
<span class="source-line-no">953</span><span id="line-953">  }</span>
<span class="source-line-no">954</span><span id="line-954"></span>
<span class="source-line-no">955</span><span id="line-955">  private static int getRegionCount(Configuration conf, TableName tableName) throws IOException {</span>
<span class="source-line-no">956</span><span id="line-956">    try (Connection conn = ConnectionFactory.createConnection(conf);</span>
<span class="source-line-no">957</span><span id="line-957">      RegionLocator locator = conn.getRegionLocator(tableName)) {</span>
<span class="source-line-no">958</span><span id="line-958">      return locator.getAllRegionLocations().size();</span>
<span class="source-line-no">959</span><span id="line-959">    }</span>
<span class="source-line-no">960</span><span id="line-960">  }</span>
<span class="source-line-no">961</span><span id="line-961">}</span>




























































</pre>
</div>
</main>
</body>
</html>
