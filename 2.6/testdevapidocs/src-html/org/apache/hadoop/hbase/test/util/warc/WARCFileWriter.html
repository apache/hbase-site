<!DOCTYPE HTML>
<html lang="en">
<head>
<!-- Generated by javadoc (17) -->
<title>Source code</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="source: package: org.apache.hadoop.hbase.test.util.warc, class: WARCFileWriter">
<meta name="generator" content="javadoc/SourceToHTMLConverter">
<link rel="stylesheet" type="text/css" href="../../../../../../../../stylesheet.css" title="Style">
</head>
<body class="source-page">
<main role="main">
<div class="source-container">
<pre><span class="source-line-no">001</span><span id="line-1">/*</span>
<span class="source-line-no">002</span><span id="line-2"> * Licensed to the Apache Software Foundation (ASF) under one</span>
<span class="source-line-no">003</span><span id="line-3"> * or more contributor license agreements.  See the NOTICE file</span>
<span class="source-line-no">004</span><span id="line-4"> * distributed with this work for additional information</span>
<span class="source-line-no">005</span><span id="line-5"> * regarding copyright ownership.  The ASF licenses this file</span>
<span class="source-line-no">006</span><span id="line-6"> * to you under the Apache License, Version 2.0 (the</span>
<span class="source-line-no">007</span><span id="line-7"> * "License"); you may not use this file except in compliance</span>
<span class="source-line-no">008</span><span id="line-8"> * with the License.  You may obtain a copy of the License at</span>
<span class="source-line-no">009</span><span id="line-9"> *</span>
<span class="source-line-no">010</span><span id="line-10"> *     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="source-line-no">011</span><span id="line-11"> *</span>
<span class="source-line-no">012</span><span id="line-12"> * Unless required by applicable law or agreed to in writing, software</span>
<span class="source-line-no">013</span><span id="line-13"> * distributed under the License is distributed on an "AS IS" BASIS,</span>
<span class="source-line-no">014</span><span id="line-14"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="source-line-no">015</span><span id="line-15"> * See the License for the specific language governing permissions and</span>
<span class="source-line-no">016</span><span id="line-16"> * limitations under the License.</span>
<span class="source-line-no">017</span><span id="line-17"> */</span>
<span class="source-line-no">018</span><span id="line-18">/*</span>
<span class="source-line-no">019</span><span id="line-19"> * The MIT License (MIT)</span>
<span class="source-line-no">020</span><span id="line-20"> * Copyright (c) 2014 Martin Kleppmann</span>
<span class="source-line-no">021</span><span id="line-21"> *</span>
<span class="source-line-no">022</span><span id="line-22"> * Permission is hereby granted, free of charge, to any person obtaining a copy</span>
<span class="source-line-no">023</span><span id="line-23"> * of this software and associated documentation files (the "Software"), to deal</span>
<span class="source-line-no">024</span><span id="line-24"> * in the Software without restriction, including without limitation the rights</span>
<span class="source-line-no">025</span><span id="line-25"> * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell</span>
<span class="source-line-no">026</span><span id="line-26"> * copies of the Software, and to permit persons to whom the Software is</span>
<span class="source-line-no">027</span><span id="line-27"> * furnished to do so, subject to the following conditions:</span>
<span class="source-line-no">028</span><span id="line-28"> *</span>
<span class="source-line-no">029</span><span id="line-29"> * The above copyright notice and this permission notice shall be included in</span>
<span class="source-line-no">030</span><span id="line-30"> * all copies or substantial portions of the Software.</span>
<span class="source-line-no">031</span><span id="line-31"> *</span>
<span class="source-line-no">032</span><span id="line-32"> * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR</span>
<span class="source-line-no">033</span><span id="line-33"> * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,</span>
<span class="source-line-no">034</span><span id="line-34"> * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE</span>
<span class="source-line-no">035</span><span id="line-35"> * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER</span>
<span class="source-line-no">036</span><span id="line-36"> * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,</span>
<span class="source-line-no">037</span><span id="line-37"> * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN</span>
<span class="source-line-no">038</span><span id="line-38"> * THE SOFTWARE.</span>
<span class="source-line-no">039</span><span id="line-39"> */</span>
<span class="source-line-no">040</span><span id="line-40">package org.apache.hadoop.hbase.test.util.warc;</span>
<span class="source-line-no">041</span><span id="line-41"></span>
<span class="source-line-no">042</span><span id="line-42">import java.io.BufferedOutputStream;</span>
<span class="source-line-no">043</span><span id="line-43">import java.io.DataOutputStream;</span>
<span class="source-line-no">044</span><span id="line-44">import java.io.FilterOutputStream;</span>
<span class="source-line-no">045</span><span id="line-45">import java.io.IOException;</span>
<span class="source-line-no">046</span><span id="line-46">import java.io.OutputStream;</span>
<span class="source-line-no">047</span><span id="line-47">import org.apache.hadoop.conf.Configuration;</span>
<span class="source-line-no">048</span><span id="line-48">import org.apache.hadoop.fs.FSDataOutputStream;</span>
<span class="source-line-no">049</span><span id="line-49">import org.apache.hadoop.fs.FileSystem;</span>
<span class="source-line-no">050</span><span id="line-50">import org.apache.hadoop.fs.Path;</span>
<span class="source-line-no">051</span><span id="line-51">import org.apache.hadoop.io.compress.CompressionCodec;</span>
<span class="source-line-no">052</span><span id="line-52">import org.apache.hadoop.util.Progressable;</span>
<span class="source-line-no">053</span><span id="line-53">import org.apache.hadoop.util.ReflectionUtils;</span>
<span class="source-line-no">054</span><span id="line-54">import org.slf4j.Logger;</span>
<span class="source-line-no">055</span><span id="line-55">import org.slf4j.LoggerFactory;</span>
<span class="source-line-no">056</span><span id="line-56"></span>
<span class="source-line-no">057</span><span id="line-57">/**</span>
<span class="source-line-no">058</span><span id="line-58"> * Writes {@link WARCRecord}s to a WARC file, using Hadoop's filesystem APIs. (This means you can</span>
<span class="source-line-no">059</span><span id="line-59"> * write to HDFS, S3 or any other filesystem supported by Hadoop). This implementation is not tied</span>
<span class="source-line-no">060</span><span id="line-60"> * to the MapReduce APIs -- that link is provided by the mapred</span>
<span class="source-line-no">061</span><span id="line-61"> * {@link com.martinkl.warc.mapred.WARCOutputFormat} and the mapreduce</span>
<span class="source-line-no">062</span><span id="line-62"> * {@link com.martinkl.warc.mapreduce.WARCOutputFormat}. WARCFileWriter keeps track of how much data</span>
<span class="source-line-no">063</span><span id="line-63"> * it has written (optionally gzip-compressed); when the file becomes larger than some threshold, it</span>
<span class="source-line-no">064</span><span id="line-64"> * is automatically closed and a new segment is started. A segment number is appended to the</span>
<span class="source-line-no">065</span><span id="line-65"> * filename for that purpose. The segment number always starts at 00000, and by default a new</span>
<span class="source-line-no">066</span><span id="line-66"> * segment is started when the file size exceeds 1GB. To change the target size for a segment, you</span>
<span class="source-line-no">067</span><span id="line-67"> * can set the `warc.output.segment.size` key in the Hadoop configuration to the number of bytes.</span>
<span class="source-line-no">068</span><span id="line-68"> * (Files may actually be a bit larger than this threshold, since we finish writing the current</span>
<span class="source-line-no">069</span><span id="line-69"> * record before opening a new file.)</span>
<span class="source-line-no">070</span><span id="line-70"> */</span>
<span class="source-line-no">071</span><span id="line-71">public class WARCFileWriter {</span>
<span class="source-line-no">072</span><span id="line-72">  private static final Logger logger = LoggerFactory.getLogger(WARCFileWriter.class);</span>
<span class="source-line-no">073</span><span id="line-73">  public static final long DEFAULT_MAX_SEGMENT_SIZE = 1000000000L; // 1 GB</span>
<span class="source-line-no">074</span><span id="line-74"></span>
<span class="source-line-no">075</span><span id="line-75">  private final Configuration conf;</span>
<span class="source-line-no">076</span><span id="line-76">  private final CompressionCodec codec;</span>
<span class="source-line-no">077</span><span id="line-77">  private final Path workOutputPath;</span>
<span class="source-line-no">078</span><span id="line-78">  private final Progressable progress;</span>
<span class="source-line-no">079</span><span id="line-79">  private final String extensionFormat;</span>
<span class="source-line-no">080</span><span id="line-80">  private final long maxSegmentSize;</span>
<span class="source-line-no">081</span><span id="line-81">  private long segmentsCreated = 0, segmentsAttempted = 0, bytesWritten = 0;</span>
<span class="source-line-no">082</span><span id="line-82">  private CountingOutputStream byteStream;</span>
<span class="source-line-no">083</span><span id="line-83">  private DataOutputStream dataStream;</span>
<span class="source-line-no">084</span><span id="line-84"></span>
<span class="source-line-no">085</span><span id="line-85">  /**</span>
<span class="source-line-no">086</span><span id="line-86">   * Creates a WARC file, and opens it for writing. If a file with the same name already exists, an</span>
<span class="source-line-no">087</span><span id="line-87">   * attempt number in the filename is incremented until we find a file that doesn't already exist.</span>
<span class="source-line-no">088</span><span id="line-88">   * @param conf           The Hadoop configuration.</span>
<span class="source-line-no">089</span><span id="line-89">   * @param codec          If null, the file is uncompressed. If non-null, this compression codec</span>
<span class="source-line-no">090</span><span id="line-90">   *                       will be used. The codec's default file extension is appended to the</span>
<span class="source-line-no">091</span><span id="line-91">   *                       filename.</span>
<span class="source-line-no">092</span><span id="line-92">   * @param workOutputPath The directory and filename prefix to which the data should be written. We</span>
<span class="source-line-no">093</span><span id="line-93">   *                       append a segment number and filename extensions to it.</span>
<span class="source-line-no">094</span><span id="line-94">   */</span>
<span class="source-line-no">095</span><span id="line-95">  public WARCFileWriter(Configuration conf, CompressionCodec codec, Path workOutputPath)</span>
<span class="source-line-no">096</span><span id="line-96">    throws IOException {</span>
<span class="source-line-no">097</span><span id="line-97">    this(conf, codec, workOutputPath, null);</span>
<span class="source-line-no">098</span><span id="line-98">  }</span>
<span class="source-line-no">099</span><span id="line-99"></span>
<span class="source-line-no">100</span><span id="line-100">  /**</span>
<span class="source-line-no">101</span><span id="line-101">   * Creates a WARC file, and opens it for writing. If a file with the same name already exists, it</span>
<span class="source-line-no">102</span><span id="line-102">   * is *overwritten*. Note that this is different behaviour from the other constructor. Yes, this</span>
<span class="source-line-no">103</span><span id="line-103">   * sucks. It will probably change in a future version.</span>
<span class="source-line-no">104</span><span id="line-104">   * @param conf           The Hadoop configuration.</span>
<span class="source-line-no">105</span><span id="line-105">   * @param codec          If null, the file is uncompressed. If non-null, this compression codec</span>
<span class="source-line-no">106</span><span id="line-106">   *                       will be used. The codec's default file extension is appended to the</span>
<span class="source-line-no">107</span><span id="line-107">   *                       filename.</span>
<span class="source-line-no">108</span><span id="line-108">   * @param workOutputPath The directory and filename prefix to which the data should be written. We</span>
<span class="source-line-no">109</span><span id="line-109">   *                       append a segment number and filename extensions to it.</span>
<span class="source-line-no">110</span><span id="line-110">   * @param progress       An object used by the mapred API for tracking a task's progress.</span>
<span class="source-line-no">111</span><span id="line-111">   */</span>
<span class="source-line-no">112</span><span id="line-112">  public WARCFileWriter(Configuration conf, CompressionCodec codec, Path workOutputPath,</span>
<span class="source-line-no">113</span><span id="line-113">    Progressable progress) throws IOException {</span>
<span class="source-line-no">114</span><span id="line-114">    this.conf = conf;</span>
<span class="source-line-no">115</span><span id="line-115">    this.codec = codec;</span>
<span class="source-line-no">116</span><span id="line-116">    this.workOutputPath = workOutputPath;</span>
<span class="source-line-no">117</span><span id="line-117">    this.progress = progress;</span>
<span class="source-line-no">118</span><span id="line-118">    this.extensionFormat =</span>
<span class="source-line-no">119</span><span id="line-119">      ".seg-%05d.attempt-%05d.warc" + (codec == null ? "" : codec.getDefaultExtension());</span>
<span class="source-line-no">120</span><span id="line-120">    this.maxSegmentSize = conf.getLong("warc.output.segment.size", DEFAULT_MAX_SEGMENT_SIZE);</span>
<span class="source-line-no">121</span><span id="line-121">    createSegment();</span>
<span class="source-line-no">122</span><span id="line-122">  }</span>
<span class="source-line-no">123</span><span id="line-123"></span>
<span class="source-line-no">124</span><span id="line-124">  /**</span>
<span class="source-line-no">125</span><span id="line-125">   * Instantiates a Hadoop codec for compressing and decompressing Gzip files. This is the most</span>
<span class="source-line-no">126</span><span id="line-126">   * common compression applied to WARC files.</span>
<span class="source-line-no">127</span><span id="line-127">   * @param conf The Hadoop configuration.</span>
<span class="source-line-no">128</span><span id="line-128">   */</span>
<span class="source-line-no">129</span><span id="line-129">  public static CompressionCodec getGzipCodec(Configuration conf) {</span>
<span class="source-line-no">130</span><span id="line-130">    try {</span>
<span class="source-line-no">131</span><span id="line-131">      return (CompressionCodec) ReflectionUtils</span>
<span class="source-line-no">132</span><span id="line-132">        .newInstance(conf.getClassByName("org.apache.hadoop.io.compress.GzipCodec")</span>
<span class="source-line-no">133</span><span id="line-133">          .asSubclass(CompressionCodec.class), conf);</span>
<span class="source-line-no">134</span><span id="line-134">    } catch (ClassNotFoundException e) {</span>
<span class="source-line-no">135</span><span id="line-135">      logger.warn("GzipCodec could not be instantiated", e);</span>
<span class="source-line-no">136</span><span id="line-136">      return null;</span>
<span class="source-line-no">137</span><span id="line-137">    }</span>
<span class="source-line-no">138</span><span id="line-138">  }</span>
<span class="source-line-no">139</span><span id="line-139"></span>
<span class="source-line-no">140</span><span id="line-140">  /**</span>
<span class="source-line-no">141</span><span id="line-141">   * Creates an output segment file and sets up the output streams to point at it. If the file</span>
<span class="source-line-no">142</span><span id="line-142">   * already exists, retries with a different filename. This is a bit nasty -- after all,</span>
<span class="source-line-no">143</span><span id="line-143">   * {@link FileOutputFormat}'s work directory concept is supposed to prevent filename clashes --</span>
<span class="source-line-no">144</span><span id="line-144">   * but it looks like Amazon Elastic MapReduce prevents use of per-task work directories if the</span>
<span class="source-line-no">145</span><span id="line-145">   * output of a job is on S3. TODO: Investigate this and find a better solution.</span>
<span class="source-line-no">146</span><span id="line-146">   */</span>
<span class="source-line-no">147</span><span id="line-147">  private void createSegment() throws IOException {</span>
<span class="source-line-no">148</span><span id="line-148">    segmentsAttempted = 0;</span>
<span class="source-line-no">149</span><span id="line-149">    bytesWritten = 0;</span>
<span class="source-line-no">150</span><span id="line-150">    boolean success = false;</span>
<span class="source-line-no">151</span><span id="line-151"></span>
<span class="source-line-no">152</span><span id="line-152">    while (!success) {</span>
<span class="source-line-no">153</span><span id="line-153">      Path path =</span>
<span class="source-line-no">154</span><span id="line-154">        workOutputPath.suffix(String.format(extensionFormat, segmentsCreated, segmentsAttempted));</span>
<span class="source-line-no">155</span><span id="line-155">      FileSystem fs = path.getFileSystem(conf);</span>
<span class="source-line-no">156</span><span id="line-156"></span>
<span class="source-line-no">157</span><span id="line-157">      try {</span>
<span class="source-line-no">158</span><span id="line-158">        // The o.a.h.mapred OutputFormats overwrite existing files, whereas</span>
<span class="source-line-no">159</span><span id="line-159">        // the o.a.h.mapreduce OutputFormats don't overwrite. Bizarre...</span>
<span class="source-line-no">160</span><span id="line-160">        // Here, overwrite if progress != null, i.e. if using mapred API.</span>
<span class="source-line-no">161</span><span id="line-161">        FSDataOutputStream fsStream =</span>
<span class="source-line-no">162</span><span id="line-162">          (progress == null) ? fs.create(path, false) : fs.create(path, progress);</span>
<span class="source-line-no">163</span><span id="line-163">        byteStream = new CountingOutputStream(new BufferedOutputStream(fsStream));</span>
<span class="source-line-no">164</span><span id="line-164">        dataStream =</span>
<span class="source-line-no">165</span><span id="line-165">          new DataOutputStream(codec == null ? byteStream : codec.createOutputStream(byteStream));</span>
<span class="source-line-no">166</span><span id="line-166">        segmentsCreated++;</span>
<span class="source-line-no">167</span><span id="line-167">        logger.info("Writing to output file: {}", path);</span>
<span class="source-line-no">168</span><span id="line-168">        success = true;</span>
<span class="source-line-no">169</span><span id="line-169"></span>
<span class="source-line-no">170</span><span id="line-170">      } catch (IOException e) {</span>
<span class="source-line-no">171</span><span id="line-171">        if (e.getMessage().startsWith("File already exists")) {</span>
<span class="source-line-no">172</span><span id="line-172">          logger.warn("Tried to create file {} but it already exists; retrying.", path);</span>
<span class="source-line-no">173</span><span id="line-173">          segmentsAttempted++; // retry</span>
<span class="source-line-no">174</span><span id="line-174">        } else {</span>
<span class="source-line-no">175</span><span id="line-175">          throw e;</span>
<span class="source-line-no">176</span><span id="line-176">        }</span>
<span class="source-line-no">177</span><span id="line-177">      }</span>
<span class="source-line-no">178</span><span id="line-178">    }</span>
<span class="source-line-no">179</span><span id="line-179">  }</span>
<span class="source-line-no">180</span><span id="line-180"></span>
<span class="source-line-no">181</span><span id="line-181">  /**</span>
<span class="source-line-no">182</span><span id="line-182">   * Appends a {@link WARCRecord} to the file, in WARC/1.0 format.</span>
<span class="source-line-no">183</span><span id="line-183">   * @param record The record to be written.</span>
<span class="source-line-no">184</span><span id="line-184">   */</span>
<span class="source-line-no">185</span><span id="line-185">  public void write(WARCRecord record) throws IOException {</span>
<span class="source-line-no">186</span><span id="line-186">    if (bytesWritten &gt; maxSegmentSize) {</span>
<span class="source-line-no">187</span><span id="line-187">      dataStream.close();</span>
<span class="source-line-no">188</span><span id="line-188">      createSegment();</span>
<span class="source-line-no">189</span><span id="line-189">    }</span>
<span class="source-line-no">190</span><span id="line-190">    record.write(dataStream);</span>
<span class="source-line-no">191</span><span id="line-191">  }</span>
<span class="source-line-no">192</span><span id="line-192"></span>
<span class="source-line-no">193</span><span id="line-193">  /**</span>
<span class="source-line-no">194</span><span id="line-194">   * Appends a {@link WARCRecord} wrapped in a {@link WARCWritable} to the file.</span>
<span class="source-line-no">195</span><span id="line-195">   * @param record The wrapper around the record to be written.</span>
<span class="source-line-no">196</span><span id="line-196">   */</span>
<span class="source-line-no">197</span><span id="line-197">  public void write(WARCWritable record) throws IOException {</span>
<span class="source-line-no">198</span><span id="line-198">    if (record.getRecord() != null) {</span>
<span class="source-line-no">199</span><span id="line-199">      write(record.getRecord());</span>
<span class="source-line-no">200</span><span id="line-200">    }</span>
<span class="source-line-no">201</span><span id="line-201">  }</span>
<span class="source-line-no">202</span><span id="line-202"></span>
<span class="source-line-no">203</span><span id="line-203">  /**</span>
<span class="source-line-no">204</span><span id="line-204">   * Flushes any buffered data and closes the file.</span>
<span class="source-line-no">205</span><span id="line-205">   */</span>
<span class="source-line-no">206</span><span id="line-206">  public void close() throws IOException {</span>
<span class="source-line-no">207</span><span id="line-207">    dataStream.close();</span>
<span class="source-line-no">208</span><span id="line-208">  }</span>
<span class="source-line-no">209</span><span id="line-209"></span>
<span class="source-line-no">210</span><span id="line-210">  private class CountingOutputStream extends FilterOutputStream {</span>
<span class="source-line-no">211</span><span id="line-211">    public CountingOutputStream(OutputStream out) {</span>
<span class="source-line-no">212</span><span id="line-212">      super(out);</span>
<span class="source-line-no">213</span><span id="line-213">    }</span>
<span class="source-line-no">214</span><span id="line-214"></span>
<span class="source-line-no">215</span><span id="line-215">    @Override</span>
<span class="source-line-no">216</span><span id="line-216">    public void write(byte[] b, int off, int len) throws IOException {</span>
<span class="source-line-no">217</span><span id="line-217">      out.write(b, off, len);</span>
<span class="source-line-no">218</span><span id="line-218">      bytesWritten += len;</span>
<span class="source-line-no">219</span><span id="line-219">    }</span>
<span class="source-line-no">220</span><span id="line-220"></span>
<span class="source-line-no">221</span><span id="line-221">    @Override</span>
<span class="source-line-no">222</span><span id="line-222">    public void write(int b) throws IOException {</span>
<span class="source-line-no">223</span><span id="line-223">      out.write(b);</span>
<span class="source-line-no">224</span><span id="line-224">      bytesWritten++;</span>
<span class="source-line-no">225</span><span id="line-225">    }</span>
<span class="source-line-no">226</span><span id="line-226"></span>
<span class="source-line-no">227</span><span id="line-227">    // Overriding close() because FilterOutputStream's close() method pre-JDK8 has bad behavior:</span>
<span class="source-line-no">228</span><span id="line-228">    // it silently ignores any exception thrown by flush(). Instead, just close the delegate</span>
<span class="source-line-no">229</span><span id="line-229">    // stream. It should flush itself if necessary. (Thanks to the Guava project for noticing</span>
<span class="source-line-no">230</span><span id="line-230">    // this.)</span>
<span class="source-line-no">231</span><span id="line-231">    @Override</span>
<span class="source-line-no">232</span><span id="line-232">    public void close() throws IOException {</span>
<span class="source-line-no">233</span><span id="line-233">      out.close();</span>
<span class="source-line-no">234</span><span id="line-234">    }</span>
<span class="source-line-no">235</span><span id="line-235">  }</span>
<span class="source-line-no">236</span><span id="line-236"></span>
<span class="source-line-no">237</span><span id="line-237">}</span>




























































</pre>
</div>
</main>
</body>
</html>
