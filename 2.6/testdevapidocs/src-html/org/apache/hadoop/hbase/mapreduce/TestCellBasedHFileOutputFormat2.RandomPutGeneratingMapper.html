<!DOCTYPE HTML>
<html lang="en">
<head>
<!-- Generated by javadoc (17) -->
<title>Source code</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="source: package: org.apache.hadoop.hbase.mapreduce, class: TestCellBasedHFileOutputFormat2, class: RandomPutGeneratingMapper">
<meta name="generator" content="javadoc/SourceToHTMLConverter">
<link rel="stylesheet" type="text/css" href="../../../../../../stylesheet.css" title="Style">
</head>
<body class="source-page">
<main role="main">
<div class="source-container">
<pre><span class="source-line-no">001</span><span id="line-1">/*</span>
<span class="source-line-no">002</span><span id="line-2"> * Licensed to the Apache Software Foundation (ASF) under one</span>
<span class="source-line-no">003</span><span id="line-3"> * or more contributor license agreements.  See the NOTICE file</span>
<span class="source-line-no">004</span><span id="line-4"> * distributed with this work for additional information</span>
<span class="source-line-no">005</span><span id="line-5"> * regarding copyright ownership.  The ASF licenses this file</span>
<span class="source-line-no">006</span><span id="line-6"> * to you under the Apache License, Version 2.0 (the</span>
<span class="source-line-no">007</span><span id="line-7"> * "License"); you may not use this file except in compliance</span>
<span class="source-line-no">008</span><span id="line-8"> * with the License.  You may obtain a copy of the License at</span>
<span class="source-line-no">009</span><span id="line-9"> *</span>
<span class="source-line-no">010</span><span id="line-10"> *     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="source-line-no">011</span><span id="line-11"> *</span>
<span class="source-line-no">012</span><span id="line-12"> * Unless required by applicable law or agreed to in writing, software</span>
<span class="source-line-no">013</span><span id="line-13"> * distributed under the License is distributed on an "AS IS" BASIS,</span>
<span class="source-line-no">014</span><span id="line-14"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="source-line-no">015</span><span id="line-15"> * See the License for the specific language governing permissions and</span>
<span class="source-line-no">016</span><span id="line-16"> * limitations under the License.</span>
<span class="source-line-no">017</span><span id="line-17"> */</span>
<span class="source-line-no">018</span><span id="line-18">package org.apache.hadoop.hbase.mapreduce;</span>
<span class="source-line-no">019</span><span id="line-19"></span>
<span class="source-line-no">020</span><span id="line-20">import static org.apache.hadoop.hbase.regionserver.HStoreFile.BLOOM_FILTER_TYPE_KEY;</span>
<span class="source-line-no">021</span><span id="line-21">import static org.junit.Assert.assertEquals;</span>
<span class="source-line-no">022</span><span id="line-22">import static org.junit.Assert.assertFalse;</span>
<span class="source-line-no">023</span><span id="line-23">import static org.junit.Assert.assertNotNull;</span>
<span class="source-line-no">024</span><span id="line-24">import static org.junit.Assert.assertNotSame;</span>
<span class="source-line-no">025</span><span id="line-25">import static org.junit.Assert.assertTrue;</span>
<span class="source-line-no">026</span><span id="line-26">import static org.junit.Assert.fail;</span>
<span class="source-line-no">027</span><span id="line-27"></span>
<span class="source-line-no">028</span><span id="line-28">import java.io.IOException;</span>
<span class="source-line-no">029</span><span id="line-29">import java.lang.reflect.Field;</span>
<span class="source-line-no">030</span><span id="line-30">import java.util.ArrayList;</span>
<span class="source-line-no">031</span><span id="line-31">import java.util.Arrays;</span>
<span class="source-line-no">032</span><span id="line-32">import java.util.HashMap;</span>
<span class="source-line-no">033</span><span id="line-33">import java.util.List;</span>
<span class="source-line-no">034</span><span id="line-34">import java.util.Map;</span>
<span class="source-line-no">035</span><span id="line-35">import java.util.Map.Entry;</span>
<span class="source-line-no">036</span><span id="line-36">import java.util.Random;</span>
<span class="source-line-no">037</span><span id="line-37">import java.util.Set;</span>
<span class="source-line-no">038</span><span id="line-38">import java.util.concurrent.Callable;</span>
<span class="source-line-no">039</span><span id="line-39">import java.util.stream.Collectors;</span>
<span class="source-line-no">040</span><span id="line-40">import java.util.stream.Stream;</span>
<span class="source-line-no">041</span><span id="line-41">import org.apache.hadoop.conf.Configuration;</span>
<span class="source-line-no">042</span><span id="line-42">import org.apache.hadoop.fs.FileStatus;</span>
<span class="source-line-no">043</span><span id="line-43">import org.apache.hadoop.fs.FileSystem;</span>
<span class="source-line-no">044</span><span id="line-44">import org.apache.hadoop.fs.LocatedFileStatus;</span>
<span class="source-line-no">045</span><span id="line-45">import org.apache.hadoop.fs.Path;</span>
<span class="source-line-no">046</span><span id="line-46">import org.apache.hadoop.fs.RemoteIterator;</span>
<span class="source-line-no">047</span><span id="line-47">import org.apache.hadoop.hbase.ArrayBackedTag;</span>
<span class="source-line-no">048</span><span id="line-48">import org.apache.hadoop.hbase.Cell;</span>
<span class="source-line-no">049</span><span id="line-49">import org.apache.hadoop.hbase.CellUtil;</span>
<span class="source-line-no">050</span><span id="line-50">import org.apache.hadoop.hbase.CompatibilitySingletonFactory;</span>
<span class="source-line-no">051</span><span id="line-51">import org.apache.hadoop.hbase.HBaseClassTestRule;</span>
<span class="source-line-no">052</span><span id="line-52">import org.apache.hadoop.hbase.HBaseConfiguration;</span>
<span class="source-line-no">053</span><span id="line-53">import org.apache.hadoop.hbase.HBaseTestingUtility;</span>
<span class="source-line-no">054</span><span id="line-54">import org.apache.hadoop.hbase.HColumnDescriptor;</span>
<span class="source-line-no">055</span><span id="line-55">import org.apache.hadoop.hbase.HConstants;</span>
<span class="source-line-no">056</span><span id="line-56">import org.apache.hadoop.hbase.HDFSBlocksDistribution;</span>
<span class="source-line-no">057</span><span id="line-57">import org.apache.hadoop.hbase.HTableDescriptor;</span>
<span class="source-line-no">058</span><span id="line-58">import org.apache.hadoop.hbase.HadoopShims;</span>
<span class="source-line-no">059</span><span id="line-59">import org.apache.hadoop.hbase.KeyValue;</span>
<span class="source-line-no">060</span><span id="line-60">import org.apache.hadoop.hbase.PerformanceEvaluation;</span>
<span class="source-line-no">061</span><span id="line-61">import org.apache.hadoop.hbase.TableName;</span>
<span class="source-line-no">062</span><span id="line-62">import org.apache.hadoop.hbase.Tag;</span>
<span class="source-line-no">063</span><span id="line-63">import org.apache.hadoop.hbase.TagType;</span>
<span class="source-line-no">064</span><span id="line-64">import org.apache.hadoop.hbase.TagUtil;</span>
<span class="source-line-no">065</span><span id="line-65">import org.apache.hadoop.hbase.client.Admin;</span>
<span class="source-line-no">066</span><span id="line-66">import org.apache.hadoop.hbase.client.Connection;</span>
<span class="source-line-no">067</span><span id="line-67">import org.apache.hadoop.hbase.client.ConnectionFactory;</span>
<span class="source-line-no">068</span><span id="line-68">import org.apache.hadoop.hbase.client.Put;</span>
<span class="source-line-no">069</span><span id="line-69">import org.apache.hadoop.hbase.client.RegionLocator;</span>
<span class="source-line-no">070</span><span id="line-70">import org.apache.hadoop.hbase.client.Result;</span>
<span class="source-line-no">071</span><span id="line-71">import org.apache.hadoop.hbase.client.ResultScanner;</span>
<span class="source-line-no">072</span><span id="line-72">import org.apache.hadoop.hbase.client.Scan;</span>
<span class="source-line-no">073</span><span id="line-73">import org.apache.hadoop.hbase.client.Table;</span>
<span class="source-line-no">074</span><span id="line-74">import org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span>
<span class="source-line-no">075</span><span id="line-75">import org.apache.hadoop.hbase.io.compress.Compression;</span>
<span class="source-line-no">076</span><span id="line-76">import org.apache.hadoop.hbase.io.compress.Compression.Algorithm;</span>
<span class="source-line-no">077</span><span id="line-77">import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;</span>
<span class="source-line-no">078</span><span id="line-78">import org.apache.hadoop.hbase.io.hfile.CacheConfig;</span>
<span class="source-line-no">079</span><span id="line-79">import org.apache.hadoop.hbase.io.hfile.HFile;</span>
<span class="source-line-no">080</span><span id="line-80">import org.apache.hadoop.hbase.io.hfile.HFile.Reader;</span>
<span class="source-line-no">081</span><span id="line-81">import org.apache.hadoop.hbase.io.hfile.HFileScanner;</span>
<span class="source-line-no">082</span><span id="line-82">import org.apache.hadoop.hbase.regionserver.BloomType;</span>
<span class="source-line-no">083</span><span id="line-83">import org.apache.hadoop.hbase.regionserver.HRegion;</span>
<span class="source-line-no">084</span><span id="line-84">import org.apache.hadoop.hbase.regionserver.HStore;</span>
<span class="source-line-no">085</span><span id="line-85">import org.apache.hadoop.hbase.regionserver.TestHRegionFileSystem;</span>
<span class="source-line-no">086</span><span id="line-86">import org.apache.hadoop.hbase.regionserver.TimeRangeTracker;</span>
<span class="source-line-no">087</span><span id="line-87">import org.apache.hadoop.hbase.testclassification.LargeTests;</span>
<span class="source-line-no">088</span><span id="line-88">import org.apache.hadoop.hbase.testclassification.VerySlowMapReduceTests;</span>
<span class="source-line-no">089</span><span id="line-89">import org.apache.hadoop.hbase.tool.LoadIncrementalHFiles;</span>
<span class="source-line-no">090</span><span id="line-90">import org.apache.hadoop.hbase.util.Bytes;</span>
<span class="source-line-no">091</span><span id="line-91">import org.apache.hadoop.hbase.util.CommonFSUtils;</span>
<span class="source-line-no">092</span><span id="line-92">import org.apache.hadoop.hbase.util.FSUtils;</span>
<span class="source-line-no">093</span><span id="line-93">import org.apache.hadoop.hbase.util.ReflectionUtils;</span>
<span class="source-line-no">094</span><span id="line-94">import org.apache.hadoop.hdfs.DistributedFileSystem;</span>
<span class="source-line-no">095</span><span id="line-95">import org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;</span>
<span class="source-line-no">096</span><span id="line-96">import org.apache.hadoop.hdfs.protocol.HdfsFileStatus;</span>
<span class="source-line-no">097</span><span id="line-97">import org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite;</span>
<span class="source-line-no">098</span><span id="line-98">import org.apache.hadoop.io.NullWritable;</span>
<span class="source-line-no">099</span><span id="line-99">import org.apache.hadoop.mapreduce.Job;</span>
<span class="source-line-no">100</span><span id="line-100">import org.apache.hadoop.mapreduce.Mapper;</span>
<span class="source-line-no">101</span><span id="line-101">import org.apache.hadoop.mapreduce.RecordWriter;</span>
<span class="source-line-no">102</span><span id="line-102">import org.apache.hadoop.mapreduce.TaskAttemptContext;</span>
<span class="source-line-no">103</span><span id="line-103">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span>
<span class="source-line-no">104</span><span id="line-104">import org.junit.ClassRule;</span>
<span class="source-line-no">105</span><span id="line-105">import org.junit.Ignore;</span>
<span class="source-line-no">106</span><span id="line-106">import org.junit.Test;</span>
<span class="source-line-no">107</span><span id="line-107">import org.junit.experimental.categories.Category;</span>
<span class="source-line-no">108</span><span id="line-108">import org.mockito.Mockito;</span>
<span class="source-line-no">109</span><span id="line-109">import org.slf4j.Logger;</span>
<span class="source-line-no">110</span><span id="line-110">import org.slf4j.LoggerFactory;</span>
<span class="source-line-no">111</span><span id="line-111"></span>
<span class="source-line-no">112</span><span id="line-112">/**</span>
<span class="source-line-no">113</span><span id="line-113"> * Simple test for {@link HFileOutputFormat2}. Sets up and runs a mapreduce job that writes hfile</span>
<span class="source-line-no">114</span><span id="line-114"> * output. Creates a few inner classes to implement splits and an inputformat that emits keys and</span>
<span class="source-line-no">115</span><span id="line-115"> * values like those of {@link PerformanceEvaluation}.</span>
<span class="source-line-no">116</span><span id="line-116"> */</span>
<span class="source-line-no">117</span><span id="line-117">@Category({ VerySlowMapReduceTests.class, LargeTests.class })</span>
<span class="source-line-no">118</span><span id="line-118">public class TestCellBasedHFileOutputFormat2 {</span>
<span class="source-line-no">119</span><span id="line-119"></span>
<span class="source-line-no">120</span><span id="line-120">  @ClassRule</span>
<span class="source-line-no">121</span><span id="line-121">  public static final HBaseClassTestRule CLASS_RULE =</span>
<span class="source-line-no">122</span><span id="line-122">    HBaseClassTestRule.forClass(TestCellBasedHFileOutputFormat2.class);</span>
<span class="source-line-no">123</span><span id="line-123"></span>
<span class="source-line-no">124</span><span id="line-124">  private final static int ROWSPERSPLIT = 1024;</span>
<span class="source-line-no">125</span><span id="line-125"></span>
<span class="source-line-no">126</span><span id="line-126">  public static final byte[] FAMILY_NAME = TestHRegionFileSystem.FAMILY_NAME;</span>
<span class="source-line-no">127</span><span id="line-127">  private static final byte[][] FAMILIES =</span>
<span class="source-line-no">128</span><span id="line-128">    { Bytes.add(FAMILY_NAME, Bytes.toBytes("-A")), Bytes.add(FAMILY_NAME, Bytes.toBytes("-B")) };</span>
<span class="source-line-no">129</span><span id="line-129">  private static final TableName[] TABLE_NAMES = Stream.of("TestTable", "TestTable2", "TestTable3")</span>
<span class="source-line-no">130</span><span id="line-130">    .map(TableName::valueOf).toArray(TableName[]::new);</span>
<span class="source-line-no">131</span><span id="line-131"></span>
<span class="source-line-no">132</span><span id="line-132">  private HBaseTestingUtility util = new HBaseTestingUtility();</span>
<span class="source-line-no">133</span><span id="line-133"></span>
<span class="source-line-no">134</span><span id="line-134">  private static final Logger LOG = LoggerFactory.getLogger(TestCellBasedHFileOutputFormat2.class);</span>
<span class="source-line-no">135</span><span id="line-135"></span>
<span class="source-line-no">136</span><span id="line-136">  /**</span>
<span class="source-line-no">137</span><span id="line-137">   * Simple mapper that makes KeyValue output.</span>
<span class="source-line-no">138</span><span id="line-138">   */</span>
<span class="source-line-no">139</span><span id="line-139">  static class RandomKVGeneratingMapper</span>
<span class="source-line-no">140</span><span id="line-140">    extends Mapper&lt;NullWritable, NullWritable, ImmutableBytesWritable, Cell&gt; {</span>
<span class="source-line-no">141</span><span id="line-141"></span>
<span class="source-line-no">142</span><span id="line-142">    private int keyLength;</span>
<span class="source-line-no">143</span><span id="line-143">    private static final int KEYLEN_DEFAULT = 10;</span>
<span class="source-line-no">144</span><span id="line-144">    private static final String KEYLEN_CONF = "randomkv.key.length";</span>
<span class="source-line-no">145</span><span id="line-145"></span>
<span class="source-line-no">146</span><span id="line-146">    private int valLength;</span>
<span class="source-line-no">147</span><span id="line-147">    private static final int VALLEN_DEFAULT = 10;</span>
<span class="source-line-no">148</span><span id="line-148">    private static final String VALLEN_CONF = "randomkv.val.length";</span>
<span class="source-line-no">149</span><span id="line-149">    private static final byte[] QUALIFIER = Bytes.toBytes("data");</span>
<span class="source-line-no">150</span><span id="line-150">    private boolean multiTableMapper = false;</span>
<span class="source-line-no">151</span><span id="line-151">    private TableName[] tables = null;</span>
<span class="source-line-no">152</span><span id="line-152"></span>
<span class="source-line-no">153</span><span id="line-153">    @Override</span>
<span class="source-line-no">154</span><span id="line-154">    protected void setup(Context context) throws IOException, InterruptedException {</span>
<span class="source-line-no">155</span><span id="line-155">      super.setup(context);</span>
<span class="source-line-no">156</span><span id="line-156"></span>
<span class="source-line-no">157</span><span id="line-157">      Configuration conf = context.getConfiguration();</span>
<span class="source-line-no">158</span><span id="line-158">      keyLength = conf.getInt(KEYLEN_CONF, KEYLEN_DEFAULT);</span>
<span class="source-line-no">159</span><span id="line-159">      valLength = conf.getInt(VALLEN_CONF, VALLEN_DEFAULT);</span>
<span class="source-line-no">160</span><span id="line-160">      multiTableMapper =</span>
<span class="source-line-no">161</span><span id="line-161">        conf.getBoolean(HFileOutputFormat2.MULTI_TABLE_HFILEOUTPUTFORMAT_CONF_KEY, false);</span>
<span class="source-line-no">162</span><span id="line-162">      if (multiTableMapper) {</span>
<span class="source-line-no">163</span><span id="line-163">        tables = TABLE_NAMES;</span>
<span class="source-line-no">164</span><span id="line-164">      } else {</span>
<span class="source-line-no">165</span><span id="line-165">        tables = new TableName[] { TABLE_NAMES[0] };</span>
<span class="source-line-no">166</span><span id="line-166">      }</span>
<span class="source-line-no">167</span><span id="line-167">    }</span>
<span class="source-line-no">168</span><span id="line-168"></span>
<span class="source-line-no">169</span><span id="line-169">    @Override</span>
<span class="source-line-no">170</span><span id="line-170">    protected void map(NullWritable n1, NullWritable n2,</span>
<span class="source-line-no">171</span><span id="line-171">      Mapper&lt;NullWritable, NullWritable, ImmutableBytesWritable, Cell&gt;.Context context)</span>
<span class="source-line-no">172</span><span id="line-172">      throws java.io.IOException, InterruptedException {</span>
<span class="source-line-no">173</span><span id="line-173"></span>
<span class="source-line-no">174</span><span id="line-174">      byte keyBytes[] = new byte[keyLength];</span>
<span class="source-line-no">175</span><span id="line-175">      byte valBytes[] = new byte[valLength];</span>
<span class="source-line-no">176</span><span id="line-176"></span>
<span class="source-line-no">177</span><span id="line-177">      int taskId = context.getTaskAttemptID().getTaskID().getId();</span>
<span class="source-line-no">178</span><span id="line-178">      assert taskId &lt; Byte.MAX_VALUE : "Unit tests dont support &gt; 127 tasks!";</span>
<span class="source-line-no">179</span><span id="line-179">      Random random = new Random();</span>
<span class="source-line-no">180</span><span id="line-180">      byte[] key;</span>
<span class="source-line-no">181</span><span id="line-181">      for (int j = 0; j &lt; tables.length; ++j) {</span>
<span class="source-line-no">182</span><span id="line-182">        for (int i = 0; i &lt; ROWSPERSPLIT; i++) {</span>
<span class="source-line-no">183</span><span id="line-183">          random.nextBytes(keyBytes);</span>
<span class="source-line-no">184</span><span id="line-184">          // Ensure that unique tasks generate unique keys</span>
<span class="source-line-no">185</span><span id="line-185">          keyBytes[keyLength - 1] = (byte) (taskId &amp; 0xFF);</span>
<span class="source-line-no">186</span><span id="line-186">          random.nextBytes(valBytes);</span>
<span class="source-line-no">187</span><span id="line-187">          key = keyBytes;</span>
<span class="source-line-no">188</span><span id="line-188">          if (multiTableMapper) {</span>
<span class="source-line-no">189</span><span id="line-189">            key = MultiTableHFileOutputFormat.createCompositeKey(tables[j].getName(), keyBytes);</span>
<span class="source-line-no">190</span><span id="line-190">          }</span>
<span class="source-line-no">191</span><span id="line-191"></span>
<span class="source-line-no">192</span><span id="line-192">          for (byte[] family : TestCellBasedHFileOutputFormat2.FAMILIES) {</span>
<span class="source-line-no">193</span><span id="line-193">            Cell kv = new KeyValue(keyBytes, family, QUALIFIER, valBytes);</span>
<span class="source-line-no">194</span><span id="line-194">            context.write(new ImmutableBytesWritable(key), kv);</span>
<span class="source-line-no">195</span><span id="line-195">          }</span>
<span class="source-line-no">196</span><span id="line-196">        }</span>
<span class="source-line-no">197</span><span id="line-197">      }</span>
<span class="source-line-no">198</span><span id="line-198">    }</span>
<span class="source-line-no">199</span><span id="line-199">  }</span>
<span class="source-line-no">200</span><span id="line-200"></span>
<span class="source-line-no">201</span><span id="line-201">  /**</span>
<span class="source-line-no">202</span><span id="line-202">   * Simple mapper that makes Put output.</span>
<span class="source-line-no">203</span><span id="line-203">   */</span>
<span class="source-line-no">204</span><span id="line-204">  static class RandomPutGeneratingMapper</span>
<span class="source-line-no">205</span><span id="line-205">    extends Mapper&lt;NullWritable, NullWritable, ImmutableBytesWritable, Put&gt; {</span>
<span class="source-line-no">206</span><span id="line-206"></span>
<span class="source-line-no">207</span><span id="line-207">    private int keyLength;</span>
<span class="source-line-no">208</span><span id="line-208">    private static final int KEYLEN_DEFAULT = 10;</span>
<span class="source-line-no">209</span><span id="line-209">    private static final String KEYLEN_CONF = "randomkv.key.length";</span>
<span class="source-line-no">210</span><span id="line-210"></span>
<span class="source-line-no">211</span><span id="line-211">    private int valLength;</span>
<span class="source-line-no">212</span><span id="line-212">    private static final int VALLEN_DEFAULT = 10;</span>
<span class="source-line-no">213</span><span id="line-213">    private static final String VALLEN_CONF = "randomkv.val.length";</span>
<span class="source-line-no">214</span><span id="line-214">    private static final byte[] QUALIFIER = Bytes.toBytes("data");</span>
<span class="source-line-no">215</span><span id="line-215">    private boolean multiTableMapper = false;</span>
<span class="source-line-no">216</span><span id="line-216">    private TableName[] tables = null;</span>
<span class="source-line-no">217</span><span id="line-217"></span>
<span class="source-line-no">218</span><span id="line-218">    @Override</span>
<span class="source-line-no">219</span><span id="line-219">    protected void setup(Context context) throws IOException, InterruptedException {</span>
<span class="source-line-no">220</span><span id="line-220">      super.setup(context);</span>
<span class="source-line-no">221</span><span id="line-221"></span>
<span class="source-line-no">222</span><span id="line-222">      Configuration conf = context.getConfiguration();</span>
<span class="source-line-no">223</span><span id="line-223">      keyLength = conf.getInt(KEYLEN_CONF, KEYLEN_DEFAULT);</span>
<span class="source-line-no">224</span><span id="line-224">      valLength = conf.getInt(VALLEN_CONF, VALLEN_DEFAULT);</span>
<span class="source-line-no">225</span><span id="line-225">      multiTableMapper =</span>
<span class="source-line-no">226</span><span id="line-226">        conf.getBoolean(HFileOutputFormat2.MULTI_TABLE_HFILEOUTPUTFORMAT_CONF_KEY, false);</span>
<span class="source-line-no">227</span><span id="line-227">      if (multiTableMapper) {</span>
<span class="source-line-no">228</span><span id="line-228">        tables = TABLE_NAMES;</span>
<span class="source-line-no">229</span><span id="line-229">      } else {</span>
<span class="source-line-no">230</span><span id="line-230">        tables = new TableName[] { TABLE_NAMES[0] };</span>
<span class="source-line-no">231</span><span id="line-231">      }</span>
<span class="source-line-no">232</span><span id="line-232">    }</span>
<span class="source-line-no">233</span><span id="line-233"></span>
<span class="source-line-no">234</span><span id="line-234">    @Override</span>
<span class="source-line-no">235</span><span id="line-235">    protected void map(NullWritable n1, NullWritable n2,</span>
<span class="source-line-no">236</span><span id="line-236">      Mapper&lt;NullWritable, NullWritable, ImmutableBytesWritable, Put&gt;.Context context)</span>
<span class="source-line-no">237</span><span id="line-237">      throws java.io.IOException, InterruptedException {</span>
<span class="source-line-no">238</span><span id="line-238"></span>
<span class="source-line-no">239</span><span id="line-239">      byte keyBytes[] = new byte[keyLength];</span>
<span class="source-line-no">240</span><span id="line-240">      byte valBytes[] = new byte[valLength];</span>
<span class="source-line-no">241</span><span id="line-241"></span>
<span class="source-line-no">242</span><span id="line-242">      int taskId = context.getTaskAttemptID().getTaskID().getId();</span>
<span class="source-line-no">243</span><span id="line-243">      assert taskId &lt; Byte.MAX_VALUE : "Unit tests dont support &gt; 127 tasks!";</span>
<span class="source-line-no">244</span><span id="line-244"></span>
<span class="source-line-no">245</span><span id="line-245">      Random random = new Random();</span>
<span class="source-line-no">246</span><span id="line-246">      byte[] key;</span>
<span class="source-line-no">247</span><span id="line-247">      for (int j = 0; j &lt; tables.length; ++j) {</span>
<span class="source-line-no">248</span><span id="line-248">        for (int i = 0; i &lt; ROWSPERSPLIT; i++) {</span>
<span class="source-line-no">249</span><span id="line-249">          random.nextBytes(keyBytes);</span>
<span class="source-line-no">250</span><span id="line-250">          // Ensure that unique tasks generate unique keys</span>
<span class="source-line-no">251</span><span id="line-251">          keyBytes[keyLength - 1] = (byte) (taskId &amp; 0xFF);</span>
<span class="source-line-no">252</span><span id="line-252">          random.nextBytes(valBytes);</span>
<span class="source-line-no">253</span><span id="line-253">          key = keyBytes;</span>
<span class="source-line-no">254</span><span id="line-254">          if (multiTableMapper) {</span>
<span class="source-line-no">255</span><span id="line-255">            key = MultiTableHFileOutputFormat.createCompositeKey(tables[j].getName(), keyBytes);</span>
<span class="source-line-no">256</span><span id="line-256">          }</span>
<span class="source-line-no">257</span><span id="line-257"></span>
<span class="source-line-no">258</span><span id="line-258">          for (byte[] family : TestCellBasedHFileOutputFormat2.FAMILIES) {</span>
<span class="source-line-no">259</span><span id="line-259">            Put p = new Put(keyBytes);</span>
<span class="source-line-no">260</span><span id="line-260">            p.addColumn(family, QUALIFIER, valBytes);</span>
<span class="source-line-no">261</span><span id="line-261">            // set TTL to very low so that the scan does not return any value</span>
<span class="source-line-no">262</span><span id="line-262">            p.setTTL(1l);</span>
<span class="source-line-no">263</span><span id="line-263">            context.write(new ImmutableBytesWritable(key), p);</span>
<span class="source-line-no">264</span><span id="line-264">          }</span>
<span class="source-line-no">265</span><span id="line-265">        }</span>
<span class="source-line-no">266</span><span id="line-266">      }</span>
<span class="source-line-no">267</span><span id="line-267">    }</span>
<span class="source-line-no">268</span><span id="line-268">  }</span>
<span class="source-line-no">269</span><span id="line-269"></span>
<span class="source-line-no">270</span><span id="line-270">  private void setupRandomGeneratorMapper(Job job, boolean putSortReducer) {</span>
<span class="source-line-no">271</span><span id="line-271">    if (putSortReducer) {</span>
<span class="source-line-no">272</span><span id="line-272">      job.setInputFormatClass(NMapInputFormat.class);</span>
<span class="source-line-no">273</span><span id="line-273">      job.setMapperClass(RandomPutGeneratingMapper.class);</span>
<span class="source-line-no">274</span><span id="line-274">      job.setMapOutputKeyClass(ImmutableBytesWritable.class);</span>
<span class="source-line-no">275</span><span id="line-275">      job.setMapOutputValueClass(Put.class);</span>
<span class="source-line-no">276</span><span id="line-276">    } else {</span>
<span class="source-line-no">277</span><span id="line-277">      job.setInputFormatClass(NMapInputFormat.class);</span>
<span class="source-line-no">278</span><span id="line-278">      job.setMapperClass(RandomKVGeneratingMapper.class);</span>
<span class="source-line-no">279</span><span id="line-279">      job.setMapOutputKeyClass(ImmutableBytesWritable.class);</span>
<span class="source-line-no">280</span><span id="line-280">      job.setMapOutputValueClass(KeyValue.class);</span>
<span class="source-line-no">281</span><span id="line-281">    }</span>
<span class="source-line-no">282</span><span id="line-282">  }</span>
<span class="source-line-no">283</span><span id="line-283"></span>
<span class="source-line-no">284</span><span id="line-284">  /**</span>
<span class="source-line-no">285</span><span id="line-285">   * Test that {@link HFileOutputFormat2} RecordWriter amends timestamps if passed a keyvalue whose</span>
<span class="source-line-no">286</span><span id="line-286">   * timestamp is {@link HConstants#LATEST_TIMESTAMP}.</span>
<span class="source-line-no">287</span><span id="line-287">   * @see &lt;a href="https://issues.apache.org/jira/browse/HBASE-2615"&gt;HBASE-2615&lt;/a&gt;</span>
<span class="source-line-no">288</span><span id="line-288">   */</span>
<span class="source-line-no">289</span><span id="line-289">  @Ignore("Goes zombie too frequently; needs work. See HBASE-14563")</span>
<span class="source-line-no">290</span><span id="line-290">  @Test</span>
<span class="source-line-no">291</span><span id="line-291">  public void test_LATEST_TIMESTAMP_isReplaced() throws Exception {</span>
<span class="source-line-no">292</span><span id="line-292">    Configuration conf = new Configuration(this.util.getConfiguration());</span>
<span class="source-line-no">293</span><span id="line-293">    RecordWriter&lt;ImmutableBytesWritable, Cell&gt; writer = null;</span>
<span class="source-line-no">294</span><span id="line-294">    TaskAttemptContext context = null;</span>
<span class="source-line-no">295</span><span id="line-295">    Path dir = util.getDataTestDir("test_LATEST_TIMESTAMP_isReplaced");</span>
<span class="source-line-no">296</span><span id="line-296">    try {</span>
<span class="source-line-no">297</span><span id="line-297">      Job job = new Job(conf);</span>
<span class="source-line-no">298</span><span id="line-298">      FileOutputFormat.setOutputPath(job, dir);</span>
<span class="source-line-no">299</span><span id="line-299">      context = createTestTaskAttemptContext(job);</span>
<span class="source-line-no">300</span><span id="line-300">      HFileOutputFormat2 hof = new HFileOutputFormat2();</span>
<span class="source-line-no">301</span><span id="line-301">      writer = hof.getRecordWriter(context);</span>
<span class="source-line-no">302</span><span id="line-302">      final byte[] b = Bytes.toBytes("b");</span>
<span class="source-line-no">303</span><span id="line-303"></span>
<span class="source-line-no">304</span><span id="line-304">      // Test 1. Pass a KV that has a ts of LATEST_TIMESTAMP. It should be</span>
<span class="source-line-no">305</span><span id="line-305">      // changed by call to write. Check all in kv is same but ts.</span>
<span class="source-line-no">306</span><span id="line-306">      KeyValue kv = new KeyValue(b, b, b);</span>
<span class="source-line-no">307</span><span id="line-307">      KeyValue original = kv.clone();</span>
<span class="source-line-no">308</span><span id="line-308">      writer.write(new ImmutableBytesWritable(), kv);</span>
<span class="source-line-no">309</span><span id="line-309">      assertFalse(original.equals(kv));</span>
<span class="source-line-no">310</span><span id="line-310">      assertTrue(Bytes.equals(CellUtil.cloneRow(original), CellUtil.cloneRow(kv)));</span>
<span class="source-line-no">311</span><span id="line-311">      assertTrue(Bytes.equals(CellUtil.cloneFamily(original), CellUtil.cloneFamily(kv)));</span>
<span class="source-line-no">312</span><span id="line-312">      assertTrue(Bytes.equals(CellUtil.cloneQualifier(original), CellUtil.cloneQualifier(kv)));</span>
<span class="source-line-no">313</span><span id="line-313">      assertNotSame(original.getTimestamp(), kv.getTimestamp());</span>
<span class="source-line-no">314</span><span id="line-314">      assertNotSame(HConstants.LATEST_TIMESTAMP, kv.getTimestamp());</span>
<span class="source-line-no">315</span><span id="line-315"></span>
<span class="source-line-no">316</span><span id="line-316">      // Test 2. Now test passing a kv that has explicit ts. It should not be</span>
<span class="source-line-no">317</span><span id="line-317">      // changed by call to record write.</span>
<span class="source-line-no">318</span><span id="line-318">      kv = new KeyValue(b, b, b, kv.getTimestamp() - 1, b);</span>
<span class="source-line-no">319</span><span id="line-319">      original = kv.clone();</span>
<span class="source-line-no">320</span><span id="line-320">      writer.write(new ImmutableBytesWritable(), kv);</span>
<span class="source-line-no">321</span><span id="line-321">      assertTrue(original.equals(kv));</span>
<span class="source-line-no">322</span><span id="line-322">    } finally {</span>
<span class="source-line-no">323</span><span id="line-323">      if (writer != null &amp;&amp; context != null) writer.close(context);</span>
<span class="source-line-no">324</span><span id="line-324">      dir.getFileSystem(conf).delete(dir, true);</span>
<span class="source-line-no">325</span><span id="line-325">    }</span>
<span class="source-line-no">326</span><span id="line-326">  }</span>
<span class="source-line-no">327</span><span id="line-327"></span>
<span class="source-line-no">328</span><span id="line-328">  private TaskAttemptContext createTestTaskAttemptContext(final Job job) throws Exception {</span>
<span class="source-line-no">329</span><span id="line-329">    HadoopShims hadoop = CompatibilitySingletonFactory.getInstance(HadoopShims.class);</span>
<span class="source-line-no">330</span><span id="line-330">    TaskAttemptContext context =</span>
<span class="source-line-no">331</span><span id="line-331">      hadoop.createTestTaskAttemptContext(job, "attempt_201402131733_0001_m_000000_0");</span>
<span class="source-line-no">332</span><span id="line-332">    return context;</span>
<span class="source-line-no">333</span><span id="line-333">  }</span>
<span class="source-line-no">334</span><span id="line-334"></span>
<span class="source-line-no">335</span><span id="line-335">  /*</span>
<span class="source-line-no">336</span><span id="line-336">   * Test that {@link HFileOutputFormat2} creates an HFile with TIMERANGE metadata used by</span>
<span class="source-line-no">337</span><span id="line-337">   * time-restricted scans.</span>
<span class="source-line-no">338</span><span id="line-338">   */</span>
<span class="source-line-no">339</span><span id="line-339">  @Ignore("Goes zombie too frequently; needs work. See HBASE-14563")</span>
<span class="source-line-no">340</span><span id="line-340">  @Test</span>
<span class="source-line-no">341</span><span id="line-341">  public void test_TIMERANGE() throws Exception {</span>
<span class="source-line-no">342</span><span id="line-342">    Configuration conf = new Configuration(this.util.getConfiguration());</span>
<span class="source-line-no">343</span><span id="line-343">    RecordWriter&lt;ImmutableBytesWritable, Cell&gt; writer = null;</span>
<span class="source-line-no">344</span><span id="line-344">    TaskAttemptContext context = null;</span>
<span class="source-line-no">345</span><span id="line-345">    Path dir = util.getDataTestDir("test_TIMERANGE_present");</span>
<span class="source-line-no">346</span><span id="line-346">    LOG.info("Timerange dir writing to dir: " + dir);</span>
<span class="source-line-no">347</span><span id="line-347">    try {</span>
<span class="source-line-no">348</span><span id="line-348">      // build a record writer using HFileOutputFormat2</span>
<span class="source-line-no">349</span><span id="line-349">      Job job = new Job(conf);</span>
<span class="source-line-no">350</span><span id="line-350">      FileOutputFormat.setOutputPath(job, dir);</span>
<span class="source-line-no">351</span><span id="line-351">      context = createTestTaskAttemptContext(job);</span>
<span class="source-line-no">352</span><span id="line-352">      HFileOutputFormat2 hof = new HFileOutputFormat2();</span>
<span class="source-line-no">353</span><span id="line-353">      writer = hof.getRecordWriter(context);</span>
<span class="source-line-no">354</span><span id="line-354"></span>
<span class="source-line-no">355</span><span id="line-355">      // Pass two key values with explicit times stamps</span>
<span class="source-line-no">356</span><span id="line-356">      final byte[] b = Bytes.toBytes("b");</span>
<span class="source-line-no">357</span><span id="line-357"></span>
<span class="source-line-no">358</span><span id="line-358">      // value 1 with timestamp 2000</span>
<span class="source-line-no">359</span><span id="line-359">      KeyValue kv = new KeyValue(b, b, b, 2000, b);</span>
<span class="source-line-no">360</span><span id="line-360">      KeyValue original = kv.clone();</span>
<span class="source-line-no">361</span><span id="line-361">      writer.write(new ImmutableBytesWritable(), kv);</span>
<span class="source-line-no">362</span><span id="line-362">      assertEquals(original, kv);</span>
<span class="source-line-no">363</span><span id="line-363"></span>
<span class="source-line-no">364</span><span id="line-364">      // value 2 with timestamp 1000</span>
<span class="source-line-no">365</span><span id="line-365">      kv = new KeyValue(b, b, b, 1000, b);</span>
<span class="source-line-no">366</span><span id="line-366">      original = kv.clone();</span>
<span class="source-line-no">367</span><span id="line-367">      writer.write(new ImmutableBytesWritable(), kv);</span>
<span class="source-line-no">368</span><span id="line-368">      assertEquals(original, kv);</span>
<span class="source-line-no">369</span><span id="line-369"></span>
<span class="source-line-no">370</span><span id="line-370">      // verify that the file has the proper FileInfo.</span>
<span class="source-line-no">371</span><span id="line-371">      writer.close(context);</span>
<span class="source-line-no">372</span><span id="line-372"></span>
<span class="source-line-no">373</span><span id="line-373">      // the generated file lives 1 directory down from the attempt directory</span>
<span class="source-line-no">374</span><span id="line-374">      // and is the only file, e.g.</span>
<span class="source-line-no">375</span><span id="line-375">      // _attempt__0000_r_000000_0/b/1979617994050536795</span>
<span class="source-line-no">376</span><span id="line-376">      FileSystem fs = FileSystem.get(conf);</span>
<span class="source-line-no">377</span><span id="line-377">      Path attemptDirectory = hof.getDefaultWorkFile(context, "").getParent();</span>
<span class="source-line-no">378</span><span id="line-378">      FileStatus[] sub1 = fs.listStatus(attemptDirectory);</span>
<span class="source-line-no">379</span><span id="line-379">      FileStatus[] file = fs.listStatus(sub1[0].getPath());</span>
<span class="source-line-no">380</span><span id="line-380"></span>
<span class="source-line-no">381</span><span id="line-381">      // open as HFile Reader and pull out TIMERANGE FileInfo.</span>
<span class="source-line-no">382</span><span id="line-382">      HFile.Reader rd =</span>
<span class="source-line-no">383</span><span id="line-383">        HFile.createReader(fs, file[0].getPath(), new CacheConfig(conf), true, conf);</span>
<span class="source-line-no">384</span><span id="line-384">      Map&lt;byte[], byte[]&gt; finfo = rd.getHFileInfo();</span>
<span class="source-line-no">385</span><span id="line-385">      byte[] range = finfo.get("TIMERANGE".getBytes("UTF-8"));</span>
<span class="source-line-no">386</span><span id="line-386">      assertNotNull(range);</span>
<span class="source-line-no">387</span><span id="line-387"></span>
<span class="source-line-no">388</span><span id="line-388">      // unmarshall and check values.</span>
<span class="source-line-no">389</span><span id="line-389">      TimeRangeTracker timeRangeTracker = TimeRangeTracker.parseFrom(range);</span>
<span class="source-line-no">390</span><span id="line-390">      LOG.info(timeRangeTracker.getMin() + "...." + timeRangeTracker.getMax());</span>
<span class="source-line-no">391</span><span id="line-391">      assertEquals(1000, timeRangeTracker.getMin());</span>
<span class="source-line-no">392</span><span id="line-392">      assertEquals(2000, timeRangeTracker.getMax());</span>
<span class="source-line-no">393</span><span id="line-393">      rd.close();</span>
<span class="source-line-no">394</span><span id="line-394">    } finally {</span>
<span class="source-line-no">395</span><span id="line-395">      if (writer != null &amp;&amp; context != null) writer.close(context);</span>
<span class="source-line-no">396</span><span id="line-396">      dir.getFileSystem(conf).delete(dir, true);</span>
<span class="source-line-no">397</span><span id="line-397">    }</span>
<span class="source-line-no">398</span><span id="line-398">  }</span>
<span class="source-line-no">399</span><span id="line-399"></span>
<span class="source-line-no">400</span><span id="line-400">  /**</span>
<span class="source-line-no">401</span><span id="line-401">   * Run small MR job.</span>
<span class="source-line-no">402</span><span id="line-402">   */</span>
<span class="source-line-no">403</span><span id="line-403">  @Ignore("Goes zombie too frequently; needs work. See HBASE-14563")</span>
<span class="source-line-no">404</span><span id="line-404">  @Test</span>
<span class="source-line-no">405</span><span id="line-405">  public void testWritingPEData() throws Exception {</span>
<span class="source-line-no">406</span><span id="line-406">    Configuration conf = util.getConfiguration();</span>
<span class="source-line-no">407</span><span id="line-407">    Path testDir = util.getDataTestDirOnTestFS("testWritingPEData");</span>
<span class="source-line-no">408</span><span id="line-408">    FileSystem fs = testDir.getFileSystem(conf);</span>
<span class="source-line-no">409</span><span id="line-409"></span>
<span class="source-line-no">410</span><span id="line-410">    // Set down this value or we OOME in eclipse.</span>
<span class="source-line-no">411</span><span id="line-411">    conf.setInt("mapreduce.task.io.sort.mb", 20);</span>
<span class="source-line-no">412</span><span id="line-412">    // Write a few files.</span>
<span class="source-line-no">413</span><span id="line-413">    conf.setLong(HConstants.HREGION_MAX_FILESIZE, 64 * 1024);</span>
<span class="source-line-no">414</span><span id="line-414"></span>
<span class="source-line-no">415</span><span id="line-415">    Job job = new Job(conf, "testWritingPEData");</span>
<span class="source-line-no">416</span><span id="line-416">    setupRandomGeneratorMapper(job, false);</span>
<span class="source-line-no">417</span><span id="line-417">    // This partitioner doesn't work well for number keys but using it anyways</span>
<span class="source-line-no">418</span><span id="line-418">    // just to demonstrate how to configure it.</span>
<span class="source-line-no">419</span><span id="line-419">    byte[] startKey = new byte[RandomKVGeneratingMapper.KEYLEN_DEFAULT];</span>
<span class="source-line-no">420</span><span id="line-420">    byte[] endKey = new byte[RandomKVGeneratingMapper.KEYLEN_DEFAULT];</span>
<span class="source-line-no">421</span><span id="line-421"></span>
<span class="source-line-no">422</span><span id="line-422">    Arrays.fill(startKey, (byte) 0);</span>
<span class="source-line-no">423</span><span id="line-423">    Arrays.fill(endKey, (byte) 0xff);</span>
<span class="source-line-no">424</span><span id="line-424"></span>
<span class="source-line-no">425</span><span id="line-425">    job.setPartitionerClass(SimpleTotalOrderPartitioner.class);</span>
<span class="source-line-no">426</span><span id="line-426">    // Set start and end rows for partitioner.</span>
<span class="source-line-no">427</span><span id="line-427">    SimpleTotalOrderPartitioner.setStartKey(job.getConfiguration(), startKey);</span>
<span class="source-line-no">428</span><span id="line-428">    SimpleTotalOrderPartitioner.setEndKey(job.getConfiguration(), endKey);</span>
<span class="source-line-no">429</span><span id="line-429">    job.setReducerClass(CellSortReducer.class);</span>
<span class="source-line-no">430</span><span id="line-430">    job.setOutputFormatClass(HFileOutputFormat2.class);</span>
<span class="source-line-no">431</span><span id="line-431">    job.setNumReduceTasks(4);</span>
<span class="source-line-no">432</span><span id="line-432">    job.getConfiguration().setStrings("io.serializations", conf.get("io.serializations"),</span>
<span class="source-line-no">433</span><span id="line-433">      MutationSerialization.class.getName(), ResultSerialization.class.getName(),</span>
<span class="source-line-no">434</span><span id="line-434">      CellSerialization.class.getName());</span>
<span class="source-line-no">435</span><span id="line-435"></span>
<span class="source-line-no">436</span><span id="line-436">    FileOutputFormat.setOutputPath(job, testDir);</span>
<span class="source-line-no">437</span><span id="line-437">    assertTrue(job.waitForCompletion(false));</span>
<span class="source-line-no">438</span><span id="line-438">    FileStatus[] files = fs.listStatus(testDir);</span>
<span class="source-line-no">439</span><span id="line-439">    assertTrue(files.length &gt; 0);</span>
<span class="source-line-no">440</span><span id="line-440">  }</span>
<span class="source-line-no">441</span><span id="line-441"></span>
<span class="source-line-no">442</span><span id="line-442">  /**</span>
<span class="source-line-no">443</span><span id="line-443">   * Test that {@link HFileOutputFormat2} RecordWriter writes tags such as ttl into hfile.</span>
<span class="source-line-no">444</span><span id="line-444">   */</span>
<span class="source-line-no">445</span><span id="line-445">  @Test</span>
<span class="source-line-no">446</span><span id="line-446">  public void test_WritingTagData() throws Exception {</span>
<span class="source-line-no">447</span><span id="line-447">    Configuration conf = new Configuration(this.util.getConfiguration());</span>
<span class="source-line-no">448</span><span id="line-448">    final String HFILE_FORMAT_VERSION_CONF_KEY = "hfile.format.version";</span>
<span class="source-line-no">449</span><span id="line-449">    conf.setInt(HFILE_FORMAT_VERSION_CONF_KEY, HFile.MIN_FORMAT_VERSION_WITH_TAGS);</span>
<span class="source-line-no">450</span><span id="line-450">    RecordWriter&lt;ImmutableBytesWritable, Cell&gt; writer = null;</span>
<span class="source-line-no">451</span><span id="line-451">    TaskAttemptContext context = null;</span>
<span class="source-line-no">452</span><span id="line-452">    Path dir = util.getDataTestDir("WritingTagData");</span>
<span class="source-line-no">453</span><span id="line-453">    try {</span>
<span class="source-line-no">454</span><span id="line-454">      conf.set(HFileOutputFormat2.OUTPUT_TABLE_NAME_CONF_KEY, TABLE_NAMES[0].getNameAsString());</span>
<span class="source-line-no">455</span><span id="line-455">      // turn locality off to eliminate getRegionLocation fail-and-retry time when writing kvs</span>
<span class="source-line-no">456</span><span id="line-456">      conf.setBoolean(HFileOutputFormat2.LOCALITY_SENSITIVE_CONF_KEY, false);</span>
<span class="source-line-no">457</span><span id="line-457">      Job job = new Job(conf);</span>
<span class="source-line-no">458</span><span id="line-458">      FileOutputFormat.setOutputPath(job, dir);</span>
<span class="source-line-no">459</span><span id="line-459">      context = createTestTaskAttemptContext(job);</span>
<span class="source-line-no">460</span><span id="line-460">      HFileOutputFormat2 hof = new HFileOutputFormat2();</span>
<span class="source-line-no">461</span><span id="line-461">      writer = hof.getRecordWriter(context);</span>
<span class="source-line-no">462</span><span id="line-462">      final byte[] b = Bytes.toBytes("b");</span>
<span class="source-line-no">463</span><span id="line-463"></span>
<span class="source-line-no">464</span><span id="line-464">      List&lt;Tag&gt; tags = new ArrayList&lt;&gt;();</span>
<span class="source-line-no">465</span><span id="line-465">      tags.add(new ArrayBackedTag(TagType.TTL_TAG_TYPE, Bytes.toBytes(978670)));</span>
<span class="source-line-no">466</span><span id="line-466">      KeyValue kv = new KeyValue(b, b, b, HConstants.LATEST_TIMESTAMP, b, tags);</span>
<span class="source-line-no">467</span><span id="line-467">      writer.write(new ImmutableBytesWritable(), kv);</span>
<span class="source-line-no">468</span><span id="line-468">      writer.close(context);</span>
<span class="source-line-no">469</span><span id="line-469">      writer = null;</span>
<span class="source-line-no">470</span><span id="line-470">      FileSystem fs = dir.getFileSystem(conf);</span>
<span class="source-line-no">471</span><span id="line-471">      RemoteIterator&lt;LocatedFileStatus&gt; iterator = fs.listFiles(dir, true);</span>
<span class="source-line-no">472</span><span id="line-472">      while (iterator.hasNext()) {</span>
<span class="source-line-no">473</span><span id="line-473">        LocatedFileStatus keyFileStatus = iterator.next();</span>
<span class="source-line-no">474</span><span id="line-474">        HFile.Reader reader =</span>
<span class="source-line-no">475</span><span id="line-475">          HFile.createReader(fs, keyFileStatus.getPath(), new CacheConfig(conf), true, conf);</span>
<span class="source-line-no">476</span><span id="line-476">        HFileScanner scanner = reader.getScanner(conf, false, false, false);</span>
<span class="source-line-no">477</span><span id="line-477">        scanner.seekTo();</span>
<span class="source-line-no">478</span><span id="line-478">        Cell cell = scanner.getCell();</span>
<span class="source-line-no">479</span><span id="line-479">        List&lt;Tag&gt; tagsFromCell =</span>
<span class="source-line-no">480</span><span id="line-480">          TagUtil.asList(cell.getTagsArray(), cell.getTagsOffset(), cell.getTagsLength());</span>
<span class="source-line-no">481</span><span id="line-481">        assertTrue(tagsFromCell.size() &gt; 0);</span>
<span class="source-line-no">482</span><span id="line-482">        for (Tag tag : tagsFromCell) {</span>
<span class="source-line-no">483</span><span id="line-483">          assertTrue(tag.getType() == TagType.TTL_TAG_TYPE);</span>
<span class="source-line-no">484</span><span id="line-484">        }</span>
<span class="source-line-no">485</span><span id="line-485">      }</span>
<span class="source-line-no">486</span><span id="line-486">    } finally {</span>
<span class="source-line-no">487</span><span id="line-487">      if (writer != null &amp;&amp; context != null) writer.close(context);</span>
<span class="source-line-no">488</span><span id="line-488">      dir.getFileSystem(conf).delete(dir, true);</span>
<span class="source-line-no">489</span><span id="line-489">    }</span>
<span class="source-line-no">490</span><span id="line-490">  }</span>
<span class="source-line-no">491</span><span id="line-491"></span>
<span class="source-line-no">492</span><span id="line-492">  @Ignore("Goes zombie too frequently; needs work. See HBASE-14563")</span>
<span class="source-line-no">493</span><span id="line-493">  @Test</span>
<span class="source-line-no">494</span><span id="line-494">  public void testJobConfiguration() throws Exception {</span>
<span class="source-line-no">495</span><span id="line-495">    Configuration conf = new Configuration(this.util.getConfiguration());</span>
<span class="source-line-no">496</span><span id="line-496">    conf.set(HConstants.TEMPORARY_FS_DIRECTORY_KEY,</span>
<span class="source-line-no">497</span><span id="line-497">      util.getDataTestDir("testJobConfiguration").toString());</span>
<span class="source-line-no">498</span><span id="line-498">    Job job = new Job(conf);</span>
<span class="source-line-no">499</span><span id="line-499">    job.setWorkingDirectory(util.getDataTestDir("testJobConfiguration"));</span>
<span class="source-line-no">500</span><span id="line-500">    Table table = Mockito.mock(Table.class);</span>
<span class="source-line-no">501</span><span id="line-501">    RegionLocator regionLocator = Mockito.mock(RegionLocator.class);</span>
<span class="source-line-no">502</span><span id="line-502">    setupMockStartKeys(regionLocator);</span>
<span class="source-line-no">503</span><span id="line-503">    setupMockTableName(regionLocator);</span>
<span class="source-line-no">504</span><span id="line-504">    HFileOutputFormat2.configureIncrementalLoad(job, table.getTableDescriptor(), regionLocator);</span>
<span class="source-line-no">505</span><span id="line-505">    assertEquals(job.getNumReduceTasks(), 4);</span>
<span class="source-line-no">506</span><span id="line-506">  }</span>
<span class="source-line-no">507</span><span id="line-507"></span>
<span class="source-line-no">508</span><span id="line-508">  private byte[][] generateRandomStartKeys(int numKeys) {</span>
<span class="source-line-no">509</span><span id="line-509">    Random random = new Random();</span>
<span class="source-line-no">510</span><span id="line-510">    byte[][] ret = new byte[numKeys][];</span>
<span class="source-line-no">511</span><span id="line-511">    // first region start key is always empty</span>
<span class="source-line-no">512</span><span id="line-512">    ret[0] = HConstants.EMPTY_BYTE_ARRAY;</span>
<span class="source-line-no">513</span><span id="line-513">    for (int i = 1; i &lt; numKeys; i++) {</span>
<span class="source-line-no">514</span><span id="line-514">      ret[i] =</span>
<span class="source-line-no">515</span><span id="line-515">        PerformanceEvaluation.generateData(random, PerformanceEvaluation.DEFAULT_VALUE_LENGTH);</span>
<span class="source-line-no">516</span><span id="line-516">    }</span>
<span class="source-line-no">517</span><span id="line-517">    return ret;</span>
<span class="source-line-no">518</span><span id="line-518">  }</span>
<span class="source-line-no">519</span><span id="line-519"></span>
<span class="source-line-no">520</span><span id="line-520">  private byte[][] generateRandomSplitKeys(int numKeys) {</span>
<span class="source-line-no">521</span><span id="line-521">    Random random = new Random();</span>
<span class="source-line-no">522</span><span id="line-522">    byte[][] ret = new byte[numKeys][];</span>
<span class="source-line-no">523</span><span id="line-523">    for (int i = 0; i &lt; numKeys; i++) {</span>
<span class="source-line-no">524</span><span id="line-524">      ret[i] =</span>
<span class="source-line-no">525</span><span id="line-525">        PerformanceEvaluation.generateData(random, PerformanceEvaluation.DEFAULT_VALUE_LENGTH);</span>
<span class="source-line-no">526</span><span id="line-526">    }</span>
<span class="source-line-no">527</span><span id="line-527">    return ret;</span>
<span class="source-line-no">528</span><span id="line-528">  }</span>
<span class="source-line-no">529</span><span id="line-529"></span>
<span class="source-line-no">530</span><span id="line-530">  @Ignore("Goes zombie too frequently; needs work. See HBASE-14563")</span>
<span class="source-line-no">531</span><span id="line-531">  @Test</span>
<span class="source-line-no">532</span><span id="line-532">  public void testMRIncrementalLoad() throws Exception {</span>
<span class="source-line-no">533</span><span id="line-533">    LOG.info("\nStarting test testMRIncrementalLoad\n");</span>
<span class="source-line-no">534</span><span id="line-534">    doIncrementalLoadTest(false, false, false, "testMRIncrementalLoad");</span>
<span class="source-line-no">535</span><span id="line-535">  }</span>
<span class="source-line-no">536</span><span id="line-536"></span>
<span class="source-line-no">537</span><span id="line-537">  @Ignore("Goes zombie too frequently; needs work. See HBASE-14563")</span>
<span class="source-line-no">538</span><span id="line-538">  @Test</span>
<span class="source-line-no">539</span><span id="line-539">  public void testMRIncrementalLoadWithSplit() throws Exception {</span>
<span class="source-line-no">540</span><span id="line-540">    LOG.info("\nStarting test testMRIncrementalLoadWithSplit\n");</span>
<span class="source-line-no">541</span><span id="line-541">    doIncrementalLoadTest(true, false, false, "testMRIncrementalLoadWithSplit");</span>
<span class="source-line-no">542</span><span id="line-542">  }</span>
<span class="source-line-no">543</span><span id="line-543"></span>
<span class="source-line-no">544</span><span id="line-544">  /**</span>
<span class="source-line-no">545</span><span id="line-545">   * Test for HFileOutputFormat2.LOCALITY_SENSITIVE_CONF_KEY = true This test could only check the</span>
<span class="source-line-no">546</span><span id="line-546">   * correctness of original logic if LOCALITY_SENSITIVE_CONF_KEY is set to true. Because</span>
<span class="source-line-no">547</span><span id="line-547">   * MiniHBaseCluster always run with single hostname (and different ports), it's not possible to</span>
<span class="source-line-no">548</span><span id="line-548">   * check the region locality by comparing region locations and DN hostnames. When MiniHBaseCluster</span>
<span class="source-line-no">549</span><span id="line-549">   * supports explicit hostnames parameter (just like MiniDFSCluster does), we could test region</span>
<span class="source-line-no">550</span><span id="line-550">   * locality features more easily.</span>
<span class="source-line-no">551</span><span id="line-551">   */</span>
<span class="source-line-no">552</span><span id="line-552">  @Ignore("Goes zombie too frequently; needs work. See HBASE-14563")</span>
<span class="source-line-no">553</span><span id="line-553">  @Test</span>
<span class="source-line-no">554</span><span id="line-554">  public void testMRIncrementalLoadWithLocality() throws Exception {</span>
<span class="source-line-no">555</span><span id="line-555">    LOG.info("\nStarting test testMRIncrementalLoadWithLocality\n");</span>
<span class="source-line-no">556</span><span id="line-556">    doIncrementalLoadTest(false, true, false, "testMRIncrementalLoadWithLocality1");</span>
<span class="source-line-no">557</span><span id="line-557">    doIncrementalLoadTest(true, true, false, "testMRIncrementalLoadWithLocality2");</span>
<span class="source-line-no">558</span><span id="line-558">  }</span>
<span class="source-line-no">559</span><span id="line-559"></span>
<span class="source-line-no">560</span><span id="line-560">  // @Ignore("Wahtevs")</span>
<span class="source-line-no">561</span><span id="line-561">  @Test</span>
<span class="source-line-no">562</span><span id="line-562">  public void testMRIncrementalLoadWithPutSortReducer() throws Exception {</span>
<span class="source-line-no">563</span><span id="line-563">    LOG.info("\nStarting test testMRIncrementalLoadWithPutSortReducer\n");</span>
<span class="source-line-no">564</span><span id="line-564">    doIncrementalLoadTest(false, false, true, "testMRIncrementalLoadWithPutSortReducer");</span>
<span class="source-line-no">565</span><span id="line-565">  }</span>
<span class="source-line-no">566</span><span id="line-566"></span>
<span class="source-line-no">567</span><span id="line-567">  private void doIncrementalLoadTest(boolean shouldChangeRegions, boolean shouldKeepLocality,</span>
<span class="source-line-no">568</span><span id="line-568">    boolean putSortReducer, String tableStr) throws Exception {</span>
<span class="source-line-no">569</span><span id="line-569">    doIncrementalLoadTest(shouldChangeRegions, shouldKeepLocality, putSortReducer, false,</span>
<span class="source-line-no">570</span><span id="line-570">      Arrays.asList(tableStr));</span>
<span class="source-line-no">571</span><span id="line-571">  }</span>
<span class="source-line-no">572</span><span id="line-572"></span>
<span class="source-line-no">573</span><span id="line-573">  @Test</span>
<span class="source-line-no">574</span><span id="line-574">  public void testMultiMRIncrementalLoadWithPutSortReducer() throws Exception {</span>
<span class="source-line-no">575</span><span id="line-575">    LOG.info("\nStarting test testMultiMRIncrementalLoadWithPutSortReducer\n");</span>
<span class="source-line-no">576</span><span id="line-576">    doIncrementalLoadTest(false, false, true, false,</span>
<span class="source-line-no">577</span><span id="line-577">      Arrays.stream(TABLE_NAMES).map(TableName::getNameAsString).collect(Collectors.toList()));</span>
<span class="source-line-no">578</span><span id="line-578">  }</span>
<span class="source-line-no">579</span><span id="line-579"></span>
<span class="source-line-no">580</span><span id="line-580">  @Test</span>
<span class="source-line-no">581</span><span id="line-581">  public void testMultiMRIncrementalLoadWithPutSortReducerWithNamespaceInPath() throws Exception {</span>
<span class="source-line-no">582</span><span id="line-582">    LOG.info("\nStarting test testMultiMRIncrementalLoadWithPutSortReducerWithNamespaceInPath\n");</span>
<span class="source-line-no">583</span><span id="line-583">    doIncrementalLoadTest(false, false, true, true,</span>
<span class="source-line-no">584</span><span id="line-584">      Arrays.stream(TABLE_NAMES).map(TableName::getNameAsString).collect(Collectors.toList()));</span>
<span class="source-line-no">585</span><span id="line-585">  }</span>
<span class="source-line-no">586</span><span id="line-586"></span>
<span class="source-line-no">587</span><span id="line-587">  private void doIncrementalLoadTest(boolean shouldChangeRegions, boolean shouldKeepLocality,</span>
<span class="source-line-no">588</span><span id="line-588">    boolean putSortReducer, boolean shouldWriteToTableWithNamespace, List&lt;String&gt; tableStr)</span>
<span class="source-line-no">589</span><span id="line-589">    throws Exception {</span>
<span class="source-line-no">590</span><span id="line-590">    util = new HBaseTestingUtility();</span>
<span class="source-line-no">591</span><span id="line-591">    Configuration conf = util.getConfiguration();</span>
<span class="source-line-no">592</span><span id="line-592">    conf.setBoolean(MultiTableHFileOutputFormat.LOCALITY_SENSITIVE_CONF_KEY, shouldKeepLocality);</span>
<span class="source-line-no">593</span><span id="line-593">    if (shouldWriteToTableWithNamespace) {</span>
<span class="source-line-no">594</span><span id="line-594">      conf.setBoolean(HFileOutputFormat2.TABLE_NAME_WITH_NAMESPACE_INCLUSIVE_KEY, true);</span>
<span class="source-line-no">595</span><span id="line-595">    }</span>
<span class="source-line-no">596</span><span id="line-596">    int hostCount = 1;</span>
<span class="source-line-no">597</span><span id="line-597">    int regionNum = 5;</span>
<span class="source-line-no">598</span><span id="line-598">    if (shouldKeepLocality) {</span>
<span class="source-line-no">599</span><span id="line-599">      // We should change host count higher than hdfs replica count when MiniHBaseCluster supports</span>
<span class="source-line-no">600</span><span id="line-600">      // explicit hostnames parameter just like MiniDFSCluster does.</span>
<span class="source-line-no">601</span><span id="line-601">      hostCount = 3;</span>
<span class="source-line-no">602</span><span id="line-602">      regionNum = 20;</span>
<span class="source-line-no">603</span><span id="line-603">    }</span>
<span class="source-line-no">604</span><span id="line-604"></span>
<span class="source-line-no">605</span><span id="line-605">    String[] hostnames = new String[hostCount];</span>
<span class="source-line-no">606</span><span id="line-606">    for (int i = 0; i &lt; hostCount; ++i) {</span>
<span class="source-line-no">607</span><span id="line-607">      hostnames[i] = "datanode_" + i;</span>
<span class="source-line-no">608</span><span id="line-608">    }</span>
<span class="source-line-no">609</span><span id="line-609">    util.startMiniCluster(1, hostCount, hostnames);</span>
<span class="source-line-no">610</span><span id="line-610"></span>
<span class="source-line-no">611</span><span id="line-611">    Map&lt;String, Table&gt; allTables = new HashMap&lt;&gt;(tableStr.size());</span>
<span class="source-line-no">612</span><span id="line-612">    List&lt;HFileOutputFormat2.TableInfo&gt; tableInfo = new ArrayList&lt;&gt;(tableStr.size());</span>
<span class="source-line-no">613</span><span id="line-613">    boolean writeMultipleTables = tableStr.size() &gt; 1;</span>
<span class="source-line-no">614</span><span id="line-614">    for (String tableStrSingle : tableStr) {</span>
<span class="source-line-no">615</span><span id="line-615">      byte[][] splitKeys = generateRandomSplitKeys(regionNum - 1);</span>
<span class="source-line-no">616</span><span id="line-616">      TableName tableName = TableName.valueOf(tableStrSingle);</span>
<span class="source-line-no">617</span><span id="line-617">      Table table = util.createTable(tableName, FAMILIES, splitKeys);</span>
<span class="source-line-no">618</span><span id="line-618"></span>
<span class="source-line-no">619</span><span id="line-619">      RegionLocator r = util.getConnection().getRegionLocator(tableName);</span>
<span class="source-line-no">620</span><span id="line-620">      assertEquals("Should start with empty table", 0, util.countRows(table));</span>
<span class="source-line-no">621</span><span id="line-621">      int numRegions = r.getStartKeys().length;</span>
<span class="source-line-no">622</span><span id="line-622">      assertEquals("Should make " + regionNum + " regions", numRegions, regionNum);</span>
<span class="source-line-no">623</span><span id="line-623"></span>
<span class="source-line-no">624</span><span id="line-624">      allTables.put(tableStrSingle, table);</span>
<span class="source-line-no">625</span><span id="line-625">      tableInfo.add(new HFileOutputFormat2.TableInfo(table.getTableDescriptor(), r));</span>
<span class="source-line-no">626</span><span id="line-626">    }</span>
<span class="source-line-no">627</span><span id="line-627">    Path testDir = util.getDataTestDirOnTestFS("testLocalMRIncrementalLoad");</span>
<span class="source-line-no">628</span><span id="line-628">    // Generate the bulk load files</span>
<span class="source-line-no">629</span><span id="line-629">    runIncrementalPELoad(conf, tableInfo, testDir, putSortReducer);</span>
<span class="source-line-no">630</span><span id="line-630">    if (shouldWriteToTableWithNamespace) {</span>
<span class="source-line-no">631</span><span id="line-631">      testDir = new Path(testDir, "default");</span>
<span class="source-line-no">632</span><span id="line-632">    }</span>
<span class="source-line-no">633</span><span id="line-633"></span>
<span class="source-line-no">634</span><span id="line-634">    for (Table tableSingle : allTables.values()) {</span>
<span class="source-line-no">635</span><span id="line-635">      // This doesn't write into the table, just makes files</span>
<span class="source-line-no">636</span><span id="line-636">      assertEquals("HFOF should not touch actual table", 0, util.countRows(tableSingle));</span>
<span class="source-line-no">637</span><span id="line-637">    }</span>
<span class="source-line-no">638</span><span id="line-638">    int numTableDirs = 0;</span>
<span class="source-line-no">639</span><span id="line-639">    FileStatus[] fss = testDir.getFileSystem(conf).listStatus(testDir);</span>
<span class="source-line-no">640</span><span id="line-640">    for (FileStatus tf : fss) {</span>
<span class="source-line-no">641</span><span id="line-641">      Path tablePath = testDir;</span>
<span class="source-line-no">642</span><span id="line-642"></span>
<span class="source-line-no">643</span><span id="line-643">      if (writeMultipleTables) {</span>
<span class="source-line-no">644</span><span id="line-644">        if (allTables.containsKey(tf.getPath().getName())) {</span>
<span class="source-line-no">645</span><span id="line-645">          ++numTableDirs;</span>
<span class="source-line-no">646</span><span id="line-646">          tablePath = tf.getPath();</span>
<span class="source-line-no">647</span><span id="line-647">        } else {</span>
<span class="source-line-no">648</span><span id="line-648">          continue;</span>
<span class="source-line-no">649</span><span id="line-649">        }</span>
<span class="source-line-no">650</span><span id="line-650">      }</span>
<span class="source-line-no">651</span><span id="line-651"></span>
<span class="source-line-no">652</span><span id="line-652">      // Make sure that a directory was created for every CF</span>
<span class="source-line-no">653</span><span id="line-653">      int dir = 0;</span>
<span class="source-line-no">654</span><span id="line-654">      fss = tablePath.getFileSystem(conf).listStatus(tablePath);</span>
<span class="source-line-no">655</span><span id="line-655">      for (FileStatus f : fss) {</span>
<span class="source-line-no">656</span><span id="line-656">        for (byte[] family : FAMILIES) {</span>
<span class="source-line-no">657</span><span id="line-657">          if (Bytes.toString(family).equals(f.getPath().getName())) {</span>
<span class="source-line-no">658</span><span id="line-658">            ++dir;</span>
<span class="source-line-no">659</span><span id="line-659">          }</span>
<span class="source-line-no">660</span><span id="line-660">        }</span>
<span class="source-line-no">661</span><span id="line-661">      }</span>
<span class="source-line-no">662</span><span id="line-662">      assertEquals("Column family not found in FS.", FAMILIES.length, dir);</span>
<span class="source-line-no">663</span><span id="line-663">    }</span>
<span class="source-line-no">664</span><span id="line-664">    if (writeMultipleTables) {</span>
<span class="source-line-no">665</span><span id="line-665">      assertEquals("Dir for all input tables not created", numTableDirs, allTables.size());</span>
<span class="source-line-no">666</span><span id="line-666">    }</span>
<span class="source-line-no">667</span><span id="line-667"></span>
<span class="source-line-no">668</span><span id="line-668">    Admin admin = util.getConnection().getAdmin();</span>
<span class="source-line-no">669</span><span id="line-669">    try {</span>
<span class="source-line-no">670</span><span id="line-670">      // handle the split case</span>
<span class="source-line-no">671</span><span id="line-671">      if (shouldChangeRegions) {</span>
<span class="source-line-no">672</span><span id="line-672">        Table chosenTable = allTables.values().iterator().next();</span>
<span class="source-line-no">673</span><span id="line-673">        // Choose a semi-random table if multiple tables are available</span>
<span class="source-line-no">674</span><span id="line-674">        LOG.info("Changing regions in table " + chosenTable.getName().getNameAsString());</span>
<span class="source-line-no">675</span><span id="line-675">        admin.disableTable(chosenTable.getName());</span>
<span class="source-line-no">676</span><span id="line-676">        util.waitUntilNoRegionsInTransition();</span>
<span class="source-line-no">677</span><span id="line-677"></span>
<span class="source-line-no">678</span><span id="line-678">        util.deleteTable(chosenTable.getName());</span>
<span class="source-line-no">679</span><span id="line-679">        byte[][] newSplitKeys = generateRandomSplitKeys(14);</span>
<span class="source-line-no">680</span><span id="line-680">        Table table = util.createTable(chosenTable.getName(), FAMILIES, newSplitKeys);</span>
<span class="source-line-no">681</span><span id="line-681"></span>
<span class="source-line-no">682</span><span id="line-682">        while (</span>
<span class="source-line-no">683</span><span id="line-683">          util.getConnection().getRegionLocator(chosenTable.getName()).getAllRegionLocations()</span>
<span class="source-line-no">684</span><span id="line-684">            .size() != 15 || !admin.isTableAvailable(table.getName())</span>
<span class="source-line-no">685</span><span id="line-685">        ) {</span>
<span class="source-line-no">686</span><span id="line-686">          Thread.sleep(200);</span>
<span class="source-line-no">687</span><span id="line-687">          LOG.info("Waiting for new region assignment to happen");</span>
<span class="source-line-no">688</span><span id="line-688">        }</span>
<span class="source-line-no">689</span><span id="line-689">      }</span>
<span class="source-line-no">690</span><span id="line-690"></span>
<span class="source-line-no">691</span><span id="line-691">      // Perform the actual load</span>
<span class="source-line-no">692</span><span id="line-692">      for (HFileOutputFormat2.TableInfo singleTableInfo : tableInfo) {</span>
<span class="source-line-no">693</span><span id="line-693">        Path tableDir = testDir;</span>
<span class="source-line-no">694</span><span id="line-694">        String tableNameStr = singleTableInfo.getHTableDescriptor().getNameAsString();</span>
<span class="source-line-no">695</span><span id="line-695">        LOG.info("Running LoadIncrementalHFiles on table" + tableNameStr);</span>
<span class="source-line-no">696</span><span id="line-696">        if (writeMultipleTables) {</span>
<span class="source-line-no">697</span><span id="line-697">          tableDir = new Path(testDir, tableNameStr);</span>
<span class="source-line-no">698</span><span id="line-698">        }</span>
<span class="source-line-no">699</span><span id="line-699">        Table currentTable = allTables.get(tableNameStr);</span>
<span class="source-line-no">700</span><span id="line-700">        TableName currentTableName = currentTable.getName();</span>
<span class="source-line-no">701</span><span id="line-701">        new LoadIncrementalHFiles(conf).doBulkLoad(tableDir, admin, currentTable,</span>
<span class="source-line-no">702</span><span id="line-702">          singleTableInfo.getRegionLocator());</span>
<span class="source-line-no">703</span><span id="line-703"></span>
<span class="source-line-no">704</span><span id="line-704">        // Ensure data shows up</span>
<span class="source-line-no">705</span><span id="line-705">        int expectedRows = 0;</span>
<span class="source-line-no">706</span><span id="line-706">        if (putSortReducer) {</span>
<span class="source-line-no">707</span><span id="line-707">          // no rows should be extracted</span>
<span class="source-line-no">708</span><span id="line-708">          assertEquals("LoadIncrementalHFiles should put expected data in table", expectedRows,</span>
<span class="source-line-no">709</span><span id="line-709">            util.countRows(currentTable));</span>
<span class="source-line-no">710</span><span id="line-710">        } else {</span>
<span class="source-line-no">711</span><span id="line-711">          expectedRows = NMapInputFormat.getNumMapTasks(conf) * ROWSPERSPLIT;</span>
<span class="source-line-no">712</span><span id="line-712">          assertEquals("LoadIncrementalHFiles should put expected data in table", expectedRows,</span>
<span class="source-line-no">713</span><span id="line-713">            util.countRows(currentTable));</span>
<span class="source-line-no">714</span><span id="line-714">          Scan scan = new Scan();</span>
<span class="source-line-no">715</span><span id="line-715">          ResultScanner results = currentTable.getScanner(scan);</span>
<span class="source-line-no">716</span><span id="line-716">          for (Result res : results) {</span>
<span class="source-line-no">717</span><span id="line-717">            assertEquals(FAMILIES.length, res.rawCells().length);</span>
<span class="source-line-no">718</span><span id="line-718">            Cell first = res.rawCells()[0];</span>
<span class="source-line-no">719</span><span id="line-719">            for (Cell kv : res.rawCells()) {</span>
<span class="source-line-no">720</span><span id="line-720">              assertTrue(CellUtil.matchingRows(first, kv));</span>
<span class="source-line-no">721</span><span id="line-721">              assertTrue(Bytes.equals(CellUtil.cloneValue(first), CellUtil.cloneValue(kv)));</span>
<span class="source-line-no">722</span><span id="line-722">            }</span>
<span class="source-line-no">723</span><span id="line-723">          }</span>
<span class="source-line-no">724</span><span id="line-724">          results.close();</span>
<span class="source-line-no">725</span><span id="line-725">        }</span>
<span class="source-line-no">726</span><span id="line-726">        String tableDigestBefore = util.checksumRows(currentTable);</span>
<span class="source-line-no">727</span><span id="line-727">        // Check region locality</span>
<span class="source-line-no">728</span><span id="line-728">        HDFSBlocksDistribution hbd = new HDFSBlocksDistribution();</span>
<span class="source-line-no">729</span><span id="line-729">        for (HRegion region : util.getHBaseCluster().getRegions(currentTableName)) {</span>
<span class="source-line-no">730</span><span id="line-730">          hbd.add(region.getHDFSBlocksDistribution());</span>
<span class="source-line-no">731</span><span id="line-731">        }</span>
<span class="source-line-no">732</span><span id="line-732">        for (String hostname : hostnames) {</span>
<span class="source-line-no">733</span><span id="line-733">          float locality = hbd.getBlockLocalityIndex(hostname);</span>
<span class="source-line-no">734</span><span id="line-734">          LOG.info("locality of [" + hostname + "]: " + locality);</span>
<span class="source-line-no">735</span><span id="line-735">          assertEquals(100, (int) (locality * 100));</span>
<span class="source-line-no">736</span><span id="line-736">        }</span>
<span class="source-line-no">737</span><span id="line-737"></span>
<span class="source-line-no">738</span><span id="line-738">        // Cause regions to reopen</span>
<span class="source-line-no">739</span><span id="line-739">        admin.disableTable(currentTableName);</span>
<span class="source-line-no">740</span><span id="line-740">        while (!admin.isTableDisabled(currentTableName)) {</span>
<span class="source-line-no">741</span><span id="line-741">          Thread.sleep(200);</span>
<span class="source-line-no">742</span><span id="line-742">          LOG.info("Waiting for table to disable");</span>
<span class="source-line-no">743</span><span id="line-743">        }</span>
<span class="source-line-no">744</span><span id="line-744">        admin.enableTable(currentTableName);</span>
<span class="source-line-no">745</span><span id="line-745">        util.waitTableAvailable(currentTableName);</span>
<span class="source-line-no">746</span><span id="line-746">        assertEquals("Data should remain after reopening of regions", tableDigestBefore,</span>
<span class="source-line-no">747</span><span id="line-747">          util.checksumRows(currentTable));</span>
<span class="source-line-no">748</span><span id="line-748">      }</span>
<span class="source-line-no">749</span><span id="line-749">    } finally {</span>
<span class="source-line-no">750</span><span id="line-750">      for (HFileOutputFormat2.TableInfo tableInfoSingle : tableInfo) {</span>
<span class="source-line-no">751</span><span id="line-751">        tableInfoSingle.getRegionLocator().close();</span>
<span class="source-line-no">752</span><span id="line-752">      }</span>
<span class="source-line-no">753</span><span id="line-753">      for (Entry&lt;String, Table&gt; singleTable : allTables.entrySet()) {</span>
<span class="source-line-no">754</span><span id="line-754">        singleTable.getValue().close();</span>
<span class="source-line-no">755</span><span id="line-755">        util.deleteTable(singleTable.getValue().getName());</span>
<span class="source-line-no">756</span><span id="line-756">      }</span>
<span class="source-line-no">757</span><span id="line-757">      testDir.getFileSystem(conf).delete(testDir, true);</span>
<span class="source-line-no">758</span><span id="line-758">      util.shutdownMiniCluster();</span>
<span class="source-line-no">759</span><span id="line-759">    }</span>
<span class="source-line-no">760</span><span id="line-760">  }</span>
<span class="source-line-no">761</span><span id="line-761"></span>
<span class="source-line-no">762</span><span id="line-762">  private void runIncrementalPELoad(Configuration conf,</span>
<span class="source-line-no">763</span><span id="line-763">    List&lt;HFileOutputFormat2.TableInfo&gt; tableInfo, Path outDir, boolean putSortReducer)</span>
<span class="source-line-no">764</span><span id="line-764">    throws IOException, InterruptedException, ClassNotFoundException {</span>
<span class="source-line-no">765</span><span id="line-765">    Job job = new Job(conf, "testLocalMRIncrementalLoad");</span>
<span class="source-line-no">766</span><span id="line-766">    job.setWorkingDirectory(util.getDataTestDirOnTestFS("runIncrementalPELoad"));</span>
<span class="source-line-no">767</span><span id="line-767">    job.getConfiguration().setStrings("io.serializations", conf.get("io.serializations"),</span>
<span class="source-line-no">768</span><span id="line-768">      MutationSerialization.class.getName(), ResultSerialization.class.getName(),</span>
<span class="source-line-no">769</span><span id="line-769">      CellSerialization.class.getName());</span>
<span class="source-line-no">770</span><span id="line-770">    setupRandomGeneratorMapper(job, putSortReducer);</span>
<span class="source-line-no">771</span><span id="line-771">    if (tableInfo.size() &gt; 1) {</span>
<span class="source-line-no">772</span><span id="line-772">      MultiTableHFileOutputFormat.configureIncrementalLoad(job, tableInfo);</span>
<span class="source-line-no">773</span><span id="line-773">      int sum = 0;</span>
<span class="source-line-no">774</span><span id="line-774">      for (HFileOutputFormat2.TableInfo tableInfoSingle : tableInfo) {</span>
<span class="source-line-no">775</span><span id="line-775">        sum += tableInfoSingle.getRegionLocator().getAllRegionLocations().size();</span>
<span class="source-line-no">776</span><span id="line-776">      }</span>
<span class="source-line-no">777</span><span id="line-777">      assertEquals(sum, job.getNumReduceTasks());</span>
<span class="source-line-no">778</span><span id="line-778">    } else {</span>
<span class="source-line-no">779</span><span id="line-779">      RegionLocator regionLocator = tableInfo.get(0).getRegionLocator();</span>
<span class="source-line-no">780</span><span id="line-780">      HFileOutputFormat2.configureIncrementalLoad(job, tableInfo.get(0).getHTableDescriptor(),</span>
<span class="source-line-no">781</span><span id="line-781">        regionLocator);</span>
<span class="source-line-no">782</span><span id="line-782">      assertEquals(regionLocator.getAllRegionLocations().size(), job.getNumReduceTasks());</span>
<span class="source-line-no">783</span><span id="line-783">    }</span>
<span class="source-line-no">784</span><span id="line-784"></span>
<span class="source-line-no">785</span><span id="line-785">    FileOutputFormat.setOutputPath(job, outDir);</span>
<span class="source-line-no">786</span><span id="line-786"></span>
<span class="source-line-no">787</span><span id="line-787">    assertFalse(util.getTestFileSystem().exists(outDir));</span>
<span class="source-line-no">788</span><span id="line-788"></span>
<span class="source-line-no">789</span><span id="line-789">    assertTrue(job.waitForCompletion(true));</span>
<span class="source-line-no">790</span><span id="line-790">  }</span>
<span class="source-line-no">791</span><span id="line-791"></span>
<span class="source-line-no">792</span><span id="line-792">  /**</span>
<span class="source-line-no">793</span><span id="line-793">   * Test for {@link HFileOutputFormat2#configureCompression(Configuration, HTableDescriptor)} and</span>
<span class="source-line-no">794</span><span id="line-794">   * {@link HFileOutputFormat2#createFamilyCompressionMap(Configuration)}. Tests that the</span>
<span class="source-line-no">795</span><span id="line-795">   * compression map is correctly serialized into and deserialized from configuration</span>
<span class="source-line-no">796</span><span id="line-796">   */</span>
<span class="source-line-no">797</span><span id="line-797">  @Ignore("Goes zombie too frequently; needs work. See HBASE-14563")</span>
<span class="source-line-no">798</span><span id="line-798">  @Test</span>
<span class="source-line-no">799</span><span id="line-799">  public void testSerializeDeserializeFamilyCompressionMap() throws IOException {</span>
<span class="source-line-no">800</span><span id="line-800">    for (int numCfs = 0; numCfs &lt;= 3; numCfs++) {</span>
<span class="source-line-no">801</span><span id="line-801">      Configuration conf = new Configuration(this.util.getConfiguration());</span>
<span class="source-line-no">802</span><span id="line-802">      Map&lt;String, Compression.Algorithm&gt; familyToCompression =</span>
<span class="source-line-no">803</span><span id="line-803">        getMockColumnFamiliesForCompression(numCfs);</span>
<span class="source-line-no">804</span><span id="line-804">      Table table = Mockito.mock(Table.class);</span>
<span class="source-line-no">805</span><span id="line-805">      setupMockColumnFamiliesForCompression(table, familyToCompression);</span>
<span class="source-line-no">806</span><span id="line-806">      conf.set(HFileOutputFormat2.COMPRESSION_FAMILIES_CONF_KEY,</span>
<span class="source-line-no">807</span><span id="line-807">        HFileOutputFormat2.serializeColumnFamilyAttribute(HFileOutputFormat2.compressionDetails,</span>
<span class="source-line-no">808</span><span id="line-808">          Arrays.asList(table.getTableDescriptor())));</span>
<span class="source-line-no">809</span><span id="line-809"></span>
<span class="source-line-no">810</span><span id="line-810">      // read back family specific compression setting from the configuration</span>
<span class="source-line-no">811</span><span id="line-811">      Map&lt;byte[], Algorithm&gt; retrievedFamilyToCompressionMap =</span>
<span class="source-line-no">812</span><span id="line-812">        HFileOutputFormat2.createFamilyCompressionMap(conf);</span>
<span class="source-line-no">813</span><span id="line-813"></span>
<span class="source-line-no">814</span><span id="line-814">      // test that we have a value for all column families that matches with the</span>
<span class="source-line-no">815</span><span id="line-815">      // used mock values</span>
<span class="source-line-no">816</span><span id="line-816">      for (Entry&lt;String, Algorithm&gt; entry : familyToCompression.entrySet()) {</span>
<span class="source-line-no">817</span><span id="line-817">        assertEquals("Compression configuration incorrect for column family:" + entry.getKey(),</span>
<span class="source-line-no">818</span><span id="line-818">          entry.getValue(), retrievedFamilyToCompressionMap.get(entry.getKey().getBytes("UTF-8")));</span>
<span class="source-line-no">819</span><span id="line-819">      }</span>
<span class="source-line-no">820</span><span id="line-820">    }</span>
<span class="source-line-no">821</span><span id="line-821">  }</span>
<span class="source-line-no">822</span><span id="line-822"></span>
<span class="source-line-no">823</span><span id="line-823">  private void setupMockColumnFamiliesForCompression(Table table,</span>
<span class="source-line-no">824</span><span id="line-824">    Map&lt;String, Compression.Algorithm&gt; familyToCompression) throws IOException {</span>
<span class="source-line-no">825</span><span id="line-825">    HTableDescriptor mockTableDescriptor = new HTableDescriptor(TABLE_NAMES[0]);</span>
<span class="source-line-no">826</span><span id="line-826">    for (Entry&lt;String, Compression.Algorithm&gt; entry : familyToCompression.entrySet()) {</span>
<span class="source-line-no">827</span><span id="line-827">      mockTableDescriptor.addFamily(new HColumnDescriptor(entry.getKey()).setMaxVersions(1)</span>
<span class="source-line-no">828</span><span id="line-828">        .setCompressionType(entry.getValue()).setBlockCacheEnabled(false).setTimeToLive(0));</span>
<span class="source-line-no">829</span><span id="line-829">    }</span>
<span class="source-line-no">830</span><span id="line-830">    Mockito.doReturn(mockTableDescriptor).when(table).getTableDescriptor();</span>
<span class="source-line-no">831</span><span id="line-831">  }</span>
<span class="source-line-no">832</span><span id="line-832"></span>
<span class="source-line-no">833</span><span id="line-833">  /**</span>
<span class="source-line-no">834</span><span id="line-834">   * @return a map from column family names to compression algorithms for testing column family</span>
<span class="source-line-no">835</span><span id="line-835">   *         compression. Column family names have special characters</span>
<span class="source-line-no">836</span><span id="line-836">   */</span>
<span class="source-line-no">837</span><span id="line-837">  private Map&lt;String, Compression.Algorithm&gt; getMockColumnFamiliesForCompression(int numCfs) {</span>
<span class="source-line-no">838</span><span id="line-838">    Map&lt;String, Compression.Algorithm&gt; familyToCompression = new HashMap&lt;&gt;();</span>
<span class="source-line-no">839</span><span id="line-839">    // use column family names having special characters</span>
<span class="source-line-no">840</span><span id="line-840">    if (numCfs-- &gt; 0) {</span>
<span class="source-line-no">841</span><span id="line-841">      familyToCompression.put("Family1!@#!@#&amp;", Compression.Algorithm.LZO);</span>
<span class="source-line-no">842</span><span id="line-842">    }</span>
<span class="source-line-no">843</span><span id="line-843">    if (numCfs-- &gt; 0) {</span>
<span class="source-line-no">844</span><span id="line-844">      familyToCompression.put("Family2=asdads&amp;!AASD", Compression.Algorithm.SNAPPY);</span>
<span class="source-line-no">845</span><span id="line-845">    }</span>
<span class="source-line-no">846</span><span id="line-846">    if (numCfs-- &gt; 0) {</span>
<span class="source-line-no">847</span><span id="line-847">      familyToCompression.put("Family2=asdads&amp;!AASD", Compression.Algorithm.GZ);</span>
<span class="source-line-no">848</span><span id="line-848">    }</span>
<span class="source-line-no">849</span><span id="line-849">    if (numCfs-- &gt; 0) {</span>
<span class="source-line-no">850</span><span id="line-850">      familyToCompression.put("Family3", Compression.Algorithm.NONE);</span>
<span class="source-line-no">851</span><span id="line-851">    }</span>
<span class="source-line-no">852</span><span id="line-852">    return familyToCompression;</span>
<span class="source-line-no">853</span><span id="line-853">  }</span>
<span class="source-line-no">854</span><span id="line-854"></span>
<span class="source-line-no">855</span><span id="line-855">  /**</span>
<span class="source-line-no">856</span><span id="line-856">   * Test for {@link HFileOutputFormat2#configureBloomType(HTableDescriptor, Configuration)} and</span>
<span class="source-line-no">857</span><span id="line-857">   * {@link HFileOutputFormat2#createFamilyBloomTypeMap(Configuration)}. Tests that the compression</span>
<span class="source-line-no">858</span><span id="line-858">   * map is correctly serialized into and deserialized from configuration</span>
<span class="source-line-no">859</span><span id="line-859">   */</span>
<span class="source-line-no">860</span><span id="line-860">  @Ignore("Goes zombie too frequently; needs work. See HBASE-14563")</span>
<span class="source-line-no">861</span><span id="line-861">  @Test</span>
<span class="source-line-no">862</span><span id="line-862">  public void testSerializeDeserializeFamilyBloomTypeMap() throws IOException {</span>
<span class="source-line-no">863</span><span id="line-863">    for (int numCfs = 0; numCfs &lt;= 2; numCfs++) {</span>
<span class="source-line-no">864</span><span id="line-864">      Configuration conf = new Configuration(this.util.getConfiguration());</span>
<span class="source-line-no">865</span><span id="line-865">      Map&lt;String, BloomType&gt; familyToBloomType = getMockColumnFamiliesForBloomType(numCfs);</span>
<span class="source-line-no">866</span><span id="line-866">      Table table = Mockito.mock(Table.class);</span>
<span class="source-line-no">867</span><span id="line-867">      setupMockColumnFamiliesForBloomType(table, familyToBloomType);</span>
<span class="source-line-no">868</span><span id="line-868">      conf.set(HFileOutputFormat2.BLOOM_TYPE_FAMILIES_CONF_KEY,</span>
<span class="source-line-no">869</span><span id="line-869">        HFileOutputFormat2.serializeColumnFamilyAttribute(HFileOutputFormat2.bloomTypeDetails,</span>
<span class="source-line-no">870</span><span id="line-870">          Arrays.asList(table.getTableDescriptor())));</span>
<span class="source-line-no">871</span><span id="line-871"></span>
<span class="source-line-no">872</span><span id="line-872">      // read back family specific data block encoding settings from the</span>
<span class="source-line-no">873</span><span id="line-873">      // configuration</span>
<span class="source-line-no">874</span><span id="line-874">      Map&lt;byte[], BloomType&gt; retrievedFamilyToBloomTypeMap =</span>
<span class="source-line-no">875</span><span id="line-875">        HFileOutputFormat2.createFamilyBloomTypeMap(conf);</span>
<span class="source-line-no">876</span><span id="line-876"></span>
<span class="source-line-no">877</span><span id="line-877">      // test that we have a value for all column families that matches with the</span>
<span class="source-line-no">878</span><span id="line-878">      // used mock values</span>
<span class="source-line-no">879</span><span id="line-879">      for (Entry&lt;String, BloomType&gt; entry : familyToBloomType.entrySet()) {</span>
<span class="source-line-no">880</span><span id="line-880">        assertEquals("BloomType configuration incorrect for column family:" + entry.getKey(),</span>
<span class="source-line-no">881</span><span id="line-881">          entry.getValue(), retrievedFamilyToBloomTypeMap.get(entry.getKey().getBytes("UTF-8")));</span>
<span class="source-line-no">882</span><span id="line-882">      }</span>
<span class="source-line-no">883</span><span id="line-883">    }</span>
<span class="source-line-no">884</span><span id="line-884">  }</span>
<span class="source-line-no">885</span><span id="line-885"></span>
<span class="source-line-no">886</span><span id="line-886">  private void setupMockColumnFamiliesForBloomType(Table table,</span>
<span class="source-line-no">887</span><span id="line-887">    Map&lt;String, BloomType&gt; familyToDataBlockEncoding) throws IOException {</span>
<span class="source-line-no">888</span><span id="line-888">    HTableDescriptor mockTableDescriptor = new HTableDescriptor(TABLE_NAMES[0]);</span>
<span class="source-line-no">889</span><span id="line-889">    for (Entry&lt;String, BloomType&gt; entry : familyToDataBlockEncoding.entrySet()) {</span>
<span class="source-line-no">890</span><span id="line-890">      mockTableDescriptor.addFamily(new HColumnDescriptor(entry.getKey()).setMaxVersions(1)</span>
<span class="source-line-no">891</span><span id="line-891">        .setBloomFilterType(entry.getValue()).setBlockCacheEnabled(false).setTimeToLive(0));</span>
<span class="source-line-no">892</span><span id="line-892">    }</span>
<span class="source-line-no">893</span><span id="line-893">    Mockito.doReturn(mockTableDescriptor).when(table).getTableDescriptor();</span>
<span class="source-line-no">894</span><span id="line-894">  }</span>
<span class="source-line-no">895</span><span id="line-895"></span>
<span class="source-line-no">896</span><span id="line-896">  /**</span>
<span class="source-line-no">897</span><span id="line-897">   * @return a map from column family names to compression algorithms for testing column family</span>
<span class="source-line-no">898</span><span id="line-898">   *         compression. Column family names have special characters</span>
<span class="source-line-no">899</span><span id="line-899">   */</span>
<span class="source-line-no">900</span><span id="line-900">  private Map&lt;String, BloomType&gt; getMockColumnFamiliesForBloomType(int numCfs) {</span>
<span class="source-line-no">901</span><span id="line-901">    Map&lt;String, BloomType&gt; familyToBloomType = new HashMap&lt;&gt;();</span>
<span class="source-line-no">902</span><span id="line-902">    // use column family names having special characters</span>
<span class="source-line-no">903</span><span id="line-903">    if (numCfs-- &gt; 0) {</span>
<span class="source-line-no">904</span><span id="line-904">      familyToBloomType.put("Family1!@#!@#&amp;", BloomType.ROW);</span>
<span class="source-line-no">905</span><span id="line-905">    }</span>
<span class="source-line-no">906</span><span id="line-906">    if (numCfs-- &gt; 0) {</span>
<span class="source-line-no">907</span><span id="line-907">      familyToBloomType.put("Family2=asdads&amp;!AASD", BloomType.ROWCOL);</span>
<span class="source-line-no">908</span><span id="line-908">    }</span>
<span class="source-line-no">909</span><span id="line-909">    if (numCfs-- &gt; 0) {</span>
<span class="source-line-no">910</span><span id="line-910">      familyToBloomType.put("Family3", BloomType.NONE);</span>
<span class="source-line-no">911</span><span id="line-911">    }</span>
<span class="source-line-no">912</span><span id="line-912">    return familyToBloomType;</span>
<span class="source-line-no">913</span><span id="line-913">  }</span>
<span class="source-line-no">914</span><span id="line-914"></span>
<span class="source-line-no">915</span><span id="line-915">  /**</span>
<span class="source-line-no">916</span><span id="line-916">   * Test for {@link HFileOutputFormat2#configureBlockSize(HTableDescriptor, Configuration)} and</span>
<span class="source-line-no">917</span><span id="line-917">   * {@link HFileOutputFormat2#createFamilyBlockSizeMap(Configuration)}. Tests that the compression</span>
<span class="source-line-no">918</span><span id="line-918">   * map is correctly serialized into and deserialized from configuration</span>
<span class="source-line-no">919</span><span id="line-919">   */</span>
<span class="source-line-no">920</span><span id="line-920">  @Ignore("Goes zombie too frequently; needs work. See HBASE-14563")</span>
<span class="source-line-no">921</span><span id="line-921">  @Test</span>
<span class="source-line-no">922</span><span id="line-922">  public void testSerializeDeserializeFamilyBlockSizeMap() throws IOException {</span>
<span class="source-line-no">923</span><span id="line-923">    for (int numCfs = 0; numCfs &lt;= 3; numCfs++) {</span>
<span class="source-line-no">924</span><span id="line-924">      Configuration conf = new Configuration(this.util.getConfiguration());</span>
<span class="source-line-no">925</span><span id="line-925">      Map&lt;String, Integer&gt; familyToBlockSize = getMockColumnFamiliesForBlockSize(numCfs);</span>
<span class="source-line-no">926</span><span id="line-926">      Table table = Mockito.mock(Table.class);</span>
<span class="source-line-no">927</span><span id="line-927">      setupMockColumnFamiliesForBlockSize(table, familyToBlockSize);</span>
<span class="source-line-no">928</span><span id="line-928">      conf.set(HFileOutputFormat2.BLOCK_SIZE_FAMILIES_CONF_KEY,</span>
<span class="source-line-no">929</span><span id="line-929">        HFileOutputFormat2.serializeColumnFamilyAttribute(HFileOutputFormat2.blockSizeDetails,</span>
<span class="source-line-no">930</span><span id="line-930">          Arrays.asList(table.getTableDescriptor())));</span>
<span class="source-line-no">931</span><span id="line-931"></span>
<span class="source-line-no">932</span><span id="line-932">      // read back family specific data block encoding settings from the</span>
<span class="source-line-no">933</span><span id="line-933">      // configuration</span>
<span class="source-line-no">934</span><span id="line-934">      Map&lt;byte[], Integer&gt; retrievedFamilyToBlockSizeMap =</span>
<span class="source-line-no">935</span><span id="line-935">        HFileOutputFormat2.createFamilyBlockSizeMap(conf);</span>
<span class="source-line-no">936</span><span id="line-936"></span>
<span class="source-line-no">937</span><span id="line-937">      // test that we have a value for all column families that matches with the</span>
<span class="source-line-no">938</span><span id="line-938">      // used mock values</span>
<span class="source-line-no">939</span><span id="line-939">      for (Entry&lt;String, Integer&gt; entry : familyToBlockSize.entrySet()) {</span>
<span class="source-line-no">940</span><span id="line-940">        assertEquals("BlockSize configuration incorrect for column family:" + entry.getKey(),</span>
<span class="source-line-no">941</span><span id="line-941">          entry.getValue(), retrievedFamilyToBlockSizeMap.get(entry.getKey().getBytes("UTF-8")));</span>
<span class="source-line-no">942</span><span id="line-942">      }</span>
<span class="source-line-no">943</span><span id="line-943">    }</span>
<span class="source-line-no">944</span><span id="line-944">  }</span>
<span class="source-line-no">945</span><span id="line-945"></span>
<span class="source-line-no">946</span><span id="line-946">  private void setupMockColumnFamiliesForBlockSize(Table table,</span>
<span class="source-line-no">947</span><span id="line-947">    Map&lt;String, Integer&gt; familyToDataBlockEncoding) throws IOException {</span>
<span class="source-line-no">948</span><span id="line-948">    HTableDescriptor mockTableDescriptor = new HTableDescriptor(TABLE_NAMES[0]);</span>
<span class="source-line-no">949</span><span id="line-949">    for (Entry&lt;String, Integer&gt; entry : familyToDataBlockEncoding.entrySet()) {</span>
<span class="source-line-no">950</span><span id="line-950">      mockTableDescriptor.addFamily(new HColumnDescriptor(entry.getKey()).setMaxVersions(1)</span>
<span class="source-line-no">951</span><span id="line-951">        .setBlocksize(entry.getValue()).setBlockCacheEnabled(false).setTimeToLive(0));</span>
<span class="source-line-no">952</span><span id="line-952">    }</span>
<span class="source-line-no">953</span><span id="line-953">    Mockito.doReturn(mockTableDescriptor).when(table).getTableDescriptor();</span>
<span class="source-line-no">954</span><span id="line-954">  }</span>
<span class="source-line-no">955</span><span id="line-955"></span>
<span class="source-line-no">956</span><span id="line-956">  /**</span>
<span class="source-line-no">957</span><span id="line-957">   * @return a map from column family names to compression algorithms for testing column family</span>
<span class="source-line-no">958</span><span id="line-958">   *         compression. Column family names have special characters</span>
<span class="source-line-no">959</span><span id="line-959">   */</span>
<span class="source-line-no">960</span><span id="line-960">  private Map&lt;String, Integer&gt; getMockColumnFamiliesForBlockSize(int numCfs) {</span>
<span class="source-line-no">961</span><span id="line-961">    Map&lt;String, Integer&gt; familyToBlockSize = new HashMap&lt;&gt;();</span>
<span class="source-line-no">962</span><span id="line-962">    // use column family names having special characters</span>
<span class="source-line-no">963</span><span id="line-963">    if (numCfs-- &gt; 0) {</span>
<span class="source-line-no">964</span><span id="line-964">      familyToBlockSize.put("Family1!@#!@#&amp;", 1234);</span>
<span class="source-line-no">965</span><span id="line-965">    }</span>
<span class="source-line-no">966</span><span id="line-966">    if (numCfs-- &gt; 0) {</span>
<span class="source-line-no">967</span><span id="line-967">      familyToBlockSize.put("Family2=asdads&amp;!AASD", Integer.MAX_VALUE);</span>
<span class="source-line-no">968</span><span id="line-968">    }</span>
<span class="source-line-no">969</span><span id="line-969">    if (numCfs-- &gt; 0) {</span>
<span class="source-line-no">970</span><span id="line-970">      familyToBlockSize.put("Family2=asdads&amp;!AASD", Integer.MAX_VALUE);</span>
<span class="source-line-no">971</span><span id="line-971">    }</span>
<span class="source-line-no">972</span><span id="line-972">    if (numCfs-- &gt; 0) {</span>
<span class="source-line-no">973</span><span id="line-973">      familyToBlockSize.put("Family3", 0);</span>
<span class="source-line-no">974</span><span id="line-974">    }</span>
<span class="source-line-no">975</span><span id="line-975">    return familyToBlockSize;</span>
<span class="source-line-no">976</span><span id="line-976">  }</span>
<span class="source-line-no">977</span><span id="line-977"></span>
<span class="source-line-no">978</span><span id="line-978">  /**</span>
<span class="source-line-no">979</span><span id="line-979">   * Test for {@link HFileOutputFormat2#configureDataBlockEncoding(HTableDescriptor, Configuration)}</span>
<span class="source-line-no">980</span><span id="line-980">   * and {@link HFileOutputFormat2#createFamilyDataBlockEncodingMap(Configuration)}. Tests that the</span>
<span class="source-line-no">981</span><span id="line-981">   * compression map is correctly serialized into and deserialized from configuration</span>
<span class="source-line-no">982</span><span id="line-982">   */</span>
<span class="source-line-no">983</span><span id="line-983">  @Ignore("Goes zombie too frequently; needs work. See HBASE-14563")</span>
<span class="source-line-no">984</span><span id="line-984">  @Test</span>
<span class="source-line-no">985</span><span id="line-985">  public void testSerializeDeserializeFamilyDataBlockEncodingMap() throws IOException {</span>
<span class="source-line-no">986</span><span id="line-986">    for (int numCfs = 0; numCfs &lt;= 3; numCfs++) {</span>
<span class="source-line-no">987</span><span id="line-987">      Configuration conf = new Configuration(this.util.getConfiguration());</span>
<span class="source-line-no">988</span><span id="line-988">      Map&lt;String, DataBlockEncoding&gt; familyToDataBlockEncoding =</span>
<span class="source-line-no">989</span><span id="line-989">        getMockColumnFamiliesForDataBlockEncoding(numCfs);</span>
<span class="source-line-no">990</span><span id="line-990">      Table table = Mockito.mock(Table.class);</span>
<span class="source-line-no">991</span><span id="line-991">      setupMockColumnFamiliesForDataBlockEncoding(table, familyToDataBlockEncoding);</span>
<span class="source-line-no">992</span><span id="line-992">      HTableDescriptor tableDescriptor = table.getTableDescriptor();</span>
<span class="source-line-no">993</span><span id="line-993">      conf.set(HFileOutputFormat2.DATABLOCK_ENCODING_FAMILIES_CONF_KEY,</span>
<span class="source-line-no">994</span><span id="line-994">        HFileOutputFormat2.serializeColumnFamilyAttribute(</span>
<span class="source-line-no">995</span><span id="line-995">          HFileOutputFormat2.dataBlockEncodingDetails, Arrays.asList(tableDescriptor)));</span>
<span class="source-line-no">996</span><span id="line-996"></span>
<span class="source-line-no">997</span><span id="line-997">      // read back family specific data block encoding settings from the</span>
<span class="source-line-no">998</span><span id="line-998">      // configuration</span>
<span class="source-line-no">999</span><span id="line-999">      Map&lt;byte[], DataBlockEncoding&gt; retrievedFamilyToDataBlockEncodingMap =</span>
<span class="source-line-no">1000</span><span id="line-1000">        HFileOutputFormat2.createFamilyDataBlockEncodingMap(conf);</span>
<span class="source-line-no">1001</span><span id="line-1001"></span>
<span class="source-line-no">1002</span><span id="line-1002">      // test that we have a value for all column families that matches with the</span>
<span class="source-line-no">1003</span><span id="line-1003">      // used mock values</span>
<span class="source-line-no">1004</span><span id="line-1004">      for (Entry&lt;String, DataBlockEncoding&gt; entry : familyToDataBlockEncoding.entrySet()) {</span>
<span class="source-line-no">1005</span><span id="line-1005">        assertEquals(</span>
<span class="source-line-no">1006</span><span id="line-1006">          "DataBlockEncoding configuration incorrect for column family:" + entry.getKey(),</span>
<span class="source-line-no">1007</span><span id="line-1007">          entry.getValue(),</span>
<span class="source-line-no">1008</span><span id="line-1008">          retrievedFamilyToDataBlockEncodingMap.get(entry.getKey().getBytes("UTF-8")));</span>
<span class="source-line-no">1009</span><span id="line-1009">      }</span>
<span class="source-line-no">1010</span><span id="line-1010">    }</span>
<span class="source-line-no">1011</span><span id="line-1011">  }</span>
<span class="source-line-no">1012</span><span id="line-1012"></span>
<span class="source-line-no">1013</span><span id="line-1013">  private void setupMockColumnFamiliesForDataBlockEncoding(Table table,</span>
<span class="source-line-no">1014</span><span id="line-1014">    Map&lt;String, DataBlockEncoding&gt; familyToDataBlockEncoding) throws IOException {</span>
<span class="source-line-no">1015</span><span id="line-1015">    HTableDescriptor mockTableDescriptor = new HTableDescriptor(TABLE_NAMES[0]);</span>
<span class="source-line-no">1016</span><span id="line-1016">    for (Entry&lt;String, DataBlockEncoding&gt; entry : familyToDataBlockEncoding.entrySet()) {</span>
<span class="source-line-no">1017</span><span id="line-1017">      mockTableDescriptor.addFamily(new HColumnDescriptor(entry.getKey()).setMaxVersions(1)</span>
<span class="source-line-no">1018</span><span id="line-1018">        .setDataBlockEncoding(entry.getValue()).setBlockCacheEnabled(false).setTimeToLive(0));</span>
<span class="source-line-no">1019</span><span id="line-1019">    }</span>
<span class="source-line-no">1020</span><span id="line-1020">    Mockito.doReturn(mockTableDescriptor).when(table).getTableDescriptor();</span>
<span class="source-line-no">1021</span><span id="line-1021">  }</span>
<span class="source-line-no">1022</span><span id="line-1022"></span>
<span class="source-line-no">1023</span><span id="line-1023">  /**</span>
<span class="source-line-no">1024</span><span id="line-1024">   * @return a map from column family names to compression algorithms for testing column family</span>
<span class="source-line-no">1025</span><span id="line-1025">   *         compression. Column family names have special characters</span>
<span class="source-line-no">1026</span><span id="line-1026">   */</span>
<span class="source-line-no">1027</span><span id="line-1027">  private Map&lt;String, DataBlockEncoding&gt; getMockColumnFamiliesForDataBlockEncoding(int numCfs) {</span>
<span class="source-line-no">1028</span><span id="line-1028">    Map&lt;String, DataBlockEncoding&gt; familyToDataBlockEncoding = new HashMap&lt;&gt;();</span>
<span class="source-line-no">1029</span><span id="line-1029">    // use column family names having special characters</span>
<span class="source-line-no">1030</span><span id="line-1030">    if (numCfs-- &gt; 0) {</span>
<span class="source-line-no">1031</span><span id="line-1031">      familyToDataBlockEncoding.put("Family1!@#!@#&amp;", DataBlockEncoding.DIFF);</span>
<span class="source-line-no">1032</span><span id="line-1032">    }</span>
<span class="source-line-no">1033</span><span id="line-1033">    if (numCfs-- &gt; 0) {</span>
<span class="source-line-no">1034</span><span id="line-1034">      familyToDataBlockEncoding.put("Family2=asdads&amp;!AASD", DataBlockEncoding.FAST_DIFF);</span>
<span class="source-line-no">1035</span><span id="line-1035">    }</span>
<span class="source-line-no">1036</span><span id="line-1036">    if (numCfs-- &gt; 0) {</span>
<span class="source-line-no">1037</span><span id="line-1037">      familyToDataBlockEncoding.put("Family2=asdads&amp;!AASD", DataBlockEncoding.PREFIX);</span>
<span class="source-line-no">1038</span><span id="line-1038">    }</span>
<span class="source-line-no">1039</span><span id="line-1039">    if (numCfs-- &gt; 0) {</span>
<span class="source-line-no">1040</span><span id="line-1040">      familyToDataBlockEncoding.put("Family3", DataBlockEncoding.NONE);</span>
<span class="source-line-no">1041</span><span id="line-1041">    }</span>
<span class="source-line-no">1042</span><span id="line-1042">    return familyToDataBlockEncoding;</span>
<span class="source-line-no">1043</span><span id="line-1043">  }</span>
<span class="source-line-no">1044</span><span id="line-1044"></span>
<span class="source-line-no">1045</span><span id="line-1045">  private void setupMockStartKeys(RegionLocator table) throws IOException {</span>
<span class="source-line-no">1046</span><span id="line-1046">    byte[][] mockKeys = new byte[][] { HConstants.EMPTY_BYTE_ARRAY, Bytes.toBytes("aaa"),</span>
<span class="source-line-no">1047</span><span id="line-1047">      Bytes.toBytes("ggg"), Bytes.toBytes("zzz") };</span>
<span class="source-line-no">1048</span><span id="line-1048">    Mockito.doReturn(mockKeys).when(table).getStartKeys();</span>
<span class="source-line-no">1049</span><span id="line-1049">  }</span>
<span class="source-line-no">1050</span><span id="line-1050"></span>
<span class="source-line-no">1051</span><span id="line-1051">  private void setupMockTableName(RegionLocator table) throws IOException {</span>
<span class="source-line-no">1052</span><span id="line-1052">    TableName mockTableName = TableName.valueOf("mock_table");</span>
<span class="source-line-no">1053</span><span id="line-1053">    Mockito.doReturn(mockTableName).when(table).getName();</span>
<span class="source-line-no">1054</span><span id="line-1054">  }</span>
<span class="source-line-no">1055</span><span id="line-1055"></span>
<span class="source-line-no">1056</span><span id="line-1056">  /**</span>
<span class="source-line-no">1057</span><span id="line-1057">   * Test that {@link HFileOutputFormat2} RecordWriter uses compression and bloom filter settings</span>
<span class="source-line-no">1058</span><span id="line-1058">   * from the column family descriptor</span>
<span class="source-line-no">1059</span><span id="line-1059">   */</span>
<span class="source-line-no">1060</span><span id="line-1060">  @Ignore("Goes zombie too frequently; needs work. See HBASE-14563")</span>
<span class="source-line-no">1061</span><span id="line-1061">  @Test</span>
<span class="source-line-no">1062</span><span id="line-1062">  public void testColumnFamilySettings() throws Exception {</span>
<span class="source-line-no">1063</span><span id="line-1063">    Configuration conf = new Configuration(this.util.getConfiguration());</span>
<span class="source-line-no">1064</span><span id="line-1064">    RecordWriter&lt;ImmutableBytesWritable, Cell&gt; writer = null;</span>
<span class="source-line-no">1065</span><span id="line-1065">    TaskAttemptContext context = null;</span>
<span class="source-line-no">1066</span><span id="line-1066">    Path dir = util.getDataTestDir("testColumnFamilySettings");</span>
<span class="source-line-no">1067</span><span id="line-1067"></span>
<span class="source-line-no">1068</span><span id="line-1068">    // Setup table descriptor</span>
<span class="source-line-no">1069</span><span id="line-1069">    Table table = Mockito.mock(Table.class);</span>
<span class="source-line-no">1070</span><span id="line-1070">    RegionLocator regionLocator = Mockito.mock(RegionLocator.class);</span>
<span class="source-line-no">1071</span><span id="line-1071">    HTableDescriptor htd = new HTableDescriptor(TABLE_NAMES[0]);</span>
<span class="source-line-no">1072</span><span id="line-1072">    Mockito.doReturn(htd).when(table).getTableDescriptor();</span>
<span class="source-line-no">1073</span><span id="line-1073">    for (HColumnDescriptor hcd : HBaseTestingUtility.generateColumnDescriptors()) {</span>
<span class="source-line-no">1074</span><span id="line-1074">      htd.addFamily(hcd);</span>
<span class="source-line-no">1075</span><span id="line-1075">    }</span>
<span class="source-line-no">1076</span><span id="line-1076"></span>
<span class="source-line-no">1077</span><span id="line-1077">    // set up the table to return some mock keys</span>
<span class="source-line-no">1078</span><span id="line-1078">    setupMockStartKeys(regionLocator);</span>
<span class="source-line-no">1079</span><span id="line-1079"></span>
<span class="source-line-no">1080</span><span id="line-1080">    try {</span>
<span class="source-line-no">1081</span><span id="line-1081">      // partial map red setup to get an operational writer for testing</span>
<span class="source-line-no">1082</span><span id="line-1082">      // We turn off the sequence file compression, because DefaultCodec</span>
<span class="source-line-no">1083</span><span id="line-1083">      // pollutes the GZip codec pool with an incompatible compressor.</span>
<span class="source-line-no">1084</span><span id="line-1084">      conf.set("io.seqfile.compression.type", "NONE");</span>
<span class="source-line-no">1085</span><span id="line-1085">      conf.set("hbase.fs.tmp.dir", dir.toString());</span>
<span class="source-line-no">1086</span><span id="line-1086">      // turn locality off to eliminate getRegionLocation fail-and-retry time when writing kvs</span>
<span class="source-line-no">1087</span><span id="line-1087">      conf.setBoolean(HFileOutputFormat2.LOCALITY_SENSITIVE_CONF_KEY, false);</span>
<span class="source-line-no">1088</span><span id="line-1088"></span>
<span class="source-line-no">1089</span><span id="line-1089">      Job job = new Job(conf, "testLocalMRIncrementalLoad");</span>
<span class="source-line-no">1090</span><span id="line-1090">      job.setWorkingDirectory(util.getDataTestDirOnTestFS("testColumnFamilySettings"));</span>
<span class="source-line-no">1091</span><span id="line-1091">      setupRandomGeneratorMapper(job, false);</span>
<span class="source-line-no">1092</span><span id="line-1092">      HFileOutputFormat2.configureIncrementalLoad(job, table.getTableDescriptor(), regionLocator);</span>
<span class="source-line-no">1093</span><span id="line-1093">      FileOutputFormat.setOutputPath(job, dir);</span>
<span class="source-line-no">1094</span><span id="line-1094">      context = createTestTaskAttemptContext(job);</span>
<span class="source-line-no">1095</span><span id="line-1095">      HFileOutputFormat2 hof = new HFileOutputFormat2();</span>
<span class="source-line-no">1096</span><span id="line-1096">      writer = hof.getRecordWriter(context);</span>
<span class="source-line-no">1097</span><span id="line-1097"></span>
<span class="source-line-no">1098</span><span id="line-1098">      // write out random rows</span>
<span class="source-line-no">1099</span><span id="line-1099">      writeRandomKeyValues(writer, context, htd.getFamiliesKeys(), ROWSPERSPLIT);</span>
<span class="source-line-no">1100</span><span id="line-1100">      writer.close(context);</span>
<span class="source-line-no">1101</span><span id="line-1101"></span>
<span class="source-line-no">1102</span><span id="line-1102">      // Make sure that a directory was created for every CF</span>
<span class="source-line-no">1103</span><span id="line-1103">      FileSystem fs = dir.getFileSystem(conf);</span>
<span class="source-line-no">1104</span><span id="line-1104"></span>
<span class="source-line-no">1105</span><span id="line-1105">      // commit so that the filesystem has one directory per column family</span>
<span class="source-line-no">1106</span><span id="line-1106">      hof.getOutputCommitter(context).commitTask(context);</span>
<span class="source-line-no">1107</span><span id="line-1107">      hof.getOutputCommitter(context).commitJob(context);</span>
<span class="source-line-no">1108</span><span id="line-1108">      FileStatus[] families = CommonFSUtils.listStatus(fs, dir, new FSUtils.FamilyDirFilter(fs));</span>
<span class="source-line-no">1109</span><span id="line-1109">      assertEquals(htd.getFamilies().size(), families.length);</span>
<span class="source-line-no">1110</span><span id="line-1110">      for (FileStatus f : families) {</span>
<span class="source-line-no">1111</span><span id="line-1111">        String familyStr = f.getPath().getName();</span>
<span class="source-line-no">1112</span><span id="line-1112">        HColumnDescriptor hcd = htd.getFamily(Bytes.toBytes(familyStr));</span>
<span class="source-line-no">1113</span><span id="line-1113">        // verify that the compression on this file matches the configured</span>
<span class="source-line-no">1114</span><span id="line-1114">        // compression</span>
<span class="source-line-no">1115</span><span id="line-1115">        Path dataFilePath = fs.listStatus(f.getPath())[0].getPath();</span>
<span class="source-line-no">1116</span><span id="line-1116">        Reader reader = HFile.createReader(fs, dataFilePath, new CacheConfig(conf), true, conf);</span>
<span class="source-line-no">1117</span><span id="line-1117">        Map&lt;byte[], byte[]&gt; fileInfo = reader.getHFileInfo();</span>
<span class="source-line-no">1118</span><span id="line-1118"></span>
<span class="source-line-no">1119</span><span id="line-1119">        byte[] bloomFilter = fileInfo.get(BLOOM_FILTER_TYPE_KEY);</span>
<span class="source-line-no">1120</span><span id="line-1120">        if (bloomFilter == null) bloomFilter = Bytes.toBytes("NONE");</span>
<span class="source-line-no">1121</span><span id="line-1121">        assertEquals(</span>
<span class="source-line-no">1122</span><span id="line-1122">          "Incorrect bloom filter used for column family " + familyStr + "(reader: " + reader + ")",</span>
<span class="source-line-no">1123</span><span id="line-1123">          hcd.getBloomFilterType(), BloomType.valueOf(Bytes.toString(bloomFilter)));</span>
<span class="source-line-no">1124</span><span id="line-1124">        assertEquals(</span>
<span class="source-line-no">1125</span><span id="line-1125">          "Incorrect compression used for column family " + familyStr + "(reader: " + reader + ")",</span>
<span class="source-line-no">1126</span><span id="line-1126">          hcd.getCompressionType(), reader.getFileContext().getCompression());</span>
<span class="source-line-no">1127</span><span id="line-1127">      }</span>
<span class="source-line-no">1128</span><span id="line-1128">    } finally {</span>
<span class="source-line-no">1129</span><span id="line-1129">      dir.getFileSystem(conf).delete(dir, true);</span>
<span class="source-line-no">1130</span><span id="line-1130">    }</span>
<span class="source-line-no">1131</span><span id="line-1131">  }</span>
<span class="source-line-no">1132</span><span id="line-1132"></span>
<span class="source-line-no">1133</span><span id="line-1133">  /**</span>
<span class="source-line-no">1134</span><span id="line-1134">   * Write random values to the writer assuming a table created using {@link #FAMILIES} as column</span>
<span class="source-line-no">1135</span><span id="line-1135">   * family descriptors</span>
<span class="source-line-no">1136</span><span id="line-1136">   */</span>
<span class="source-line-no">1137</span><span id="line-1137">  private void writeRandomKeyValues(RecordWriter&lt;ImmutableBytesWritable, Cell&gt; writer,</span>
<span class="source-line-no">1138</span><span id="line-1138">    TaskAttemptContext context, Set&lt;byte[]&gt; families, int numRows)</span>
<span class="source-line-no">1139</span><span id="line-1139">    throws IOException, InterruptedException {</span>
<span class="source-line-no">1140</span><span id="line-1140">    byte keyBytes[] = new byte[Bytes.SIZEOF_INT];</span>
<span class="source-line-no">1141</span><span id="line-1141">    int valLength = 10;</span>
<span class="source-line-no">1142</span><span id="line-1142">    byte valBytes[] = new byte[valLength];</span>
<span class="source-line-no">1143</span><span id="line-1143"></span>
<span class="source-line-no">1144</span><span id="line-1144">    int taskId = context.getTaskAttemptID().getTaskID().getId();</span>
<span class="source-line-no">1145</span><span id="line-1145">    assert taskId &lt; Byte.MAX_VALUE : "Unit tests dont support &gt; 127 tasks!";</span>
<span class="source-line-no">1146</span><span id="line-1146">    final byte[] qualifier = Bytes.toBytes("data");</span>
<span class="source-line-no">1147</span><span id="line-1147">    Random random = new Random();</span>
<span class="source-line-no">1148</span><span id="line-1148">    for (int i = 0; i &lt; numRows; i++) {</span>
<span class="source-line-no">1149</span><span id="line-1149"></span>
<span class="source-line-no">1150</span><span id="line-1150">      Bytes.putInt(keyBytes, 0, i);</span>
<span class="source-line-no">1151</span><span id="line-1151">      random.nextBytes(valBytes);</span>
<span class="source-line-no">1152</span><span id="line-1152">      ImmutableBytesWritable key = new ImmutableBytesWritable(keyBytes);</span>
<span class="source-line-no">1153</span><span id="line-1153"></span>
<span class="source-line-no">1154</span><span id="line-1154">      for (byte[] family : families) {</span>
<span class="source-line-no">1155</span><span id="line-1155">        Cell kv = new KeyValue(keyBytes, family, qualifier, valBytes);</span>
<span class="source-line-no">1156</span><span id="line-1156">        writer.write(key, kv);</span>
<span class="source-line-no">1157</span><span id="line-1157">      }</span>
<span class="source-line-no">1158</span><span id="line-1158">    }</span>
<span class="source-line-no">1159</span><span id="line-1159">  }</span>
<span class="source-line-no">1160</span><span id="line-1160"></span>
<span class="source-line-no">1161</span><span id="line-1161">  /**</span>
<span class="source-line-no">1162</span><span id="line-1162">   * This test is to test the scenario happened in HBASE-6901. All files are bulk loaded and</span>
<span class="source-line-no">1163</span><span id="line-1163">   * excluded from minor compaction. Without the fix of HBASE-6901, an</span>
<span class="source-line-no">1164</span><span id="line-1164">   * ArrayIndexOutOfBoundsException will be thrown.</span>
<span class="source-line-no">1165</span><span id="line-1165">   */</span>
<span class="source-line-no">1166</span><span id="line-1166">  @Ignore("Flakey: See HBASE-9051")</span>
<span class="source-line-no">1167</span><span id="line-1167">  @Test</span>
<span class="source-line-no">1168</span><span id="line-1168">  public void testExcludeAllFromMinorCompaction() throws Exception {</span>
<span class="source-line-no">1169</span><span id="line-1169">    Configuration conf = util.getConfiguration();</span>
<span class="source-line-no">1170</span><span id="line-1170">    conf.setInt("hbase.hstore.compaction.min", 2);</span>
<span class="source-line-no">1171</span><span id="line-1171">    generateRandomStartKeys(5);</span>
<span class="source-line-no">1172</span><span id="line-1172"></span>
<span class="source-line-no">1173</span><span id="line-1173">    util.startMiniCluster();</span>
<span class="source-line-no">1174</span><span id="line-1174">    try (Connection conn = ConnectionFactory.createConnection(); Admin admin = conn.getAdmin();</span>
<span class="source-line-no">1175</span><span id="line-1175">      Table table = util.createTable(TABLE_NAMES[0], FAMILIES);</span>
<span class="source-line-no">1176</span><span id="line-1176">      RegionLocator locator = conn.getRegionLocator(TABLE_NAMES[0])) {</span>
<span class="source-line-no">1177</span><span id="line-1177">      final FileSystem fs = util.getDFSCluster().getFileSystem();</span>
<span class="source-line-no">1178</span><span id="line-1178">      assertEquals("Should start with empty table", 0, util.countRows(table));</span>
<span class="source-line-no">1179</span><span id="line-1179"></span>
<span class="source-line-no">1180</span><span id="line-1180">      // deep inspection: get the StoreFile dir</span>
<span class="source-line-no">1181</span><span id="line-1181">      final Path storePath =</span>
<span class="source-line-no">1182</span><span id="line-1182">        new Path(CommonFSUtils.getTableDir(CommonFSUtils.getRootDir(conf), TABLE_NAMES[0]),</span>
<span class="source-line-no">1183</span><span id="line-1183">          new Path(admin.getTableRegions(TABLE_NAMES[0]).get(0).getEncodedName(),</span>
<span class="source-line-no">1184</span><span id="line-1184">            Bytes.toString(FAMILIES[0])));</span>
<span class="source-line-no">1185</span><span id="line-1185">      assertEquals(0, fs.listStatus(storePath).length);</span>
<span class="source-line-no">1186</span><span id="line-1186"></span>
<span class="source-line-no">1187</span><span id="line-1187">      // Generate two bulk load files</span>
<span class="source-line-no">1188</span><span id="line-1188">      conf.setBoolean("hbase.mapreduce.hfileoutputformat.compaction.exclude", true);</span>
<span class="source-line-no">1189</span><span id="line-1189"></span>
<span class="source-line-no">1190</span><span id="line-1190">      for (int i = 0; i &lt; 2; i++) {</span>
<span class="source-line-no">1191</span><span id="line-1191">        Path testDir = util.getDataTestDirOnTestFS("testExcludeAllFromMinorCompaction_" + i);</span>
<span class="source-line-no">1192</span><span id="line-1192">        runIncrementalPELoad(conf,</span>
<span class="source-line-no">1193</span><span id="line-1193">          Arrays.asList(new HFileOutputFormat2.TableInfo(table.getTableDescriptor(),</span>
<span class="source-line-no">1194</span><span id="line-1194">            conn.getRegionLocator(TABLE_NAMES[0]))),</span>
<span class="source-line-no">1195</span><span id="line-1195">          testDir, false);</span>
<span class="source-line-no">1196</span><span id="line-1196">        // Perform the actual load</span>
<span class="source-line-no">1197</span><span id="line-1197">        new LoadIncrementalHFiles(conf).doBulkLoad(testDir, admin, table, locator);</span>
<span class="source-line-no">1198</span><span id="line-1198">      }</span>
<span class="source-line-no">1199</span><span id="line-1199"></span>
<span class="source-line-no">1200</span><span id="line-1200">      // Ensure data shows up</span>
<span class="source-line-no">1201</span><span id="line-1201">      int expectedRows = 2 * NMapInputFormat.getNumMapTasks(conf) * ROWSPERSPLIT;</span>
<span class="source-line-no">1202</span><span id="line-1202">      assertEquals("LoadIncrementalHFiles should put expected data in table", expectedRows,</span>
<span class="source-line-no">1203</span><span id="line-1203">        util.countRows(table));</span>
<span class="source-line-no">1204</span><span id="line-1204"></span>
<span class="source-line-no">1205</span><span id="line-1205">      // should have a second StoreFile now</span>
<span class="source-line-no">1206</span><span id="line-1206">      assertEquals(2, fs.listStatus(storePath).length);</span>
<span class="source-line-no">1207</span><span id="line-1207"></span>
<span class="source-line-no">1208</span><span id="line-1208">      // minor compactions shouldn't get rid of the file</span>
<span class="source-line-no">1209</span><span id="line-1209">      admin.compact(TABLE_NAMES[0]);</span>
<span class="source-line-no">1210</span><span id="line-1210">      try {</span>
<span class="source-line-no">1211</span><span id="line-1211">        quickPoll(new Callable&lt;Boolean&gt;() {</span>
<span class="source-line-no">1212</span><span id="line-1212">          @Override</span>
<span class="source-line-no">1213</span><span id="line-1213">          public Boolean call() throws Exception {</span>
<span class="source-line-no">1214</span><span id="line-1214">            List&lt;HRegion&gt; regions = util.getMiniHBaseCluster().getRegions(TABLE_NAMES[0]);</span>
<span class="source-line-no">1215</span><span id="line-1215">            for (HRegion region : regions) {</span>
<span class="source-line-no">1216</span><span id="line-1216">              for (HStore store : region.getStores()) {</span>
<span class="source-line-no">1217</span><span id="line-1217">                store.closeAndArchiveCompactedFiles();</span>
<span class="source-line-no">1218</span><span id="line-1218">              }</span>
<span class="source-line-no">1219</span><span id="line-1219">            }</span>
<span class="source-line-no">1220</span><span id="line-1220">            return fs.listStatus(storePath).length == 1;</span>
<span class="source-line-no">1221</span><span id="line-1221">          }</span>
<span class="source-line-no">1222</span><span id="line-1222">        }, 5000);</span>
<span class="source-line-no">1223</span><span id="line-1223">        throw new IOException("SF# = " + fs.listStatus(storePath).length);</span>
<span class="source-line-no">1224</span><span id="line-1224">      } catch (AssertionError ae) {</span>
<span class="source-line-no">1225</span><span id="line-1225">        // this is expected behavior</span>
<span class="source-line-no">1226</span><span id="line-1226">      }</span>
<span class="source-line-no">1227</span><span id="line-1227"></span>
<span class="source-line-no">1228</span><span id="line-1228">      // a major compaction should work though</span>
<span class="source-line-no">1229</span><span id="line-1229">      admin.majorCompact(TABLE_NAMES[0]);</span>
<span class="source-line-no">1230</span><span id="line-1230">      quickPoll(new Callable&lt;Boolean&gt;() {</span>
<span class="source-line-no">1231</span><span id="line-1231">        @Override</span>
<span class="source-line-no">1232</span><span id="line-1232">        public Boolean call() throws Exception {</span>
<span class="source-line-no">1233</span><span id="line-1233">          List&lt;HRegion&gt; regions = util.getMiniHBaseCluster().getRegions(TABLE_NAMES[0]);</span>
<span class="source-line-no">1234</span><span id="line-1234">          for (HRegion region : regions) {</span>
<span class="source-line-no">1235</span><span id="line-1235">            for (HStore store : region.getStores()) {</span>
<span class="source-line-no">1236</span><span id="line-1236">              store.closeAndArchiveCompactedFiles();</span>
<span class="source-line-no">1237</span><span id="line-1237">            }</span>
<span class="source-line-no">1238</span><span id="line-1238">          }</span>
<span class="source-line-no">1239</span><span id="line-1239">          return fs.listStatus(storePath).length == 1;</span>
<span class="source-line-no">1240</span><span id="line-1240">        }</span>
<span class="source-line-no">1241</span><span id="line-1241">      }, 5000);</span>
<span class="source-line-no">1242</span><span id="line-1242"></span>
<span class="source-line-no">1243</span><span id="line-1243">    } finally {</span>
<span class="source-line-no">1244</span><span id="line-1244">      util.shutdownMiniCluster();</span>
<span class="source-line-no">1245</span><span id="line-1245">    }</span>
<span class="source-line-no">1246</span><span id="line-1246">  }</span>
<span class="source-line-no">1247</span><span id="line-1247"></span>
<span class="source-line-no">1248</span><span id="line-1248">  @Ignore("Goes zombie too frequently; needs work. See HBASE-14563")</span>
<span class="source-line-no">1249</span><span id="line-1249">  @Test</span>
<span class="source-line-no">1250</span><span id="line-1250">  public void testExcludeMinorCompaction() throws Exception {</span>
<span class="source-line-no">1251</span><span id="line-1251">    Configuration conf = util.getConfiguration();</span>
<span class="source-line-no">1252</span><span id="line-1252">    conf.setInt("hbase.hstore.compaction.min", 2);</span>
<span class="source-line-no">1253</span><span id="line-1253">    generateRandomStartKeys(5);</span>
<span class="source-line-no">1254</span><span id="line-1254"></span>
<span class="source-line-no">1255</span><span id="line-1255">    util.startMiniCluster();</span>
<span class="source-line-no">1256</span><span id="line-1256">    try (Connection conn = ConnectionFactory.createConnection(conf);</span>
<span class="source-line-no">1257</span><span id="line-1257">      Admin admin = conn.getAdmin()) {</span>
<span class="source-line-no">1258</span><span id="line-1258">      Path testDir = util.getDataTestDirOnTestFS("testExcludeMinorCompaction");</span>
<span class="source-line-no">1259</span><span id="line-1259">      final FileSystem fs = util.getDFSCluster().getFileSystem();</span>
<span class="source-line-no">1260</span><span id="line-1260">      Table table = util.createTable(TABLE_NAMES[0], FAMILIES);</span>
<span class="source-line-no">1261</span><span id="line-1261">      assertEquals("Should start with empty table", 0, util.countRows(table));</span>
<span class="source-line-no">1262</span><span id="line-1262"></span>
<span class="source-line-no">1263</span><span id="line-1263">      // deep inspection: get the StoreFile dir</span>
<span class="source-line-no">1264</span><span id="line-1264">      final Path storePath =</span>
<span class="source-line-no">1265</span><span id="line-1265">        new Path(CommonFSUtils.getTableDir(CommonFSUtils.getRootDir(conf), TABLE_NAMES[0]),</span>
<span class="source-line-no">1266</span><span id="line-1266">          new Path(admin.getTableRegions(TABLE_NAMES[0]).get(0).getEncodedName(),</span>
<span class="source-line-no">1267</span><span id="line-1267">            Bytes.toString(FAMILIES[0])));</span>
<span class="source-line-no">1268</span><span id="line-1268">      assertEquals(0, fs.listStatus(storePath).length);</span>
<span class="source-line-no">1269</span><span id="line-1269"></span>
<span class="source-line-no">1270</span><span id="line-1270">      // put some data in it and flush to create a storefile</span>
<span class="source-line-no">1271</span><span id="line-1271">      Put p = new Put(Bytes.toBytes("test"));</span>
<span class="source-line-no">1272</span><span id="line-1272">      p.addColumn(FAMILIES[0], Bytes.toBytes("1"), Bytes.toBytes("1"));</span>
<span class="source-line-no">1273</span><span id="line-1273">      table.put(p);</span>
<span class="source-line-no">1274</span><span id="line-1274">      admin.flush(TABLE_NAMES[0]);</span>
<span class="source-line-no">1275</span><span id="line-1275">      assertEquals(1, util.countRows(table));</span>
<span class="source-line-no">1276</span><span id="line-1276">      quickPoll(new Callable&lt;Boolean&gt;() {</span>
<span class="source-line-no">1277</span><span id="line-1277">        @Override</span>
<span class="source-line-no">1278</span><span id="line-1278">        public Boolean call() throws Exception {</span>
<span class="source-line-no">1279</span><span id="line-1279">          return fs.listStatus(storePath).length == 1;</span>
<span class="source-line-no">1280</span><span id="line-1280">        }</span>
<span class="source-line-no">1281</span><span id="line-1281">      }, 5000);</span>
<span class="source-line-no">1282</span><span id="line-1282"></span>
<span class="source-line-no">1283</span><span id="line-1283">      // Generate a bulk load file with more rows</span>
<span class="source-line-no">1284</span><span id="line-1284">      conf.setBoolean("hbase.mapreduce.hfileoutputformat.compaction.exclude", true);</span>
<span class="source-line-no">1285</span><span id="line-1285"></span>
<span class="source-line-no">1286</span><span id="line-1286">      RegionLocator regionLocator = conn.getRegionLocator(TABLE_NAMES[0]);</span>
<span class="source-line-no">1287</span><span id="line-1287">      runIncrementalPELoad(conf,</span>
<span class="source-line-no">1288</span><span id="line-1288">        Arrays.asList(new HFileOutputFormat2.TableInfo(table.getTableDescriptor(), regionLocator)),</span>
<span class="source-line-no">1289</span><span id="line-1289">        testDir, false);</span>
<span class="source-line-no">1290</span><span id="line-1290"></span>
<span class="source-line-no">1291</span><span id="line-1291">      // Perform the actual load</span>
<span class="source-line-no">1292</span><span id="line-1292">      new LoadIncrementalHFiles(conf).doBulkLoad(testDir, admin, table, regionLocator);</span>
<span class="source-line-no">1293</span><span id="line-1293"></span>
<span class="source-line-no">1294</span><span id="line-1294">      // Ensure data shows up</span>
<span class="source-line-no">1295</span><span id="line-1295">      int expectedRows = NMapInputFormat.getNumMapTasks(conf) * ROWSPERSPLIT;</span>
<span class="source-line-no">1296</span><span id="line-1296">      assertEquals("LoadIncrementalHFiles should put expected data in table", expectedRows + 1,</span>
<span class="source-line-no">1297</span><span id="line-1297">        util.countRows(table));</span>
<span class="source-line-no">1298</span><span id="line-1298"></span>
<span class="source-line-no">1299</span><span id="line-1299">      // should have a second StoreFile now</span>
<span class="source-line-no">1300</span><span id="line-1300">      assertEquals(2, fs.listStatus(storePath).length);</span>
<span class="source-line-no">1301</span><span id="line-1301"></span>
<span class="source-line-no">1302</span><span id="line-1302">      // minor compactions shouldn't get rid of the file</span>
<span class="source-line-no">1303</span><span id="line-1303">      admin.compact(TABLE_NAMES[0]);</span>
<span class="source-line-no">1304</span><span id="line-1304">      try {</span>
<span class="source-line-no">1305</span><span id="line-1305">        quickPoll(new Callable&lt;Boolean&gt;() {</span>
<span class="source-line-no">1306</span><span id="line-1306">          @Override</span>
<span class="source-line-no">1307</span><span id="line-1307">          public Boolean call() throws Exception {</span>
<span class="source-line-no">1308</span><span id="line-1308">            return fs.listStatus(storePath).length == 1;</span>
<span class="source-line-no">1309</span><span id="line-1309">          }</span>
<span class="source-line-no">1310</span><span id="line-1310">        }, 5000);</span>
<span class="source-line-no">1311</span><span id="line-1311">        throw new IOException("SF# = " + fs.listStatus(storePath).length);</span>
<span class="source-line-no">1312</span><span id="line-1312">      } catch (AssertionError ae) {</span>
<span class="source-line-no">1313</span><span id="line-1313">        // this is expected behavior</span>
<span class="source-line-no">1314</span><span id="line-1314">      }</span>
<span class="source-line-no">1315</span><span id="line-1315"></span>
<span class="source-line-no">1316</span><span id="line-1316">      // a major compaction should work though</span>
<span class="source-line-no">1317</span><span id="line-1317">      admin.majorCompact(TABLE_NAMES[0]);</span>
<span class="source-line-no">1318</span><span id="line-1318">      quickPoll(new Callable&lt;Boolean&gt;() {</span>
<span class="source-line-no">1319</span><span id="line-1319">        @Override</span>
<span class="source-line-no">1320</span><span id="line-1320">        public Boolean call() throws Exception {</span>
<span class="source-line-no">1321</span><span id="line-1321">          return fs.listStatus(storePath).length == 1;</span>
<span class="source-line-no">1322</span><span id="line-1322">        }</span>
<span class="source-line-no">1323</span><span id="line-1323">      }, 5000);</span>
<span class="source-line-no">1324</span><span id="line-1324"></span>
<span class="source-line-no">1325</span><span id="line-1325">    } finally {</span>
<span class="source-line-no">1326</span><span id="line-1326">      util.shutdownMiniCluster();</span>
<span class="source-line-no">1327</span><span id="line-1327">    }</span>
<span class="source-line-no">1328</span><span id="line-1328">  }</span>
<span class="source-line-no">1329</span><span id="line-1329"></span>
<span class="source-line-no">1330</span><span id="line-1330">  private void quickPoll(Callable&lt;Boolean&gt; c, int waitMs) throws Exception {</span>
<span class="source-line-no">1331</span><span id="line-1331">    int sleepMs = 10;</span>
<span class="source-line-no">1332</span><span id="line-1332">    int retries = (int) Math.ceil(((double) waitMs) / sleepMs);</span>
<span class="source-line-no">1333</span><span id="line-1333">    while (retries-- &gt; 0) {</span>
<span class="source-line-no">1334</span><span id="line-1334">      if (c.call().booleanValue()) {</span>
<span class="source-line-no">1335</span><span id="line-1335">        return;</span>
<span class="source-line-no">1336</span><span id="line-1336">      }</span>
<span class="source-line-no">1337</span><span id="line-1337">      Thread.sleep(sleepMs);</span>
<span class="source-line-no">1338</span><span id="line-1338">    }</span>
<span class="source-line-no">1339</span><span id="line-1339">    fail();</span>
<span class="source-line-no">1340</span><span id="line-1340">  }</span>
<span class="source-line-no">1341</span><span id="line-1341"></span>
<span class="source-line-no">1342</span><span id="line-1342">  public static void main(String args[]) throws Exception {</span>
<span class="source-line-no">1343</span><span id="line-1343">    new TestCellBasedHFileOutputFormat2().manualTest(args);</span>
<span class="source-line-no">1344</span><span id="line-1344">  }</span>
<span class="source-line-no">1345</span><span id="line-1345"></span>
<span class="source-line-no">1346</span><span id="line-1346">  public void manualTest(String args[]) throws Exception {</span>
<span class="source-line-no">1347</span><span id="line-1347">    Configuration conf = HBaseConfiguration.create();</span>
<span class="source-line-no">1348</span><span id="line-1348">    util = new HBaseTestingUtility(conf);</span>
<span class="source-line-no">1349</span><span id="line-1349">    if ("newtable".equals(args[0])) {</span>
<span class="source-line-no">1350</span><span id="line-1350">      TableName tname = TableName.valueOf(args[1]);</span>
<span class="source-line-no">1351</span><span id="line-1351">      byte[][] splitKeys = generateRandomSplitKeys(4);</span>
<span class="source-line-no">1352</span><span id="line-1352">      Table table = util.createTable(tname, FAMILIES, splitKeys);</span>
<span class="source-line-no">1353</span><span id="line-1353">    } else if ("incremental".equals(args[0])) {</span>
<span class="source-line-no">1354</span><span id="line-1354">      TableName tname = TableName.valueOf(args[1]);</span>
<span class="source-line-no">1355</span><span id="line-1355">      try (Connection c = ConnectionFactory.createConnection(conf); Admin admin = c.getAdmin();</span>
<span class="source-line-no">1356</span><span id="line-1356">        RegionLocator regionLocator = c.getRegionLocator(tname)) {</span>
<span class="source-line-no">1357</span><span id="line-1357">        Path outDir = new Path("incremental-out");</span>
<span class="source-line-no">1358</span><span id="line-1358">        runIncrementalPELoad(conf,</span>
<span class="source-line-no">1359</span><span id="line-1359">          Arrays.asList(</span>
<span class="source-line-no">1360</span><span id="line-1360">            new HFileOutputFormat2.TableInfo(admin.getTableDescriptor(tname), regionLocator)),</span>
<span class="source-line-no">1361</span><span id="line-1361">          outDir, false);</span>
<span class="source-line-no">1362</span><span id="line-1362">      }</span>
<span class="source-line-no">1363</span><span id="line-1363">    } else {</span>
<span class="source-line-no">1364</span><span id="line-1364">      throw new RuntimeException("usage: TestHFileOutputFormat2 newtable | incremental");</span>
<span class="source-line-no">1365</span><span id="line-1365">    }</span>
<span class="source-line-no">1366</span><span id="line-1366">  }</span>
<span class="source-line-no">1367</span><span id="line-1367"></span>
<span class="source-line-no">1368</span><span id="line-1368">  @Test</span>
<span class="source-line-no">1369</span><span id="line-1369">  public void testBlockStoragePolicy() throws Exception {</span>
<span class="source-line-no">1370</span><span id="line-1370">    util = new HBaseTestingUtility();</span>
<span class="source-line-no">1371</span><span id="line-1371">    Configuration conf = util.getConfiguration();</span>
<span class="source-line-no">1372</span><span id="line-1372">    conf.set(HFileOutputFormat2.STORAGE_POLICY_PROPERTY, "ALL_SSD");</span>
<span class="source-line-no">1373</span><span id="line-1373"></span>
<span class="source-line-no">1374</span><span id="line-1374">    conf.set(</span>
<span class="source-line-no">1375</span><span id="line-1375">      HFileOutputFormat2.STORAGE_POLICY_PROPERTY_CF_PREFIX + Bytes</span>
<span class="source-line-no">1376</span><span id="line-1376">        .toString(HFileOutputFormat2.combineTableNameSuffix(TABLE_NAMES[0].getName(), FAMILIES[0])),</span>
<span class="source-line-no">1377</span><span id="line-1377">      "ONE_SSD");</span>
<span class="source-line-no">1378</span><span id="line-1378">    Path cf1Dir = new Path(util.getDataTestDir(), Bytes.toString(FAMILIES[0]));</span>
<span class="source-line-no">1379</span><span id="line-1379">    Path cf2Dir = new Path(util.getDataTestDir(), Bytes.toString(FAMILIES[1]));</span>
<span class="source-line-no">1380</span><span id="line-1380">    util.startMiniDFSCluster(3);</span>
<span class="source-line-no">1381</span><span id="line-1381">    FileSystem fs = util.getDFSCluster().getFileSystem();</span>
<span class="source-line-no">1382</span><span id="line-1382">    try {</span>
<span class="source-line-no">1383</span><span id="line-1383">      fs.mkdirs(cf1Dir);</span>
<span class="source-line-no">1384</span><span id="line-1384">      fs.mkdirs(cf2Dir);</span>
<span class="source-line-no">1385</span><span id="line-1385"></span>
<span class="source-line-no">1386</span><span id="line-1386">      // the original block storage policy would be HOT</span>
<span class="source-line-no">1387</span><span id="line-1387">      String spA = getStoragePolicyName(fs, cf1Dir);</span>
<span class="source-line-no">1388</span><span id="line-1388">      String spB = getStoragePolicyName(fs, cf2Dir);</span>
<span class="source-line-no">1389</span><span id="line-1389">      LOG.debug("Storage policy of cf 0: [" + spA + "].");</span>
<span class="source-line-no">1390</span><span id="line-1390">      LOG.debug("Storage policy of cf 1: [" + spB + "].");</span>
<span class="source-line-no">1391</span><span id="line-1391">      assertEquals("HOT", spA);</span>
<span class="source-line-no">1392</span><span id="line-1392">      assertEquals("HOT", spB);</span>
<span class="source-line-no">1393</span><span id="line-1393"></span>
<span class="source-line-no">1394</span><span id="line-1394">      // alter table cf schema to change storage policies</span>
<span class="source-line-no">1395</span><span id="line-1395">      HFileOutputFormat2.configureStoragePolicy(conf, fs,</span>
<span class="source-line-no">1396</span><span id="line-1396">        HFileOutputFormat2.combineTableNameSuffix(TABLE_NAMES[0].getName(), FAMILIES[0]), cf1Dir);</span>
<span class="source-line-no">1397</span><span id="line-1397">      HFileOutputFormat2.configureStoragePolicy(conf, fs,</span>
<span class="source-line-no">1398</span><span id="line-1398">        HFileOutputFormat2.combineTableNameSuffix(TABLE_NAMES[0].getName(), FAMILIES[1]), cf2Dir);</span>
<span class="source-line-no">1399</span><span id="line-1399">      spA = getStoragePolicyName(fs, cf1Dir);</span>
<span class="source-line-no">1400</span><span id="line-1400">      spB = getStoragePolicyName(fs, cf2Dir);</span>
<span class="source-line-no">1401</span><span id="line-1401">      LOG.debug("Storage policy of cf 0: [" + spA + "].");</span>
<span class="source-line-no">1402</span><span id="line-1402">      LOG.debug("Storage policy of cf 1: [" + spB + "].");</span>
<span class="source-line-no">1403</span><span id="line-1403">      assertNotNull(spA);</span>
<span class="source-line-no">1404</span><span id="line-1404">      assertEquals("ONE_SSD", spA);</span>
<span class="source-line-no">1405</span><span id="line-1405">      assertNotNull(spB);</span>
<span class="source-line-no">1406</span><span id="line-1406">      assertEquals("ALL_SSD", spB);</span>
<span class="source-line-no">1407</span><span id="line-1407">    } finally {</span>
<span class="source-line-no">1408</span><span id="line-1408">      fs.delete(cf1Dir, true);</span>
<span class="source-line-no">1409</span><span id="line-1409">      fs.delete(cf2Dir, true);</span>
<span class="source-line-no">1410</span><span id="line-1410">      util.shutdownMiniDFSCluster();</span>
<span class="source-line-no">1411</span><span id="line-1411">    }</span>
<span class="source-line-no">1412</span><span id="line-1412">  }</span>
<span class="source-line-no">1413</span><span id="line-1413"></span>
<span class="source-line-no">1414</span><span id="line-1414">  private String getStoragePolicyName(FileSystem fs, Path path) {</span>
<span class="source-line-no">1415</span><span id="line-1415">    try {</span>
<span class="source-line-no">1416</span><span id="line-1416">      Object blockStoragePolicySpi = ReflectionUtils.invokeMethod(fs, "getStoragePolicy", path);</span>
<span class="source-line-no">1417</span><span id="line-1417">      return (String) ReflectionUtils.invokeMethod(blockStoragePolicySpi, "getName");</span>
<span class="source-line-no">1418</span><span id="line-1418">    } catch (Exception e) {</span>
<span class="source-line-no">1419</span><span id="line-1419">      // Maybe fail because of using old HDFS version, try the old way</span>
<span class="source-line-no">1420</span><span id="line-1420">      if (LOG.isTraceEnabled()) {</span>
<span class="source-line-no">1421</span><span id="line-1421">        LOG.trace("Failed to get policy directly", e);</span>
<span class="source-line-no">1422</span><span id="line-1422">      }</span>
<span class="source-line-no">1423</span><span id="line-1423">      String policy = getStoragePolicyNameForOldHDFSVersion(fs, path);</span>
<span class="source-line-no">1424</span><span id="line-1424">      return policy == null ? "HOT" : policy;// HOT by default</span>
<span class="source-line-no">1425</span><span id="line-1425">    }</span>
<span class="source-line-no">1426</span><span id="line-1426">  }</span>
<span class="source-line-no">1427</span><span id="line-1427"></span>
<span class="source-line-no">1428</span><span id="line-1428">  private String getStoragePolicyNameForOldHDFSVersion(FileSystem fs, Path path) {</span>
<span class="source-line-no">1429</span><span id="line-1429">    try {</span>
<span class="source-line-no">1430</span><span id="line-1430">      if (fs instanceof DistributedFileSystem) {</span>
<span class="source-line-no">1431</span><span id="line-1431">        DistributedFileSystem dfs = (DistributedFileSystem) fs;</span>
<span class="source-line-no">1432</span><span id="line-1432">        HdfsFileStatus status = dfs.getClient().getFileInfo(path.toUri().getPath());</span>
<span class="source-line-no">1433</span><span id="line-1433">        if (null != status) {</span>
<span class="source-line-no">1434</span><span id="line-1434">          byte storagePolicyId = status.getStoragePolicy();</span>
<span class="source-line-no">1435</span><span id="line-1435">          Field idUnspecified = BlockStoragePolicySuite.class.getField("ID_UNSPECIFIED");</span>
<span class="source-line-no">1436</span><span id="line-1436">          if (storagePolicyId != idUnspecified.getByte(BlockStoragePolicySuite.class)) {</span>
<span class="source-line-no">1437</span><span id="line-1437">            BlockStoragePolicy[] policies = dfs.getStoragePolicies();</span>
<span class="source-line-no">1438</span><span id="line-1438">            for (BlockStoragePolicy policy : policies) {</span>
<span class="source-line-no">1439</span><span id="line-1439">              if (policy.getId() == storagePolicyId) {</span>
<span class="source-line-no">1440</span><span id="line-1440">                return policy.getName();</span>
<span class="source-line-no">1441</span><span id="line-1441">              }</span>
<span class="source-line-no">1442</span><span id="line-1442">            }</span>
<span class="source-line-no">1443</span><span id="line-1443">          }</span>
<span class="source-line-no">1444</span><span id="line-1444">        }</span>
<span class="source-line-no">1445</span><span id="line-1445">      }</span>
<span class="source-line-no">1446</span><span id="line-1446">    } catch (Throwable e) {</span>
<span class="source-line-no">1447</span><span id="line-1447">      LOG.warn("failed to get block storage policy of [" + path + "]", e);</span>
<span class="source-line-no">1448</span><span id="line-1448">    }</span>
<span class="source-line-no">1449</span><span id="line-1449"></span>
<span class="source-line-no">1450</span><span id="line-1450">    return null;</span>
<span class="source-line-no">1451</span><span id="line-1451">  }</span>
<span class="source-line-no">1452</span><span id="line-1452">}</span>




























































</pre>
</div>
</main>
</body>
</html>
