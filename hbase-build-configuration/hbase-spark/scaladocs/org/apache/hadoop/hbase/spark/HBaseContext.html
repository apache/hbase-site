<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html>
        <head>
          <title>HBaseContext - Apache HBase - Spark 3.0.0-SNAPSHOT API - org.apache.hadoop.hbase.spark.HBaseContext</title>
          <meta name="description" content="HBaseContext - Apache HBase - Spark 3.0.0 - SNAPSHOT API - org.apache.hadoop.hbase.spark.HBaseContext" />
          <meta name="keywords" content="HBaseContext Apache HBase Spark 3.0.0 SNAPSHOT API org.apache.hadoop.hbase.spark.HBaseContext" />
          <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
          
      <link href="../../../../../lib/template.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../../../../lib/diagrams.css" media="screen" type="text/css" rel="stylesheet" id="diagrams-css" />
      <script type="text/javascript">
         if(top === self) {
            var url = '../../../../../index.html';
            var hash = 'org.apache.hadoop.hbase.spark.HBaseContext';
            var anchor = window.location.hash;
            var anchor_opt = '';
            if (anchor.length >= 1)
              anchor_opt = '@' + anchor.substring(1);
            window.location.href = url + '#' + hash + anchor_opt;
         }
   	  </script>
    
        </head>
        <body class="type">
      <div id="definition">
        <img src="../../../../../lib/class_big.png" />
        <p id="owner"><a href="../../../../package.html" class="extype" name="org">org</a>.<a href="../../../package.html" class="extype" name="org.apache">apache</a>.<a href="../../package.html" class="extype" name="org.apache.hadoop">hadoop</a>.<a href="../package.html" class="extype" name="org.apache.hadoop.hbase">hbase</a>.<a href="package.html" class="extype" name="org.apache.hadoop.hbase.spark">spark</a></p>
        <h1>HBaseContext</h1>
      </div>

      <h4 id="signature" class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <span class="name">HBaseContext</span><span class="result"> extends <span class="extype" name="java.io.Serializable">Serializable</span> with <span class="extype" name="org.apache.spark.Logging">Logging</span></span>
      </span>
      </h4>
      
          <div id="comment" class="fullcommenttop"><div class="comment cmt"><p>HBaseContext is a façade for HBase operations
like bulk put, get, increment, delete, and scan</p><p>HBaseContext will take the responsibilities
of disseminating the configuration information
to the working and managing the life cycle of Connections.
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Public</span><span class="args">()</span>
              
        </dd></dl><div class="toggleContainer block">
          <span class="toggle">Linear Supertypes</span>
          <div class="superTypes hiddenContent"><span class="extype" name="org.apache.spark.Logging">Logging</span>, <span class="extype" name="java.io.Serializable">Serializable</span>, <span class="extype" name="scala.AnyRef">AnyRef</span>, <span class="extype" name="scala.Any">Any</span></div>
        </div></div>
        

      <div id="mbrsel">
        <div id="textfilter"><span class="pre"></span><span class="input"><input id="mbrsel-input" type="text" accesskey="/" /></span><span class="post"></span></div>
        <div id="order">
              <span class="filtertype">Ordering</span>
              <ol>
                
                <li class="alpha in"><span>Alphabetic</span></li>
                <li class="inherit out"><span>By inheritance</span></li>
              </ol>
            </div>
        <div id="ancestors">
                <span class="filtertype">Inherited<br />
                </span>
                <ol id="linearization">
                  <li class="in" name="org.apache.hadoop.hbase.spark.HBaseContext"><span>HBaseContext</span></li><li class="in" name="org.apache.spark.Logging"><span>Logging</span></li><li class="in" name="java.io.Serializable"><span>Serializable</span></li><li class="in" name="scala.AnyRef"><span>AnyRef</span></li><li class="in" name="scala.Any"><span>Any</span></li>
                </ol>
              </div><div id="ancestors">
            <span class="filtertype"></span>
            <ol>
              <li class="hideall out"><span>Hide All</span></li>
              <li class="showall in"><span>Show all</span></li>
            </ol>
            <a href="http://docs.scala-lang.org/overviews/scaladoc/usage.html#members" target="_blank">Learn more about member selection</a>
          </div>
        <div id="visbl">
            <span class="filtertype">Visibility</span>
            <ol><li class="public in"><span>Public</span></li><li class="all out"><span>All</span></li></ol>
          </div>
      </div>

      <div id="template">
        <div id="allMembers">
        <div id="constructors" class="members">
              <h3>Instance Constructors</h3>
              <ol><li name="org.apache.hadoop.hbase.spark.HBaseContext#&lt;init&gt;" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="&lt;init&gt;(sc:org.apache.spark.SparkContext,config:org.apache.hadoop.conf.Configuration,tmpHdfsConfgFile:String):org.apache.hadoop.hbase.spark.HBaseContext"></a>
      <a id="&lt;init&gt;:HBaseContext"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">new</span>
      </span>
      <span class="symbol">
        <span class="name">HBaseContext</span><span class="params">(<span name="sc">sc: <span class="extype" name="org.apache.spark.SparkContext">SparkContext</span></span>, <span name="config">config: <span class="extype" name="org.apache.hadoop.conf.Configuration">Configuration</span></span>, <span name="tmpHdfsConfgFile">tmpHdfsConfgFile: <span class="extype" name="scala.Predef.String">String</span> = <span class="symbol">null</span></span>)</span>
      </span>
      </h4>
      
    </li></ol>
            </div>

        <div id="types" class="types members">
              <h3>Type Members</h3>
              <ol><li name="org.apache.hadoop.hbase.spark.HBaseContext.WriterLength" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="WriterLengthextendsAnyRef"></a>
      <a id="WriterLength:WriterLength"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="HBaseContext$WriterLength.html"><span class="name">WriterLength</span></a><span class="result"> extends <span class="extype" name="scala.AnyRef">AnyRef</span></span>
      </span>
      </h4>
      <p class="comment cmt">This is a wrapper class around StoreFileWriter.</p>
    </li></ol>
            </div>

        

        <div id="values" class="values members">
              <h3>Value Members</h3>
              <ol><li name="scala.AnyRef#!=" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="!=(x$1:AnyRef):Boolean"></a>
      <a id="!=(AnyRef):Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span title="gt4s: $bang$eq" class="name">!=</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.AnyRef">AnyRef</span></span>)</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd></dl></div>
    </li><li name="scala.Any#!=" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="!=(x$1:Any):Boolean"></a>
      <a id="!=(Any):Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span title="gt4s: $bang$eq" class="name">!=</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.Any">Any</span></span>)</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>Any</dd></dl></div>
    </li><li name="scala.AnyRef###" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="##():Int"></a>
      <a id="##():Int"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span title="gt4s: $hash$hash" class="name">##</span><span class="params">()</span><span class="result">: <span class="extype" name="scala.Int">Int</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div>
    </li><li name="scala.AnyRef#==" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="==(x$1:AnyRef):Boolean"></a>
      <a id="==(AnyRef):Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span title="gt4s: $eq$eq" class="name">==</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.AnyRef">AnyRef</span></span>)</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd></dl></div>
    </li><li name="scala.Any#==" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="==(x$1:Any):Boolean"></a>
      <a id="==(Any):Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span title="gt4s: $eq$eq" class="name">==</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.Any">Any</span></span>)</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>Any</dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#appliedCredentials" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="appliedCredentials:Boolean"></a>
      <a id="appliedCredentials:Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">var</span>
      </span>
      <span class="symbol">
        <span class="name">appliedCredentials</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#applyCreds" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="applyCreds[T]():Unit"></a>
      <a id="applyCreds[T]():Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">applyCreds</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">()</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      
    </li><li name="scala.Any#asInstanceOf" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="asInstanceOf[T0]:T0"></a>
      <a id="asInstanceOf[T0]:T0"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">asInstanceOf</span><span class="tparams">[<span name="T0">T0</span>]</span><span class="result">: <span class="extype" name="scala.Any.asInstanceOf.T0">T0</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>Any</dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#broadcastedConf" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="broadcastedConf:org.apache.spark.broadcast.Broadcast[org.apache.spark.SerializableWritable[org.apache.hadoop.conf.Configuration]]"></a>
      <a id="broadcastedConf:Broadcast[SerializableWritable[Configuration]]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">val</span>
      </span>
      <span class="symbol">
        <span class="name">broadcastedConf</span><span class="result">: <span class="extype" name="org.apache.spark.broadcast.Broadcast">Broadcast</span>[<span class="extype" name="org.apache.spark.SerializableWritable">SerializableWritable</span>[<span class="extype" name="org.apache.hadoop.conf.Configuration">Configuration</span>]]</span>
      </span>
      </h4>
      
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#bulkDelete" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="bulkDelete[T](rdd:org.apache.spark.rdd.RDD[T],tableName:org.apache.hadoop.hbase.TableName,f:T=&gt;org.apache.hadoop.hbase.client.Delete,batchSize:Integer):Unit"></a>
      <a id="bulkDelete[T](RDD[T],TableName,(T)⇒Delete,Integer):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">bulkDelete</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="rdd">rdd: <span class="extype" name="org.apache.spark.rdd.RDD">RDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.bulkDelete.T">T</span>]</span>, <span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="f">f: (<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.bulkDelete.T">T</span>) ⇒ <span class="extype" name="org.apache.hadoop.hbase.client.Delete">Delete</span></span>, <span name="batchSize">batchSize: <span class="extype" name="java.lang.Integer">Integer</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple abstraction over the HBaseContext.</p><div class="fullcomment"><div class="comment cmt"><p>A simple abstraction over the HBaseContext.foreachPartition method.</p><p>It allow addition support for a user to take a RDD and generate delete
and send them to HBase.  The complexity of managing the Connection is
removed from the developer
</p></div><dl class="paramcmts block"><dt class="param">rdd</dt><dd class="cmt"><p>Original RDD with data to iterate over</p></dd><dt class="param">tableName</dt><dd class="cmt"><p>The name of the table to delete from</p></dd><dt class="param">f</dt><dd class="cmt"><p>Function to convert a value in the RDD to a
                 HBase Deletes</p></dd><dt class="param">batchSize</dt><dd class="cmt"><p>The number of delete to batch before sending to HBase
</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#bulkGet" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="bulkGet[T,U](tableName:org.apache.hadoop.hbase.TableName,batchSize:Integer,rdd:org.apache.spark.rdd.RDD[T],makeGet:T=&gt;org.apache.hadoop.hbase.client.Get,convertResult:org.apache.hadoop.hbase.client.Result=&gt;U)(implicitevidence$3:scala.reflect.ClassTag[U]):org.apache.spark.rdd.RDD[U]"></a>
      <a id="bulkGet[T,U](TableName,Integer,RDD[T],(T)⇒Get,(Result)⇒U)(ClassTag[U]):RDD[U]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">bulkGet</span><span class="tparams">[<span name="T">T</span>, <span name="U">U</span>]</span><span class="params">(<span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="batchSize">batchSize: <span class="extype" name="java.lang.Integer">Integer</span></span>, <span name="rdd">rdd: <span class="extype" name="org.apache.spark.rdd.RDD">RDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.bulkGet.T">T</span>]</span>, <span name="makeGet">makeGet: (<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.bulkGet.T">T</span>) ⇒ <span class="extype" name="org.apache.hadoop.hbase.client.Get">Get</span></span>, <span name="convertResult">convertResult: (<span class="extype" name="org.apache.hadoop.hbase.client.Result">Result</span>) ⇒ <span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.bulkGet.U">U</span></span>)</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <span class="extype" name="scala.reflect.ClassTag">ClassTag</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.bulkGet.U">U</span>]</span>)</span><span class="result">: <span class="extype" name="org.apache.spark.rdd.RDD">RDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.bulkGet.U">U</span>]</span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple abstraction over the HBaseContext.</p><div class="fullcomment"><div class="comment cmt"><p>A simple abstraction over the HBaseContext.mapPartition method.</p><p>It allow addition support for a user to take a RDD and generates a
new RDD based on Gets and the results they bring back from HBase
</p></div><dl class="paramcmts block"><dt class="param">tableName</dt><dd class="cmt"><p>The name of the table to get from</p></dd><dt class="param">rdd</dt><dd class="cmt"><p>Original RDD with data to iterate over</p></dd><dt class="param">makeGet</dt><dd class="cmt"><p>function to convert a value in the RDD to a
                  HBase Get</p></dd><dt class="param">convertResult</dt><dd class="cmt"><p>This will convert the HBase Result object to
                  what ever the user wants to put in the resulting
                  RDD
return            new RDD that is created by the Get to HBase
</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#bulkLoad" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="bulkLoad[T](rdd:org.apache.spark.rdd.RDD[T],tableName:org.apache.hadoop.hbase.TableName,flatMap:T=&gt;Iterator[(org.apache.hadoop.hbase.spark.KeyFamilyQualifier,Array[Byte])],stagingDir:String,familyHFileWriteOptionsMap:java.util.Map[Array[Byte],org.apache.hadoop.hbase.spark.FamilyHFileWriteOptions],compactionExclude:Boolean,maxSize:Long):Unit"></a>
      <a id="bulkLoad[T](RDD[T],TableName,(T)⇒Iterator[(KeyFamilyQualifier,Array[Byte])],String,Map[Array[Byte],FamilyHFileWriteOptions],Boolean,Long):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">bulkLoad</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="rdd">rdd: <span class="extype" name="org.apache.spark.rdd.RDD">RDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.bulkLoad.T">T</span>]</span>, <span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="flatMap">flatMap: (<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.bulkLoad.T">T</span>) ⇒ <span class="extype" name="scala.Iterator">Iterator</span>[(<a href="KeyFamilyQualifier.html" class="extype" name="org.apache.hadoop.hbase.spark.KeyFamilyQualifier">KeyFamilyQualifier</a>, <span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Byte">Byte</span>])]</span>, <span name="stagingDir">stagingDir: <span class="extype" name="scala.Predef.String">String</span></span>, <span name="familyHFileWriteOptionsMap">familyHFileWriteOptionsMap: <span class="extype" name="java.util.Map">Map</span>[<span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Byte">Byte</span>], <a href="FamilyHFileWriteOptions.html" class="extype" name="org.apache.hadoop.hbase.spark.FamilyHFileWriteOptions">FamilyHFileWriteOptions</a>] = <span class="defval" name="new util.HashMap[Array[Byte], FamilyHFileWriteOptions]">...</span></span>, <span name="compactionExclude">compactionExclude: <span class="extype" name="scala.Boolean">Boolean</span> = <span class="symbol">false</span></span>, <span name="maxSize">maxSize: <span class="extype" name="scala.Long">Long</span> = <span class="symbol"><span class="name"><a href="../../../../package.html">HConstants.DEFAULT_MAX_FILE_SIZE</a></span></span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">Spark Implementation of HBase Bulk load for wide rows or when
values are not already combined at the time of the map process</p><div class="fullcomment"><div class="comment cmt"><p>Spark Implementation of HBase Bulk load for wide rows or when
values are not already combined at the time of the map process</p><p>This will take the content from an existing RDD then sort and shuffle
it with respect to region splits.  The result of that sort and shuffle
will be written to HFiles.</p><p>After this function is executed the user will have to call
LoadIncrementalHFiles.doBulkLoad(...) to move the files into HBase</p><p>Also note this version of bulk load is different from past versions in
that it includes the qualifier as part of the sort process. The
reason for this is to be able to support rows will very large number
of columns.
</p></div><dl class="paramcmts block"><dt class="tparam">T</dt><dd class="cmt"><p>The Type of values in the original RDD
</p></dd><dt class="param">rdd</dt><dd class="cmt"><p>The RDD we are bulk loading from</p></dd><dt class="param">tableName</dt><dd class="cmt"><p>The HBase table we are loading into</p></dd><dt class="param">flatMap</dt><dd class="cmt"><p>A flapMap function that will make every
                                      row in the RDD
                                      into N cells for the bulk load</p></dd><dt class="param">stagingDir</dt><dd class="cmt"><p>The location on the FileSystem to bulk load into</p></dd><dt class="param">familyHFileWriteOptionsMap</dt><dd class="cmt"><p>Options that will define how the HFile for a
                                      column family is written</p></dd><dt class="param">compactionExclude</dt><dd class="cmt"><p>Compaction excluded for the HFiles</p></dd><dt class="param">maxSize</dt><dd class="cmt"><p>Max size for the HFiles before they roll</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#bulkLoadThinRows" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="bulkLoadThinRows[T](rdd:org.apache.spark.rdd.RDD[T],tableName:org.apache.hadoop.hbase.TableName,mapFunction:T=&gt;(org.apache.hadoop.hbase.spark.ByteArrayWrapper,org.apache.hadoop.hbase.spark.FamiliesQualifiersValues),stagingDir:String,familyHFileWriteOptionsMap:java.util.Map[Array[Byte],org.apache.hadoop.hbase.spark.FamilyHFileWriteOptions],compactionExclude:Boolean,maxSize:Long):Unit"></a>
      <a id="bulkLoadThinRows[T](RDD[T],TableName,(T)⇒(ByteArrayWrapper,FamiliesQualifiersValues),String,Map[Array[Byte],FamilyHFileWriteOptions],Boolean,Long):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">bulkLoadThinRows</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="rdd">rdd: <span class="extype" name="org.apache.spark.rdd.RDD">RDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.bulkLoadThinRows.T">T</span>]</span>, <span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="mapFunction">mapFunction: (<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.bulkLoadThinRows.T">T</span>) ⇒ (<a href="ByteArrayWrapper.html" class="extype" name="org.apache.hadoop.hbase.spark.ByteArrayWrapper">ByteArrayWrapper</a>, <a href="FamiliesQualifiersValues.html" class="extype" name="org.apache.hadoop.hbase.spark.FamiliesQualifiersValues">FamiliesQualifiersValues</a>)</span>, <span name="stagingDir">stagingDir: <span class="extype" name="scala.Predef.String">String</span></span>, <span name="familyHFileWriteOptionsMap">familyHFileWriteOptionsMap: <span class="extype" name="java.util.Map">Map</span>[<span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Byte">Byte</span>], <a href="FamilyHFileWriteOptions.html" class="extype" name="org.apache.hadoop.hbase.spark.FamilyHFileWriteOptions">FamilyHFileWriteOptions</a>] = <span class="defval" name="new util.HashMap[Array[Byte], FamilyHFileWriteOptions]">...</span></span>, <span name="compactionExclude">compactionExclude: <span class="extype" name="scala.Boolean">Boolean</span> = <span class="symbol">false</span></span>, <span name="maxSize">maxSize: <span class="extype" name="scala.Long">Long</span> = <span class="symbol"><span class="name"><a href="../../../../package.html">HConstants.DEFAULT_MAX_FILE_SIZE</a></span></span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">Spark Implementation of HBase Bulk load for short rows some where less then
a 1000 columns.</p><div class="fullcomment"><div class="comment cmt"><p>Spark Implementation of HBase Bulk load for short rows some where less then
a 1000 columns.  This bulk load should be faster for tables will thinner
rows then the other spark implementation of bulk load that puts only one
value into a record going into a shuffle</p><p>This will take the content from an existing RDD then sort and shuffle
it with respect to region splits.  The result of that sort and shuffle
will be written to HFiles.</p><p>After this function is executed the user will have to call
LoadIncrementalHFiles.doBulkLoad(...) to move the files into HBase</p><p>In this implementation, only the rowKey is given to the shuffle as the key
and all the columns are already linked to the RowKey before the shuffle
stage.  The sorting of the qualifier is done in memory out side of the
shuffle stage</p><p>Also make sure that incoming RDDs only have one record for every row key.
</p></div><dl class="paramcmts block"><dt class="tparam">T</dt><dd class="cmt"><p>The Type of values in the original RDD
</p></dd><dt class="param">rdd</dt><dd class="cmt"><p>The RDD we are bulk loading from</p></dd><dt class="param">tableName</dt><dd class="cmt"><p>The HBase table we are loading into</p></dd><dt class="param">mapFunction</dt><dd class="cmt"><p>A function that will convert the RDD records to
                                      the key value format used for the shuffle to prep
                                      for writing to the bulk loaded HFiles</p></dd><dt class="param">stagingDir</dt><dd class="cmt"><p>The location on the FileSystem to bulk load into</p></dd><dt class="param">familyHFileWriteOptionsMap</dt><dd class="cmt"><p>Options that will define how the HFile for a
                                      column family is written</p></dd><dt class="param">compactionExclude</dt><dd class="cmt"><p>Compaction excluded for the HFiles</p></dd><dt class="param">maxSize</dt><dd class="cmt"><p>Max size for the HFiles before they roll</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#bulkPut" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="bulkPut[T](rdd:org.apache.spark.rdd.RDD[T],tableName:org.apache.hadoop.hbase.TableName,f:T=&gt;org.apache.hadoop.hbase.client.Put):Unit"></a>
      <a id="bulkPut[T](RDD[T],TableName,(T)⇒Put):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">bulkPut</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="rdd">rdd: <span class="extype" name="org.apache.spark.rdd.RDD">RDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.bulkPut.T">T</span>]</span>, <span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="f">f: (<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.bulkPut.T">T</span>) ⇒ <span class="extype" name="org.apache.hadoop.hbase.client.Put">Put</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple abstraction over the HBaseContext.</p><div class="fullcomment"><div class="comment cmt"><p>A simple abstraction over the HBaseContext.foreachPartition method.</p><p>It allow addition support for a user to take RDD
and generate puts and send them to HBase.
The complexity of managing the Connection is
removed from the developer
</p></div><dl class="paramcmts block"><dt class="param">rdd</dt><dd class="cmt"><p>Original RDD with data to iterate over</p></dd><dt class="param">tableName</dt><dd class="cmt"><p>The name of the table to put into</p></dd><dt class="param">f</dt><dd class="cmt"><p>Function to convert a value in the RDD to a HBase Put
</p></dd></dl></div>
    </li><li name="scala.AnyRef#clone" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="clone():Object"></a>
      <a id="clone():AnyRef"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">clone</span><span class="params">()</span><span class="result">: <span class="extype" name="scala.AnyRef">AnyRef</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected[<a href="../../../../../java$lang.html" class="extype" name="java.lang">java.lang</a>] </dd><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd>
                <span class="name">@throws</span><span class="args">(<span>
      
      <span class="defval" name="classOf[java.lang.CloneNotSupportedException]">...</span>
    </span>)</span>
              
        </dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#config" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="config:org.apache.hadoop.conf.Configuration"></a>
      <a id="config:Configuration"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">val</span>
      </span>
      <span class="symbol">
        <span class="name">config</span><span class="result">: <span class="extype" name="org.apache.hadoop.conf.Configuration">Configuration</span></span>
      </span>
      </h4>
      
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#credentials" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="credentials:org.apache.hadoop.security.Credentials"></a>
      <a id="credentials:Credentials"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">var</span>
      </span>
      <span class="symbol">
        <span class="name">credentials</span><span class="result">: <span class="extype" name="org.apache.hadoop.security.Credentials">Credentials</span></span>
      </span>
      </h4>
      
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#credentialsConf" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="credentialsConf:org.apache.spark.broadcast.Broadcast[org.apache.spark.SerializableWritable[org.apache.hadoop.security.Credentials]]"></a>
      <a id="credentialsConf:Broadcast[SerializableWritable[Credentials]]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">val</span>
      </span>
      <span class="symbol">
        <span class="name">credentialsConf</span><span class="result">: <span class="extype" name="org.apache.spark.broadcast.Broadcast">Broadcast</span>[<span class="extype" name="org.apache.spark.SerializableWritable">SerializableWritable</span>[<span class="extype" name="org.apache.hadoop.security.Credentials">Credentials</span>]]</span>
      </span>
      </h4>
      
    </li><li name="scala.AnyRef#eq" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="eq(x$1:AnyRef):Boolean"></a>
      <a id="eq(AnyRef):Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">eq</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.AnyRef">AnyRef</span></span>)</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd></dl></div>
    </li><li name="scala.AnyRef#equals" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="equals(x$1:Any):Boolean"></a>
      <a id="equals(Any):Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">equals</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.Any">Any</span></span>)</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div>
    </li><li name="scala.AnyRef#finalize" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="finalize():Unit"></a>
      <a id="finalize():Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">finalize</span><span class="params">()</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected[<a href="../../../../../java$lang.html" class="extype" name="java.lang">java.lang</a>] </dd><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd>
                <span class="name">@throws</span><span class="args">(<span>
      
      <span class="symbol">classOf[java.lang.Throwable]</span>
    </span>)</span>
              
        </dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#foreachPartition" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="foreachPartition[T](dstream:org.apache.spark.streaming.dstream.DStream[T],f:(Iterator[T],org.apache.hadoop.hbase.client.Connection)=&gt;Unit):Unit"></a>
      <a id="foreachPartition[T](DStream[T],(Iterator[T],Connection)⇒Unit):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">foreachPartition</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="dstream">dstream: <span class="extype" name="org.apache.spark.streaming.dstream.DStream">DStream</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.foreachPartition.T">T</span>]</span>, <span name="f">f: (<span class="extype" name="scala.Iterator">Iterator</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.foreachPartition.T">T</span>], <span class="extype" name="org.apache.hadoop.hbase.client.Connection">Connection</span>) ⇒ <span class="extype" name="scala.Unit">Unit</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple enrichment of the traditional Spark Streaming dStream foreach
This function differs from the original in that it offers the
developer access to a already connected Connection object</p><div class="fullcomment"><div class="comment cmt"><p>A simple enrichment of the traditional Spark Streaming dStream foreach
This function differs from the original in that it offers the
developer access to a already connected Connection object</p><p>Note: Do not close the Connection object.  All Connection
management is handled outside this method
</p></div><dl class="paramcmts block"><dt class="param">dstream</dt><dd class="cmt"><p>Original DStream with data to iterate over</p></dd><dt class="param">f</dt><dd class="cmt"><p>Function to be given a iterator to iterate through
                the DStream values and a Connection object to
                interact with HBase
</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#foreachPartition" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="foreachPartition[T](rdd:org.apache.spark.rdd.RDD[T],f:(Iterator[T],org.apache.hadoop.hbase.client.Connection)=&gt;Unit):Unit"></a>
      <a id="foreachPartition[T](RDD[T],(Iterator[T],Connection)⇒Unit):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">foreachPartition</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="rdd">rdd: <span class="extype" name="org.apache.spark.rdd.RDD">RDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.foreachPartition.T">T</span>]</span>, <span name="f">f: (<span class="extype" name="scala.Iterator">Iterator</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.foreachPartition.T">T</span>], <span class="extype" name="org.apache.hadoop.hbase.client.Connection">Connection</span>) ⇒ <span class="extype" name="scala.Unit">Unit</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple enrichment of the traditional Spark RDD foreachPartition.</p><div class="fullcomment"><div class="comment cmt"><p>A simple enrichment of the traditional Spark RDD foreachPartition.
This function differs from the original in that it offers the
developer access to a already connected Connection object</p><p>Note: Do not close the Connection object.  All Connection
management is handled outside this method
</p></div><dl class="paramcmts block"><dt class="param">rdd</dt><dd class="cmt"><p>Original RDD with data to iterate over</p></dd><dt class="param">f</dt><dd class="cmt"><p>Function to be given a iterator to iterate through
            the RDD values and a Connection object to interact
            with HBase
</p></dd></dl></div>
    </li><li name="scala.AnyRef#getClass" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="getClass():Class[_]"></a>
      <a id="getClass():Class[_]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">getClass</span><span class="params">()</span><span class="result">: <span class="extype" name="java.lang.Class">Class</span>[_]</span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div>
    </li><li name="scala.AnyRef#hashCode" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="hashCode():Int"></a>
      <a id="hashCode():Int"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">hashCode</span><span class="params">()</span><span class="result">: <span class="extype" name="scala.Int">Int</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#hbaseRDD" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="hbaseRDD(tableName:org.apache.hadoop.hbase.TableName,scans:org.apache.hadoop.hbase.client.Scan):org.apache.spark.rdd.RDD[(org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.hbase.client.Result)]"></a>
      <a id="hbaseRDD(TableName,Scan):RDD[(ImmutableBytesWritable,Result)]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">hbaseRDD</span><span class="params">(<span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="scans">scans: <span class="extype" name="org.apache.hadoop.hbase.client.Scan">Scan</span></span>)</span><span class="result">: <span class="extype" name="org.apache.spark.rdd.RDD">RDD</span>[(<span class="extype" name="org.apache.hadoop.hbase.io.ImmutableBytesWritable">ImmutableBytesWritable</span>, <span class="extype" name="org.apache.hadoop.hbase.client.Result">Result</span>)]</span>
      </span>
      </h4>
      <p class="shortcomment cmt">A overloaded version of HBaseContext hbaseRDD that defines the
type of the resulting RDD
</p><div class="fullcomment"><div class="comment cmt"><p>A overloaded version of HBaseContext hbaseRDD that defines the
type of the resulting RDD
</p></div><dl class="paramcmts block"><dt class="param">tableName</dt><dd class="cmt"><p>the name of the table to scan</p></dd><dt class="param">scans</dt><dd class="cmt"><p>the HBase scan object to use to read data from HBase</p></dd><dt>returns</dt><dd class="cmt"><p>New RDD with results from scan</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#hbaseRDD" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="hbaseRDD[U](tableName:org.apache.hadoop.hbase.TableName,scan:org.apache.hadoop.hbase.client.Scan,f:((org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.hbase.client.Result))=&gt;U)(implicitevidence$5:scala.reflect.ClassTag[U]):org.apache.spark.rdd.RDD[U]"></a>
      <a id="hbaseRDD[U](TableName,Scan,((ImmutableBytesWritable,Result))⇒U)(ClassTag[U]):RDD[U]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">hbaseRDD</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="scan">scan: <span class="extype" name="org.apache.hadoop.hbase.client.Scan">Scan</span></span>, <span name="f">f: ((<span class="extype" name="org.apache.hadoop.hbase.io.ImmutableBytesWritable">ImmutableBytesWritable</span>, <span class="extype" name="org.apache.hadoop.hbase.client.Result">Result</span>)) ⇒ <span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.hbaseRDD.U">U</span></span>)</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <span class="extype" name="scala.reflect.ClassTag">ClassTag</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.hbaseRDD.U">U</span>]</span>)</span><span class="result">: <span class="extype" name="org.apache.spark.rdd.RDD">RDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.hbaseRDD.U">U</span>]</span>
      </span>
      </h4>
      <p class="shortcomment cmt">This function will use the native HBase TableInputFormat with the
given scan object to generate a new RDD
</p><div class="fullcomment"><div class="comment cmt"><p>This function will use the native HBase TableInputFormat with the
given scan object to generate a new RDD
</p></div><dl class="paramcmts block"><dt class="param">tableName</dt><dd class="cmt"><p>the name of the table to scan</p></dd><dt class="param">scan</dt><dd class="cmt"><p>the HBase scan object to use to read data from HBase</p></dd><dt class="param">f</dt><dd class="cmt"><p>function to convert a Result object from HBase into
                  what the user wants in the final generated RDD</p></dd><dt>returns</dt><dd class="cmt"><p>new RDD with results from scan
</p></dd></dl></div>
    </li><li name="scala.Any#isInstanceOf" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="isInstanceOf[T0]:Boolean"></a>
      <a id="isInstanceOf[T0]:Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">isInstanceOf</span><span class="tparams">[<span name="T0">T0</span>]</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>Any</dd></dl></div>
    </li><li name="org.apache.spark.Logging#isTraceEnabled" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="isTraceEnabled():Boolean"></a>
      <a id="isTraceEnabled():Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">isTraceEnabled</span><span class="params">()</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected </dd><dt>Definition Classes</dt><dd>Logging</dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#job" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="job:org.apache.hadoop.mapreduce.Job"></a>
      <a id="job:Job"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">val</span>
      </span>
      <span class="symbol">
        <span class="name">job</span><span class="result">: <span class="extype" name="org.apache.hadoop.mapreduce.Job">Job</span></span>
      </span>
      </h4>
      
    </li><li name="org.apache.spark.Logging#log" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="log:org.slf4j.Logger"></a>
      <a id="log:Logger"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">log</span><span class="result">: <span class="extype" name="org.slf4j.Logger">Logger</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected </dd><dt>Definition Classes</dt><dd>Logging</dd></dl></div>
    </li><li name="org.apache.spark.Logging#logDebug" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="logDebug(msg:=&gt;String,throwable:Throwable):Unit"></a>
      <a id="logDebug(⇒String,Throwable):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">logDebug</span><span class="params">(<span name="msg">msg: ⇒ <span class="extype" name="scala.Predef.String">String</span></span>, <span name="throwable">throwable: <span class="extype" name="scala.Throwable">Throwable</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected </dd><dt>Definition Classes</dt><dd>Logging</dd></dl></div>
    </li><li name="org.apache.spark.Logging#logDebug" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="logDebug(msg:=&gt;String):Unit"></a>
      <a id="logDebug(⇒String):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">logDebug</span><span class="params">(<span name="msg">msg: ⇒ <span class="extype" name="scala.Predef.String">String</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected </dd><dt>Definition Classes</dt><dd>Logging</dd></dl></div>
    </li><li name="org.apache.spark.Logging#logError" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="logError(msg:=&gt;String,throwable:Throwable):Unit"></a>
      <a id="logError(⇒String,Throwable):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">logError</span><span class="params">(<span name="msg">msg: ⇒ <span class="extype" name="scala.Predef.String">String</span></span>, <span name="throwable">throwable: <span class="extype" name="scala.Throwable">Throwable</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected </dd><dt>Definition Classes</dt><dd>Logging</dd></dl></div>
    </li><li name="org.apache.spark.Logging#logError" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="logError(msg:=&gt;String):Unit"></a>
      <a id="logError(⇒String):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">logError</span><span class="params">(<span name="msg">msg: ⇒ <span class="extype" name="scala.Predef.String">String</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected </dd><dt>Definition Classes</dt><dd>Logging</dd></dl></div>
    </li><li name="org.apache.spark.Logging#logInfo" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="logInfo(msg:=&gt;String,throwable:Throwable):Unit"></a>
      <a id="logInfo(⇒String,Throwable):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">logInfo</span><span class="params">(<span name="msg">msg: ⇒ <span class="extype" name="scala.Predef.String">String</span></span>, <span name="throwable">throwable: <span class="extype" name="scala.Throwable">Throwable</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected </dd><dt>Definition Classes</dt><dd>Logging</dd></dl></div>
    </li><li name="org.apache.spark.Logging#logInfo" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="logInfo(msg:=&gt;String):Unit"></a>
      <a id="logInfo(⇒String):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">logInfo</span><span class="params">(<span name="msg">msg: ⇒ <span class="extype" name="scala.Predef.String">String</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected </dd><dt>Definition Classes</dt><dd>Logging</dd></dl></div>
    </li><li name="org.apache.spark.Logging#logName" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="logName:String"></a>
      <a id="logName:String"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">logName</span><span class="result">: <span class="extype" name="java.lang.String">String</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected </dd><dt>Definition Classes</dt><dd>Logging</dd></dl></div>
    </li><li name="org.apache.spark.Logging#logTrace" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="logTrace(msg:=&gt;String,throwable:Throwable):Unit"></a>
      <a id="logTrace(⇒String,Throwable):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">logTrace</span><span class="params">(<span name="msg">msg: ⇒ <span class="extype" name="scala.Predef.String">String</span></span>, <span name="throwable">throwable: <span class="extype" name="scala.Throwable">Throwable</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected </dd><dt>Definition Classes</dt><dd>Logging</dd></dl></div>
    </li><li name="org.apache.spark.Logging#logTrace" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="logTrace(msg:=&gt;String):Unit"></a>
      <a id="logTrace(⇒String):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">logTrace</span><span class="params">(<span name="msg">msg: ⇒ <span class="extype" name="scala.Predef.String">String</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected </dd><dt>Definition Classes</dt><dd>Logging</dd></dl></div>
    </li><li name="org.apache.spark.Logging#logWarning" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="logWarning(msg:=&gt;String,throwable:Throwable):Unit"></a>
      <a id="logWarning(⇒String,Throwable):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">logWarning</span><span class="params">(<span name="msg">msg: ⇒ <span class="extype" name="scala.Predef.String">String</span></span>, <span name="throwable">throwable: <span class="extype" name="scala.Throwable">Throwable</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected </dd><dt>Definition Classes</dt><dd>Logging</dd></dl></div>
    </li><li name="org.apache.spark.Logging#logWarning" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="logWarning(msg:=&gt;String):Unit"></a>
      <a id="logWarning(⇒String):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">logWarning</span><span class="params">(<span name="msg">msg: ⇒ <span class="extype" name="scala.Predef.String">String</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected </dd><dt>Definition Classes</dt><dd>Logging</dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#mapPartitions" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="mapPartitions[T,R](rdd:org.apache.spark.rdd.RDD[T],mp:(Iterator[T],org.apache.hadoop.hbase.client.Connection)=&gt;Iterator[R])(implicitevidence$1:scala.reflect.ClassTag[R]):org.apache.spark.rdd.RDD[R]"></a>
      <a id="mapPartitions[T,R](RDD[T],(Iterator[T],Connection)⇒Iterator[R])(ClassTag[R]):RDD[R]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">mapPartitions</span><span class="tparams">[<span name="T">T</span>, <span name="R">R</span>]</span><span class="params">(<span name="rdd">rdd: <span class="extype" name="org.apache.spark.rdd.RDD">RDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.mapPartitions.T">T</span>]</span>, <span name="mp">mp: (<span class="extype" name="scala.Iterator">Iterator</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.mapPartitions.T">T</span>], <span class="extype" name="org.apache.hadoop.hbase.client.Connection">Connection</span>) ⇒ <span class="extype" name="scala.Iterator">Iterator</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.mapPartitions.R">R</span>]</span>)</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <span class="extype" name="scala.reflect.ClassTag">ClassTag</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.mapPartitions.R">R</span>]</span>)</span><span class="result">: <span class="extype" name="org.apache.spark.rdd.RDD">RDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.mapPartitions.R">R</span>]</span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple enrichment of the traditional Spark RDD mapPartition.</p><div class="fullcomment"><div class="comment cmt"><p>A simple enrichment of the traditional Spark RDD mapPartition.
This function differs from the original in that it offers the
developer access to a already connected Connection object</p><p>Note: Do not close the Connection object.  All Connection
management is handled outside this method
</p></div><dl class="paramcmts block"><dt class="param">rdd</dt><dd class="cmt"><p>Original RDD with data to iterate over</p></dd><dt class="param">mp</dt><dd class="cmt"><p>Function to be given a iterator to iterate through
            the RDD values and a Connection object to interact
            with HBase</p></dd><dt>returns</dt><dd class="cmt"><p>Returns a new RDD generated by the user definition
            function just like normal mapPartition
</p></dd></dl></div>
    </li><li name="scala.AnyRef#ne" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ne(x$1:AnyRef):Boolean"></a>
      <a id="ne(AnyRef):Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">ne</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.AnyRef">AnyRef</span></span>)</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd></dl></div>
    </li><li name="scala.AnyRef#notify" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="notify():Unit"></a>
      <a id="notify():Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">notify</span><span class="params">()</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd></dl></div>
    </li><li name="scala.AnyRef#notifyAll" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="notifyAll():Unit"></a>
      <a id="notifyAll():Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">notifyAll</span><span class="params">()</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#streamBulkDelete" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="streamBulkDelete[T](dstream:org.apache.spark.streaming.dstream.DStream[T],tableName:org.apache.hadoop.hbase.TableName,f:T=&gt;org.apache.hadoop.hbase.client.Delete,batchSize:Integer):Unit"></a>
      <a id="streamBulkDelete[T](DStream[T],TableName,(T)⇒Delete,Integer):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">streamBulkDelete</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="dstream">dstream: <span class="extype" name="org.apache.spark.streaming.dstream.DStream">DStream</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.streamBulkDelete.T">T</span>]</span>, <span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="f">f: (<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.streamBulkDelete.T">T</span>) ⇒ <span class="extype" name="org.apache.hadoop.hbase.client.Delete">Delete</span></span>, <span name="batchSize">batchSize: <span class="extype" name="java.lang.Integer">Integer</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple abstraction over the HBaseContext.</p><div class="fullcomment"><div class="comment cmt"><p>A simple abstraction over the HBaseContext.streamBulkMutation method.</p><p>It allow addition support for a user to take a DStream and
generate Delete and send them to HBase.</p><p>The complexity of managing the Connection is
removed from the developer
</p></div><dl class="paramcmts block"><dt class="param">dstream</dt><dd class="cmt"><p>Original DStream with data to iterate over</p></dd><dt class="param">tableName</dt><dd class="cmt"><p>The name of the table to delete from</p></dd><dt class="param">f</dt><dd class="cmt"><p>function to convert a value in the DStream to a
                  HBase Delete</p></dd><dt class="param">batchSize</dt><dd class="cmt"><p>The number of deletes to batch before sending to HBase
</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#streamBulkGet" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="streamBulkGet[T,U](tableName:org.apache.hadoop.hbase.TableName,batchSize:Integer,dStream:org.apache.spark.streaming.dstream.DStream[T],makeGet:T=&gt;org.apache.hadoop.hbase.client.Get,convertResult:org.apache.hadoop.hbase.client.Result=&gt;U)(implicitevidence$4:scala.reflect.ClassTag[U]):org.apache.spark.streaming.dstream.DStream[U]"></a>
      <a id="streamBulkGet[T,U](TableName,Integer,DStream[T],(T)⇒Get,(Result)⇒U)(ClassTag[U]):DStream[U]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">streamBulkGet</span><span class="tparams">[<span name="T">T</span>, <span name="U">U</span>]</span><span class="params">(<span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="batchSize">batchSize: <span class="extype" name="java.lang.Integer">Integer</span></span>, <span name="dStream">dStream: <span class="extype" name="org.apache.spark.streaming.dstream.DStream">DStream</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.streamBulkGet.T">T</span>]</span>, <span name="makeGet">makeGet: (<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.streamBulkGet.T">T</span>) ⇒ <span class="extype" name="org.apache.hadoop.hbase.client.Get">Get</span></span>, <span name="convertResult">convertResult: (<span class="extype" name="org.apache.hadoop.hbase.client.Result">Result</span>) ⇒ <span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.streamBulkGet.U">U</span></span>)</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <span class="extype" name="scala.reflect.ClassTag">ClassTag</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.streamBulkGet.U">U</span>]</span>)</span><span class="result">: <span class="extype" name="org.apache.spark.streaming.dstream.DStream">DStream</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.streamBulkGet.U">U</span>]</span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple abstraction over the HBaseContext.</p><div class="fullcomment"><div class="comment cmt"><p>A simple abstraction over the HBaseContext.streamMap method.</p><p>It allow addition support for a user to take a DStream and
generates a new DStream based on Gets and the results
they bring back from HBase
</p></div><dl class="paramcmts block"><dt class="param">tableName</dt><dd class="cmt"><p>The name of the table to get from</p></dd><dt class="param">batchSize</dt><dd class="cmt"><p>The number of Gets to be sent in a single batch</p></dd><dt class="param">dStream</dt><dd class="cmt"><p>Original DStream with data to iterate over</p></dd><dt class="param">makeGet</dt><dd class="cmt"><p>Function to convert a value in the DStream to a
                     HBase Get</p></dd><dt class="param">convertResult</dt><dd class="cmt"><p>This will convert the HBase Result object to
                     what ever the user wants to put in the resulting
                     DStream</p></dd><dt>returns</dt><dd class="cmt"><p>A new DStream that is created by the Get to HBase
</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#streamBulkPut" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="streamBulkPut[T](dstream:org.apache.spark.streaming.dstream.DStream[T],tableName:org.apache.hadoop.hbase.TableName,f:T=&gt;org.apache.hadoop.hbase.client.Put):Unit"></a>
      <a id="streamBulkPut[T](DStream[T],TableName,(T)⇒Put):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">streamBulkPut</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="dstream">dstream: <span class="extype" name="org.apache.spark.streaming.dstream.DStream">DStream</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.streamBulkPut.T">T</span>]</span>, <span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="f">f: (<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.streamBulkPut.T">T</span>) ⇒ <span class="extype" name="org.apache.hadoop.hbase.client.Put">Put</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple abstraction over the HBaseContext.</p><div class="fullcomment"><div class="comment cmt"><p>A simple abstraction over the HBaseContext.streamMapPartition method.</p><p>It allow addition support for a user to take a DStream and
generate puts and send them to HBase.</p><p>The complexity of managing the Connection is
removed from the developer
</p></div><dl class="paramcmts block"><dt class="param">dstream</dt><dd class="cmt"><p>Original DStream with data to iterate over</p></dd><dt class="param">tableName</dt><dd class="cmt"><p>The name of the table to put into</p></dd><dt class="param">f</dt><dd class="cmt"><p>Function to convert a value in
                  the DStream to a HBase Put
</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#streamForeachPartition" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="streamForeachPartition[T](dstream:org.apache.spark.streaming.dstream.DStream[T],f:(Iterator[T],org.apache.hadoop.hbase.client.Connection)=&gt;Unit):Unit"></a>
      <a id="streamForeachPartition[T](DStream[T],(Iterator[T],Connection)⇒Unit):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">streamForeachPartition</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="dstream">dstream: <span class="extype" name="org.apache.spark.streaming.dstream.DStream">DStream</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.streamForeachPartition.T">T</span>]</span>, <span name="f">f: (<span class="extype" name="scala.Iterator">Iterator</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.streamForeachPartition.T">T</span>], <span class="extype" name="org.apache.hadoop.hbase.client.Connection">Connection</span>) ⇒ <span class="extype" name="scala.Unit">Unit</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple enrichment of the traditional Spark Streaming DStream
foreachPartition.</p><div class="fullcomment"><div class="comment cmt"><p>A simple enrichment of the traditional Spark Streaming DStream
foreachPartition.</p><p>This function differs from the original in that it offers the
developer access to a already connected Connection object</p><p>Note: Do not close the Connection object.  All Connection
management is handled outside this method</p><p>Note: Make sure to partition correctly to avoid memory issue when
      getting data from HBase
</p></div><dl class="paramcmts block"><dt class="param">dstream</dt><dd class="cmt"><p>Original DStream with data to iterate over</p></dd><dt class="param">f</dt><dd class="cmt"><p>Function to be given a iterator to iterate through
                the DStream values and a Connection object to
                interact with HBase</p></dd><dt>returns</dt><dd class="cmt"><p>Returns a new DStream generated by the user
                definition function just like normal mapPartition
</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#streamMapPartitions" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="streamMapPartitions[T,U](dstream:org.apache.spark.streaming.dstream.DStream[T],f:(Iterator[T],org.apache.hadoop.hbase.client.Connection)=&gt;Iterator[U])(implicitevidence$2:scala.reflect.ClassTag[U]):org.apache.spark.streaming.dstream.DStream[U]"></a>
      <a id="streamMapPartitions[T,U](DStream[T],(Iterator[T],Connection)⇒Iterator[U])(ClassTag[U]):DStream[U]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">streamMapPartitions</span><span class="tparams">[<span name="T">T</span>, <span name="U">U</span>]</span><span class="params">(<span name="dstream">dstream: <span class="extype" name="org.apache.spark.streaming.dstream.DStream">DStream</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.streamMapPartitions.T">T</span>]</span>, <span name="f">f: (<span class="extype" name="scala.Iterator">Iterator</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.streamMapPartitions.T">T</span>], <span class="extype" name="org.apache.hadoop.hbase.client.Connection">Connection</span>) ⇒ <span class="extype" name="scala.Iterator">Iterator</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.streamMapPartitions.U">U</span>]</span>)</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <span class="extype" name="scala.reflect.ClassTag">ClassTag</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.streamMapPartitions.U">U</span>]</span>)</span><span class="result">: <span class="extype" name="org.apache.spark.streaming.dstream.DStream">DStream</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext.streamMapPartitions.U">U</span>]</span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple enrichment of the traditional Spark Streaming DStream
mapPartition.</p><div class="fullcomment"><div class="comment cmt"><p>A simple enrichment of the traditional Spark Streaming DStream
mapPartition.</p><p>This function differs from the original in that it offers the
developer access to a already connected Connection object</p><p>Note: Do not close the Connection object.  All Connection
management is handled outside this method</p><p>Note: Make sure to partition correctly to avoid memory issue when
      getting data from HBase
</p></div><dl class="paramcmts block"><dt class="param">dstream</dt><dd class="cmt"><p>Original DStream with data to iterate over</p></dd><dt class="param">f</dt><dd class="cmt"><p>Function to be given a iterator to iterate through
                the DStream values and a Connection object to
                interact with HBase</p></dd><dt>returns</dt><dd class="cmt"><p>Returns a new DStream generated by the user
                definition function just like normal mapPartition
</p></dd></dl></div>
    </li><li name="scala.AnyRef#synchronized" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="synchronized[T0](x$1:=&gt;T0):T0"></a>
      <a id="synchronized[T0](⇒T0):T0"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">synchronized</span><span class="tparams">[<span name="T0">T0</span>]</span><span class="params">(<span name="arg0">arg0: ⇒ <span class="extype" name="java.lang.AnyRef.synchronized.T0">T0</span></span>)</span><span class="result">: <span class="extype" name="java.lang.AnyRef.synchronized.T0">T0</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#tmpHdfsConfgFile" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="tmpHdfsConfgFile:String"></a>
      <a id="tmpHdfsConfgFile:String"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">val</span>
      </span>
      <span class="symbol">
        <span class="name">tmpHdfsConfgFile</span><span class="result">: <span class="extype" name="scala.Predef.String">String</span></span>
      </span>
      </h4>
      
    </li><li name="org.apache.hadoop.hbase.spark.HBaseContext#tmpHdfsConfiguration" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="tmpHdfsConfiguration:org.apache.hadoop.conf.Configuration"></a>
      <a id="tmpHdfsConfiguration:Configuration"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">var</span>
      </span>
      <span class="symbol">
        <span class="name">tmpHdfsConfiguration</span><span class="result">: <span class="extype" name="org.apache.hadoop.conf.Configuration">Configuration</span></span>
      </span>
      </h4>
      
    </li><li name="scala.AnyRef#toString" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="toString():String"></a>
      <a id="toString():String"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">toString</span><span class="params">()</span><span class="result">: <span class="extype" name="java.lang.String">String</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div>
    </li><li name="scala.AnyRef#wait" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="wait():Unit"></a>
      <a id="wait():Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">wait</span><span class="params">()</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd>
                <span class="name">@throws</span><span class="args">(<span>
      
      <span class="defval" name="classOf[java.lang.InterruptedException]">...</span>
    </span>)</span>
              
        </dd></dl></div>
    </li><li name="scala.AnyRef#wait" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="wait(x$1:Long,x$2:Int):Unit"></a>
      <a id="wait(Long,Int):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">wait</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.Long">Long</span></span>, <span name="arg1">arg1: <span class="extype" name="scala.Int">Int</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd>
                <span class="name">@throws</span><span class="args">(<span>
      
      <span class="defval" name="classOf[java.lang.InterruptedException]">...</span>
    </span>)</span>
              
        </dd></dl></div>
    </li><li name="scala.AnyRef#wait" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="wait(x$1:Long):Unit"></a>
      <a id="wait(Long):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">wait</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.Long">Long</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd>
                <span class="name">@throws</span><span class="args">(<span>
      
      <span class="defval" name="classOf[java.lang.InterruptedException]">...</span>
    </span>)</span>
              
        </dd></dl></div>
    </li></ol>
            </div>

        

        
        </div>

        <div id="inheritedMembers">
        <div class="parent" name="org.apache.spark.Logging">
              <h3>Inherited from <span class="extype" name="org.apache.spark.Logging">Logging</span></h3>
            </div><div class="parent" name="java.io.Serializable">
              <h3>Inherited from <span class="extype" name="java.io.Serializable">Serializable</span></h3>
            </div><div class="parent" name="scala.AnyRef">
              <h3>Inherited from <span class="extype" name="scala.AnyRef">AnyRef</span></h3>
            </div><div class="parent" name="scala.Any">
              <h3>Inherited from <span class="extype" name="scala.Any">Any</span></h3>
            </div>
        
        </div>

        <div id="groupedMembers">
        <div class="group" name="Ungrouped">
              <h3>Ungrouped</h3>
              
            </div>
        </div>

      </div>

      <div id="tooltip"></div>

      <div id="footer">  </div>
      <script defer="defer" type="text/javascript" id="jquery-js" src="../../../../../lib/jquery.js"></script><script defer="defer" type="text/javascript" id="jquery-ui-js" src="../../../../../lib/jquery-ui.js"></script><script defer="defer" type="text/javascript" id="tools-tooltip-js" src="../../../../../lib/tools.tooltip.js"></script><script defer="defer" type="text/javascript" id="template-js" src="../../../../../lib/template.js"></script>
    </body>
      </html>