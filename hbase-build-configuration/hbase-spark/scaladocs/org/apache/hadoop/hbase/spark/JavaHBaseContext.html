<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html>
        <head>
          <title>JavaHBaseContext - Apache HBase - Spark 3.0.0-SNAPSHOT API - org.apache.hadoop.hbase.spark.JavaHBaseContext</title>
          <meta name="description" content="JavaHBaseContext - Apache HBase - Spark 3.0.0 - SNAPSHOT API - org.apache.hadoop.hbase.spark.JavaHBaseContext" />
          <meta name="keywords" content="JavaHBaseContext Apache HBase Spark 3.0.0 SNAPSHOT API org.apache.hadoop.hbase.spark.JavaHBaseContext" />
          <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
          
      <link href="../../../../../lib/template.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../../../../lib/diagrams.css" media="screen" type="text/css" rel="stylesheet" id="diagrams-css" />
      <script type="text/javascript">
         if(top === self) {
            var url = '../../../../../index.html';
            var hash = 'org.apache.hadoop.hbase.spark.JavaHBaseContext';
            var anchor = window.location.hash;
            var anchor_opt = '';
            if (anchor.length >= 1)
              anchor_opt = '@' + anchor.substring(1);
            window.location.href = url + '#' + hash + anchor_opt;
         }
   	  </script>
    
        </head>
        <body class="type">
      <div id="definition">
        <img src="../../../../../lib/class_big.png" />
        <p id="owner"><a href="../../../../package.html" class="extype" name="org">org</a>.<a href="../../../package.html" class="extype" name="org.apache">apache</a>.<a href="../../package.html" class="extype" name="org.apache.hadoop">hadoop</a>.<a href="../package.html" class="extype" name="org.apache.hadoop.hbase">hbase</a>.<a href="package.html" class="extype" name="org.apache.hadoop.hbase.spark">spark</a></p>
        <h1>JavaHBaseContext</h1>
      </div>

      <h4 id="signature" class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <span class="name">JavaHBaseContext</span><span class="result"> extends <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      </h4>
      
          <div id="comment" class="fullcommenttop"><div class="comment cmt"><p>This is the Java Wrapper over HBaseContext which is written in
Scala.  This class will be used by developers that want to
work with Spark or Spark Streaming in Java
</p></div><dl class="attributes block"> <dt>Annotations</dt><dd>
                <span class="name">@Public</span><span class="args">()</span>
              
        </dd></dl><div class="toggleContainer block">
          <span class="toggle">Linear Supertypes</span>
          <div class="superTypes hiddenContent"><span class="extype" name="scala.Serializable">Serializable</span>, <span class="extype" name="java.io.Serializable">Serializable</span>, <span class="extype" name="scala.AnyRef">AnyRef</span>, <span class="extype" name="scala.Any">Any</span></div>
        </div></div>
        

      <div id="mbrsel">
        <div id="textfilter"><span class="pre"></span><span class="input"><input id="mbrsel-input" type="text" accesskey="/" /></span><span class="post"></span></div>
        <div id="order">
              <span class="filtertype">Ordering</span>
              <ol>
                
                <li class="alpha in"><span>Alphabetic</span></li>
                <li class="inherit out"><span>By inheritance</span></li>
              </ol>
            </div>
        <div id="ancestors">
                <span class="filtertype">Inherited<br />
                </span>
                <ol id="linearization">
                  <li class="in" name="org.apache.hadoop.hbase.spark.JavaHBaseContext"><span>JavaHBaseContext</span></li><li class="in" name="scala.Serializable"><span>Serializable</span></li><li class="in" name="java.io.Serializable"><span>Serializable</span></li><li class="in" name="scala.AnyRef"><span>AnyRef</span></li><li class="in" name="scala.Any"><span>Any</span></li>
                </ol>
              </div><div id="ancestors">
            <span class="filtertype"></span>
            <ol>
              <li class="hideall out"><span>Hide All</span></li>
              <li class="showall in"><span>Show all</span></li>
            </ol>
            <a href="http://docs.scala-lang.org/overviews/scaladoc/usage.html#members" target="_blank">Learn more about member selection</a>
          </div>
        <div id="visbl">
            <span class="filtertype">Visibility</span>
            <ol><li class="public in"><span>Public</span></li><li class="all out"><span>All</span></li></ol>
          </div>
      </div>

      <div id="template">
        <div id="allMembers">
        <div id="constructors" class="members">
              <h3>Instance Constructors</h3>
              <ol><li name="org.apache.hadoop.hbase.spark.JavaHBaseContext#&lt;init&gt;" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="&lt;init&gt;(jsc:org.apache.spark.api.java.JavaSparkContext,config:org.apache.hadoop.conf.Configuration):org.apache.hadoop.hbase.spark.JavaHBaseContext"></a>
      <a id="&lt;init&gt;:JavaHBaseContext"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">new</span>
      </span>
      <span class="symbol">
        <span class="name">JavaHBaseContext</span><span class="params">(<span name="jsc">jsc: <span class="extype" name="org.apache.spark.api.java.JavaSparkContext">JavaSparkContext</span></span>, <span name="config">config: <span class="extype" name="org.apache.hadoop.conf.Configuration">Configuration</span></span>)</span>
      </span>
      </h4>
      <p class="shortcomment cmt"></p><div class="fullcomment"><div class="comment cmt"></div><dl class="paramcmts block"><dt class="param">jsc</dt><dd class="cmt"><p>This is the JavaSparkContext that we will wrap</p></dd><dt class="param">config</dt><dd class="cmt"><p>This is the config information to out HBase cluster
</p></dd></dl></div>
    </li></ol>
            </div>

        

        

        <div id="values" class="values members">
              <h3>Value Members</h3>
              <ol><li name="scala.AnyRef#!=" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="!=(x$1:AnyRef):Boolean"></a>
      <a id="!=(AnyRef):Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span title="gt4s: $bang$eq" class="name">!=</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.AnyRef">AnyRef</span></span>)</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd></dl></div>
    </li><li name="scala.Any#!=" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="!=(x$1:Any):Boolean"></a>
      <a id="!=(Any):Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span title="gt4s: $bang$eq" class="name">!=</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.Any">Any</span></span>)</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>Any</dd></dl></div>
    </li><li name="scala.AnyRef###" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="##():Int"></a>
      <a id="##():Int"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span title="gt4s: $hash$hash" class="name">##</span><span class="params">()</span><span class="result">: <span class="extype" name="scala.Int">Int</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div>
    </li><li name="scala.AnyRef#==" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="==(x$1:AnyRef):Boolean"></a>
      <a id="==(AnyRef):Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span title="gt4s: $eq$eq" class="name">==</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.AnyRef">AnyRef</span></span>)</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd></dl></div>
    </li><li name="scala.Any#==" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="==(x$1:Any):Boolean"></a>
      <a id="==(Any):Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span title="gt4s: $eq$eq" class="name">==</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.Any">Any</span></span>)</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>Any</dd></dl></div>
    </li><li name="scala.Any#asInstanceOf" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="asInstanceOf[T0]:T0"></a>
      <a id="asInstanceOf[T0]:T0"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">asInstanceOf</span><span class="tparams">[<span name="T0">T0</span>]</span><span class="result">: <span class="extype" name="scala.Any.asInstanceOf.T0">T0</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>Any</dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.JavaHBaseContext#bulkDelete" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="bulkDelete[T](javaRdd:org.apache.spark.api.java.JavaRDD[T],tableName:org.apache.hadoop.hbase.TableName,f:org.apache.spark.api.java.function.Function[T,org.apache.hadoop.hbase.client.Delete],batchSize:Integer):Unit"></a>
      <a id="bulkDelete[T](JavaRDD[T],TableName,Function[T,Delete],Integer):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">bulkDelete</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="javaRdd">javaRdd: <span class="extype" name="org.apache.spark.api.java.JavaRDD">JavaRDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.bulkDelete.T">T</span>]</span>, <span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="f">f: <span class="extype" name="org.apache.spark.api.java.function.Function">Function</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.bulkDelete.T">T</span>, <span class="extype" name="org.apache.hadoop.hbase.client.Delete">Delete</span>]</span>, <span name="batchSize">batchSize: <span class="extype" name="java.lang.Integer">Integer</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple abstraction over the HBaseContext.</p><div class="fullcomment"><div class="comment cmt"><p>A simple abstraction over the HBaseContext.foreachPartition method.</p><p>It allow addition support for a user to take a JavaRDD and
generate delete and send them to HBase.</p><p>The complexity of managing the Connection is
removed from the developer
</p></div><dl class="paramcmts block"><dt class="param">javaRdd</dt><dd class="cmt"><p>Original JavaRDD with data to iterate over</p></dd><dt class="param">tableName</dt><dd class="cmt"><p>The name of the table to delete from</p></dd><dt class="param">f</dt><dd class="cmt"><p>Function to convert a value in the JavaRDD to a
                 HBase Deletes</p></dd><dt class="param">batchSize</dt><dd class="cmt"><p>The number of deletes to batch before sending to HBase
</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.JavaHBaseContext#bulkGet" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="bulkGet[T,U](tableName:org.apache.hadoop.hbase.TableName,batchSize:Integer,javaRdd:org.apache.spark.api.java.JavaRDD[T],makeGet:org.apache.spark.api.java.function.Function[T,org.apache.hadoop.hbase.client.Get],convertResult:org.apache.spark.api.java.function.Function[org.apache.hadoop.hbase.client.Result,U]):org.apache.spark.api.java.JavaRDD[U]"></a>
      <a id="bulkGet[T,U](TableName,Integer,JavaRDD[T],Function[T,Get],Function[Result,U]):JavaRDD[U]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">bulkGet</span><span class="tparams">[<span name="T">T</span>, <span name="U">U</span>]</span><span class="params">(<span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="batchSize">batchSize: <span class="extype" name="java.lang.Integer">Integer</span></span>, <span name="javaRdd">javaRdd: <span class="extype" name="org.apache.spark.api.java.JavaRDD">JavaRDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.bulkGet.T">T</span>]</span>, <span name="makeGet">makeGet: <span class="extype" name="org.apache.spark.api.java.function.Function">Function</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.bulkGet.T">T</span>, <span class="extype" name="org.apache.hadoop.hbase.client.Get">Get</span>]</span>, <span name="convertResult">convertResult: <span class="extype" name="org.apache.spark.api.java.function.Function">Function</span>[<span class="extype" name="org.apache.hadoop.hbase.client.Result">Result</span>, <span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.bulkGet.U">U</span>]</span>)</span><span class="result">: <span class="extype" name="org.apache.spark.api.java.JavaRDD">JavaRDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.bulkGet.U">U</span>]</span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple abstraction over the HBaseContext.</p><div class="fullcomment"><div class="comment cmt"><p>A simple abstraction over the HBaseContext.mapPartition method.</p><p>It allow addition support for a user to take a JavaRDD and generates a
new RDD based on Gets and the results they bring back from HBase
</p></div><dl class="paramcmts block"><dt class="param">tableName</dt><dd class="cmt"><p>The name of the table to get from</p></dd><dt class="param">batchSize</dt><dd class="cmt"><p>batch size of how many gets to retrieve in a single fetch</p></dd><dt class="param">javaRdd</dt><dd class="cmt"><p>Original JavaRDD with data to iterate over</p></dd><dt class="param">makeGet</dt><dd class="cmt"><p>Function to convert a value in the JavaRDD to a
                     HBase Get</p></dd><dt class="param">convertResult</dt><dd class="cmt"><p>This will convert the HBase Result object to
                     what ever the user wants to put in the resulting
                     JavaRDD</p></dd><dt>returns</dt><dd class="cmt"><p>New JavaRDD that is created by the Get to HBase
</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.JavaHBaseContext#bulkLoad" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="bulkLoad[T](javaRdd:org.apache.spark.api.java.JavaRDD[T],tableName:org.apache.hadoop.hbase.TableName,mapFunc:org.apache.spark.api.java.function.Function[T,org.apache.hadoop.hbase.util.Pair[org.apache.hadoop.hbase.spark.KeyFamilyQualifier,Array[Byte]]],stagingDir:String,familyHFileWriteOptionsMap:java.util.Map[Array[Byte],org.apache.hadoop.hbase.spark.FamilyHFileWriteOptions],compactionExclude:Boolean,maxSize:Long):Unit"></a>
      <a id="bulkLoad[T](JavaRDD[T],TableName,Function[T,Pair[KeyFamilyQualifier,Array[Byte]]],String,Map[Array[Byte],FamilyHFileWriteOptions],Boolean,Long):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">bulkLoad</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="javaRdd">javaRdd: <span class="extype" name="org.apache.spark.api.java.JavaRDD">JavaRDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.bulkLoad.T">T</span>]</span>, <span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="mapFunc">mapFunc: <span class="extype" name="org.apache.spark.api.java.function.Function">Function</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.bulkLoad.T">T</span>, <span class="extype" name="org.apache.hadoop.hbase.util.Pair">Pair</span>[<a href="KeyFamilyQualifier.html" class="extype" name="org.apache.hadoop.hbase.spark.KeyFamilyQualifier">KeyFamilyQualifier</a>, <span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Byte">Byte</span>]]]</span>, <span name="stagingDir">stagingDir: <span class="extype" name="scala.Predef.String">String</span></span>, <span name="familyHFileWriteOptionsMap">familyHFileWriteOptionsMap: <span class="extype" name="java.util.Map">Map</span>[<span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Byte">Byte</span>], <a href="FamilyHFileWriteOptions.html" class="extype" name="org.apache.hadoop.hbase.spark.FamilyHFileWriteOptions">FamilyHFileWriteOptions</a>]</span>, <span name="compactionExclude">compactionExclude: <span class="extype" name="scala.Boolean">Boolean</span></span>, <span name="maxSize">maxSize: <span class="extype" name="scala.Long">Long</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple abstraction over the HBaseContext.</p><div class="fullcomment"><div class="comment cmt"><p>A simple abstraction over the HBaseContext.bulkLoad method.
It allow addition support for a user to take a JavaRDD and
convert into new JavaRDD[Pair] based on MapFunction,
and HFiles will be generated in stagingDir for bulk load
</p></div><dl class="paramcmts block"><dt class="param">javaRdd</dt><dd class="cmt"><p>The javaRDD we are bulk loading from</p></dd><dt class="param">tableName</dt><dd class="cmt"><p>The HBase table we are loading into</p></dd><dt class="param">mapFunc</dt><dd class="cmt"><p>A Function that will convert a value in JavaRDD
                                      to Pair(KeyFamilyQualifier, Array[Byte])</p></dd><dt class="param">stagingDir</dt><dd class="cmt"><p>The location on the FileSystem to bulk load into</p></dd><dt class="param">familyHFileWriteOptionsMap</dt><dd class="cmt"><p>Options that will define how the HFile for a
                                      column family is written</p></dd><dt class="param">compactionExclude</dt><dd class="cmt"><p>Compaction excluded for the HFiles</p></dd><dt class="param">maxSize</dt><dd class="cmt"><p>Max size for the HFiles before they roll
</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.JavaHBaseContext#bulkLoadThinRows" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="bulkLoadThinRows[T](javaRdd:org.apache.spark.api.java.JavaRDD[T],tableName:org.apache.hadoop.hbase.TableName,mapFunc:org.apache.spark.api.java.function.Function[T,org.apache.hadoop.hbase.util.Pair[org.apache.hadoop.hbase.spark.ByteArrayWrapper,org.apache.hadoop.hbase.spark.FamiliesQualifiersValues]],stagingDir:String,familyHFileWriteOptionsMap:java.util.Map[Array[Byte],org.apache.hadoop.hbase.spark.FamilyHFileWriteOptions],compactionExclude:Boolean,maxSize:Long):Unit"></a>
      <a id="bulkLoadThinRows[T](JavaRDD[T],TableName,Function[T,Pair[ByteArrayWrapper,FamiliesQualifiersValues]],String,Map[Array[Byte],FamilyHFileWriteOptions],Boolean,Long):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">bulkLoadThinRows</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="javaRdd">javaRdd: <span class="extype" name="org.apache.spark.api.java.JavaRDD">JavaRDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.bulkLoadThinRows.T">T</span>]</span>, <span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="mapFunc">mapFunc: <span class="extype" name="org.apache.spark.api.java.function.Function">Function</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.bulkLoadThinRows.T">T</span>, <span class="extype" name="org.apache.hadoop.hbase.util.Pair">Pair</span>[<a href="ByteArrayWrapper.html" class="extype" name="org.apache.hadoop.hbase.spark.ByteArrayWrapper">ByteArrayWrapper</a>, <a href="FamiliesQualifiersValues.html" class="extype" name="org.apache.hadoop.hbase.spark.FamiliesQualifiersValues">FamiliesQualifiersValues</a>]]</span>, <span name="stagingDir">stagingDir: <span class="extype" name="scala.Predef.String">String</span></span>, <span name="familyHFileWriteOptionsMap">familyHFileWriteOptionsMap: <span class="extype" name="java.util.Map">Map</span>[<span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Byte">Byte</span>], <a href="FamilyHFileWriteOptions.html" class="extype" name="org.apache.hadoop.hbase.spark.FamilyHFileWriteOptions">FamilyHFileWriteOptions</a>]</span>, <span name="compactionExclude">compactionExclude: <span class="extype" name="scala.Boolean">Boolean</span></span>, <span name="maxSize">maxSize: <span class="extype" name="scala.Long">Long</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple abstraction over the HBaseContext.</p><div class="fullcomment"><div class="comment cmt"><p>A simple abstraction over the HBaseContext.bulkLoadThinRows method.
It allow addition support for a user to take a JavaRDD and
convert into new JavaRDD[Pair] based on MapFunction,
and HFiles will be generated in stagingDir for bulk load
</p></div><dl class="paramcmts block"><dt class="param">javaRdd</dt><dd class="cmt"><p>The javaRDD we are bulk loading from</p></dd><dt class="param">tableName</dt><dd class="cmt"><p>The HBase table we are loading into</p></dd><dt class="param">mapFunc</dt><dd class="cmt"><p>A Function that will convert a value in JavaRDD
                                      to Pair(ByteArrayWrapper, FamiliesQualifiersValues)</p></dd><dt class="param">stagingDir</dt><dd class="cmt"><p>The location on the FileSystem to bulk load into</p></dd><dt class="param">familyHFileWriteOptionsMap</dt><dd class="cmt"><p>Options that will define how the HFile for a
                                      column family is written</p></dd><dt class="param">compactionExclude</dt><dd class="cmt"><p>Compaction excluded for the HFiles</p></dd><dt class="param">maxSize</dt><dd class="cmt"><p>Max size for the HFiles before they roll
</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.JavaHBaseContext#bulkPut" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="bulkPut[T](javaRdd:org.apache.spark.api.java.JavaRDD[T],tableName:org.apache.hadoop.hbase.TableName,f:org.apache.spark.api.java.function.Function[T,org.apache.hadoop.hbase.client.Put]):Unit"></a>
      <a id="bulkPut[T](JavaRDD[T],TableName,Function[T,Put]):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">bulkPut</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="javaRdd">javaRdd: <span class="extype" name="org.apache.spark.api.java.JavaRDD">JavaRDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.bulkPut.T">T</span>]</span>, <span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="f">f: <span class="extype" name="org.apache.spark.api.java.function.Function">Function</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.bulkPut.T">T</span>, <span class="extype" name="org.apache.hadoop.hbase.client.Put">Put</span>]</span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple abstraction over the HBaseContext.</p><div class="fullcomment"><div class="comment cmt"><p>A simple abstraction over the HBaseContext.foreachPartition method.</p><p>It allow addition support for a user to take JavaRDD
and generate puts and send them to HBase.
The complexity of managing the Connection is
removed from the developer
</p></div><dl class="paramcmts block"><dt class="param">javaRdd</dt><dd class="cmt"><p>Original JavaRDD with data to iterate over</p></dd><dt class="param">tableName</dt><dd class="cmt"><p>The name of the table to put into</p></dd><dt class="param">f</dt><dd class="cmt"><p>Function to convert a value in the JavaRDD
                 to a HBase Put
</p></dd></dl></div>
    </li><li name="scala.AnyRef#clone" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="clone():Object"></a>
      <a id="clone():AnyRef"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">clone</span><span class="params">()</span><span class="result">: <span class="extype" name="scala.AnyRef">AnyRef</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected[<a href="../../../../../java$lang.html" class="extype" name="java.lang">java.lang</a>] </dd><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd>
                <span class="name">@throws</span><span class="args">(<span>
      
      <span class="defval" name="classOf[java.lang.CloneNotSupportedException]">...</span>
    </span>)</span>
              
        </dd></dl></div>
    </li><li name="scala.AnyRef#eq" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="eq(x$1:AnyRef):Boolean"></a>
      <a id="eq(AnyRef):Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">eq</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.AnyRef">AnyRef</span></span>)</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd></dl></div>
    </li><li name="scala.AnyRef#equals" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="equals(x$1:Any):Boolean"></a>
      <a id="equals(Any):Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">equals</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.Any">Any</span></span>)</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div>
    </li><li name="scala.AnyRef#finalize" visbl="prt" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="finalize():Unit"></a>
      <a id="finalize():Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">finalize</span><span class="params">()</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Attributes</dt><dd>protected[<a href="../../../../../java$lang.html" class="extype" name="java.lang">java.lang</a>] </dd><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd>
                <span class="name">@throws</span><span class="args">(<span>
      
      <span class="symbol">classOf[java.lang.Throwable]</span>
    </span>)</span>
              
        </dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.JavaHBaseContext#foreachPartition" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="foreachPartition[T](javaDstream:org.apache.spark.streaming.api.java.JavaDStream[T],f:org.apache.spark.api.java.function.VoidFunction[(Iterator[T],org.apache.hadoop.hbase.client.Connection)]):Unit"></a>
      <a id="foreachPartition[T](JavaDStream[T],VoidFunction[(Iterator[T],Connection)]):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">foreachPartition</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="javaDstream">javaDstream: <span class="extype" name="org.apache.spark.streaming.api.java.JavaDStream">JavaDStream</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.foreachPartition.T">T</span>]</span>, <span name="f">f: <span class="extype" name="org.apache.spark.api.java.function.VoidFunction">VoidFunction</span>[(<span class="extype" name="scala.Iterator">Iterator</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.foreachPartition.T">T</span>], <span class="extype" name="org.apache.hadoop.hbase.client.Connection">Connection</span>)]</span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple enrichment of the traditional Spark Streaming dStream foreach
This function differs from the original in that it offers the
developer access to a already connected Connection object</p><div class="fullcomment"><div class="comment cmt"><p>A simple enrichment of the traditional Spark Streaming dStream foreach
This function differs from the original in that it offers the
developer access to a already connected Connection object</p><p>Note: Do not close the Connection object.  All Connection
management is handled outside this method
</p></div><dl class="paramcmts block"><dt class="param">javaDstream</dt><dd class="cmt"><p>Original DStream with data to iterate over</p></dd><dt class="param">f</dt><dd class="cmt"><p>Function to be given a iterator to iterate through
                   the JavaDStream values and a Connection object to
                   interact with HBase
</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.JavaHBaseContext#foreachPartition" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="foreachPartition[T](javaRdd:org.apache.spark.api.java.JavaRDD[T],f:org.apache.spark.api.java.function.VoidFunction[(java.util.Iterator[T],org.apache.hadoop.hbase.client.Connection)]):Unit"></a>
      <a id="foreachPartition[T](JavaRDD[T],VoidFunction[(Iterator[T],Connection)]):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">foreachPartition</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="javaRdd">javaRdd: <span class="extype" name="org.apache.spark.api.java.JavaRDD">JavaRDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.foreachPartition.T">T</span>]</span>, <span name="f">f: <span class="extype" name="org.apache.spark.api.java.function.VoidFunction">VoidFunction</span>[(<span class="extype" name="java.util.Iterator">Iterator</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.foreachPartition.T">T</span>], <span class="extype" name="org.apache.hadoop.hbase.client.Connection">Connection</span>)]</span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple enrichment of the traditional Spark javaRdd foreachPartition.</p><div class="fullcomment"><div class="comment cmt"><p>A simple enrichment of the traditional Spark javaRdd foreachPartition.
This function differs from the original in that it offers the
developer access to a already connected Connection object</p><p>Note: Do not close the Connection object.  All Connection
management is handled outside this method
</p></div><dl class="paramcmts block"><dt class="param">javaRdd</dt><dd class="cmt"><p>Original javaRdd with data to iterate over</p></dd><dt class="param">f</dt><dd class="cmt"><p>Function to be given a iterator to iterate through
               the RDD values and a Connection object to interact
               with HBase
</p></dd></dl></div>
    </li><li name="scala.AnyRef#getClass" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="getClass():Class[_]"></a>
      <a id="getClass():Class[_]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">getClass</span><span class="params">()</span><span class="result">: <span class="extype" name="java.lang.Class">Class</span>[_]</span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div>
    </li><li name="scala.AnyRef#hashCode" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="hashCode():Int"></a>
      <a id="hashCode():Int"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">hashCode</span><span class="params">()</span><span class="result">: <span class="extype" name="scala.Int">Int</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.JavaHBaseContext#hbaseContext" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="hbaseContext:org.apache.hadoop.hbase.spark.HBaseContext"></a>
      <a id="hbaseContext:HBaseContext"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">val</span>
      </span>
      <span class="symbol">
        <span class="name">hbaseContext</span><span class="result">: <a href="HBaseContext.html" class="extype" name="org.apache.hadoop.hbase.spark.HBaseContext">HBaseContext</a></span>
      </span>
      </h4>
      
    </li><li name="org.apache.hadoop.hbase.spark.JavaHBaseContext#hbaseRDD" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="hbaseRDD(tableName:org.apache.hadoop.hbase.TableName,scans:org.apache.hadoop.hbase.client.Scan):org.apache.spark.api.java.JavaRDD[(org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.hbase.client.Result)]"></a>
      <a id="hbaseRDD(TableName,Scan):JavaRDD[(ImmutableBytesWritable,Result)]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">hbaseRDD</span><span class="params">(<span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="scans">scans: <span class="extype" name="org.apache.hadoop.hbase.client.Scan">Scan</span></span>)</span><span class="result">: <span class="extype" name="org.apache.spark.api.java.JavaRDD">JavaRDD</span>[(<span class="extype" name="org.apache.hadoop.hbase.io.ImmutableBytesWritable">ImmutableBytesWritable</span>, <span class="extype" name="org.apache.hadoop.hbase.client.Result">Result</span>)]</span>
      </span>
      </h4>
      <p class="shortcomment cmt">A overloaded version of HBaseContext hbaseRDD that define the
type of the resulting JavaRDD
</p><div class="fullcomment"><div class="comment cmt"><p>A overloaded version of HBaseContext hbaseRDD that define the
type of the resulting JavaRDD
</p></div><dl class="paramcmts block"><dt class="param">tableName</dt><dd class="cmt"><p>The name of the table to scan</p></dd><dt class="param">scans</dt><dd class="cmt"><p>The HBase scan object to use to read data from HBase</p></dd><dt>returns</dt><dd class="cmt"><p>New JavaRDD with results from scan
</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.JavaHBaseContext#hbaseRDD" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="hbaseRDD[U](tableName:org.apache.hadoop.hbase.TableName,scans:org.apache.hadoop.hbase.client.Scan,f:org.apache.spark.api.java.function.Function[(org.apache.hadoop.hbase.io.ImmutableBytesWritable,org.apache.hadoop.hbase.client.Result),U]):org.apache.spark.api.java.JavaRDD[U]"></a>
      <a id="hbaseRDD[U](TableName,Scan,Function[(ImmutableBytesWritable,Result),U]):JavaRDD[U]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">hbaseRDD</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="scans">scans: <span class="extype" name="org.apache.hadoop.hbase.client.Scan">Scan</span></span>, <span name="f">f: <span class="extype" name="org.apache.spark.api.java.function.Function">Function</span>[(<span class="extype" name="org.apache.hadoop.hbase.io.ImmutableBytesWritable">ImmutableBytesWritable</span>, <span class="extype" name="org.apache.hadoop.hbase.client.Result">Result</span>), <span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.hbaseRDD.U">U</span>]</span>)</span><span class="result">: <span class="extype" name="org.apache.spark.api.java.JavaRDD">JavaRDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.hbaseRDD.U">U</span>]</span>
      </span>
      </h4>
      <p class="shortcomment cmt">This function will use the native HBase TableInputFormat with the
given scan object to generate a new JavaRDD
</p><div class="fullcomment"><div class="comment cmt"><p>This function will use the native HBase TableInputFormat with the
given scan object to generate a new JavaRDD
</p></div><dl class="paramcmts block"><dt class="param">tableName</dt><dd class="cmt"><p>The name of the table to scan</p></dd><dt class="param">scans</dt><dd class="cmt"><p>The HBase scan object to use to read data from HBase</p></dd><dt class="param">f</dt><dd class="cmt"><p>Function to convert a Result object from HBase into
                 What the user wants in the final generated JavaRDD</p></dd><dt>returns</dt><dd class="cmt"><p>New JavaRDD with results from scan
</p></dd></dl></div>
    </li><li name="scala.Any#isInstanceOf" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="isInstanceOf[T0]:Boolean"></a>
      <a id="isInstanceOf[T0]:Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">isInstanceOf</span><span class="tparams">[<span name="T0">T0</span>]</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>Any</dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.JavaHBaseContext#mapPartitions" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="mapPartitions[T,R](javaRdd:org.apache.spark.api.java.JavaRDD[T],f:org.apache.spark.api.java.function.FlatMapFunction[(java.util.Iterator[T],org.apache.hadoop.hbase.client.Connection),R]):org.apache.spark.api.java.JavaRDD[R]"></a>
      <a id="mapPartitions[T,R](JavaRDD[T],FlatMapFunction[(Iterator[T],Connection),R]):JavaRDD[R]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">mapPartitions</span><span class="tparams">[<span name="T">T</span>, <span name="R">R</span>]</span><span class="params">(<span name="javaRdd">javaRdd: <span class="extype" name="org.apache.spark.api.java.JavaRDD">JavaRDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.mapPartitions.T">T</span>]</span>, <span name="f">f: <span class="extype" name="org.apache.spark.api.java.function.FlatMapFunction">FlatMapFunction</span>[(<span class="extype" name="java.util.Iterator">Iterator</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.mapPartitions.T">T</span>], <span class="extype" name="org.apache.hadoop.hbase.client.Connection">Connection</span>), <span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.mapPartitions.R">R</span>]</span>)</span><span class="result">: <span class="extype" name="org.apache.spark.api.java.JavaRDD">JavaRDD</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.mapPartitions.R">R</span>]</span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple enrichment of the traditional Spark JavaRDD mapPartition.</p><div class="fullcomment"><div class="comment cmt"><p>A simple enrichment of the traditional Spark JavaRDD mapPartition.
This function differs from the original in that it offers the
developer access to a already connected Connection object</p><p>Note: Do not close the Connection object.  All Connection
management is handled outside this method</p><p>Note: Make sure to partition correctly to avoid memory issue when
getting data from HBase
</p></div><dl class="paramcmts block"><dt class="param">javaRdd</dt><dd class="cmt"><p>Original JavaRdd with data to iterate over</p></dd><dt class="param">f</dt><dd class="cmt"><p>Function to be given a iterator to iterate through
               the RDD values and a Connection object to interact
               with HBase</p></dd><dt>returns</dt><dd class="cmt"><p>Returns a new RDD generated by the user definition
               function just like normal mapPartition
</p></dd></dl></div>
    </li><li name="scala.AnyRef#ne" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ne(x$1:AnyRef):Boolean"></a>
      <a id="ne(AnyRef):Boolean"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">ne</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.AnyRef">AnyRef</span></span>)</span><span class="result">: <span class="extype" name="scala.Boolean">Boolean</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd></dl></div>
    </li><li name="scala.AnyRef#notify" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="notify():Unit"></a>
      <a id="notify():Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">notify</span><span class="params">()</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd></dl></div>
    </li><li name="scala.AnyRef#notifyAll" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="notifyAll():Unit"></a>
      <a id="notifyAll():Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">notifyAll</span><span class="params">()</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.JavaHBaseContext#streamBulkDelete" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="streamBulkDelete[T](javaDStream:org.apache.spark.streaming.api.java.JavaDStream[T],tableName:org.apache.hadoop.hbase.TableName,f:org.apache.spark.api.java.function.Function[T,org.apache.hadoop.hbase.client.Delete],batchSize:Integer):Unit"></a>
      <a id="streamBulkDelete[T](JavaDStream[T],TableName,Function[T,Delete],Integer):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">streamBulkDelete</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="javaDStream">javaDStream: <span class="extype" name="org.apache.spark.streaming.api.java.JavaDStream">JavaDStream</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.streamBulkDelete.T">T</span>]</span>, <span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="f">f: <span class="extype" name="org.apache.spark.api.java.function.Function">Function</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.streamBulkDelete.T">T</span>, <span class="extype" name="org.apache.hadoop.hbase.client.Delete">Delete</span>]</span>, <span name="batchSize">batchSize: <span class="extype" name="java.lang.Integer">Integer</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple abstraction over the HBaseContext.</p><div class="fullcomment"><div class="comment cmt"><p>A simple abstraction over the HBaseContext.streamBulkMutation method.</p><p>It allow addition support for a user to take a JavaDStream and
generate Delete and send them to HBase.</p><p>The complexity of managing the Connection is
removed from the developer
</p></div><dl class="paramcmts block"><dt class="param">javaDStream</dt><dd class="cmt"><p>Original DStream with data to iterate over</p></dd><dt class="param">tableName</dt><dd class="cmt"><p>The name of the table to delete from</p></dd><dt class="param">f</dt><dd class="cmt"><p>Function to convert a value in the JavaDStream to a
                   HBase Delete</p></dd><dt class="param">batchSize</dt><dd class="cmt"><p>The number of deletes to be sent at once
</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.JavaHBaseContext#streamBulkGet" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="streamBulkGet[T,U](tableName:org.apache.hadoop.hbase.TableName,batchSize:Integer,javaDStream:org.apache.spark.streaming.api.java.JavaDStream[T],makeGet:org.apache.spark.api.java.function.Function[T,org.apache.hadoop.hbase.client.Get],convertResult:org.apache.spark.api.java.function.Function[org.apache.hadoop.hbase.client.Result,U]):org.apache.spark.streaming.api.java.JavaDStream[U]"></a>
      <a id="streamBulkGet[T,U](TableName,Integer,JavaDStream[T],Function[T,Get],Function[Result,U]):JavaDStream[U]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">streamBulkGet</span><span class="tparams">[<span name="T">T</span>, <span name="U">U</span>]</span><span class="params">(<span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="batchSize">batchSize: <span class="extype" name="java.lang.Integer">Integer</span></span>, <span name="javaDStream">javaDStream: <span class="extype" name="org.apache.spark.streaming.api.java.JavaDStream">JavaDStream</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.streamBulkGet.T">T</span>]</span>, <span name="makeGet">makeGet: <span class="extype" name="org.apache.spark.api.java.function.Function">Function</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.streamBulkGet.T">T</span>, <span class="extype" name="org.apache.hadoop.hbase.client.Get">Get</span>]</span>, <span name="convertResult">convertResult: <span class="extype" name="org.apache.spark.api.java.function.Function">Function</span>[<span class="extype" name="org.apache.hadoop.hbase.client.Result">Result</span>, <span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.streamBulkGet.U">U</span>]</span>)</span><span class="result">: <span class="extype" name="org.apache.spark.streaming.api.java.JavaDStream">JavaDStream</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.streamBulkGet.U">U</span>]</span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple abstraction over the HBaseContext.</p><div class="fullcomment"><div class="comment cmt"><p>A simple abstraction over the HBaseContext.streamMap method.</p><p>It allow addition support for a user to take a DStream and
generates a new DStream based on Gets and the results
they bring back from HBase
</p></div><dl class="paramcmts block"><dt class="param">tableName</dt><dd class="cmt"><p>The name of the table to get from</p></dd><dt class="param">batchSize</dt><dd class="cmt"><p>The number of gets to be batched together</p></dd><dt class="param">javaDStream</dt><dd class="cmt"><p>Original DStream with data to iterate over</p></dd><dt class="param">makeGet</dt><dd class="cmt"><p>Function to convert a value in the JavaDStream to a
                     HBase Get</p></dd><dt class="param">convertResult</dt><dd class="cmt"><p>This will convert the HBase Result object to
                     what ever the user wants to put in the resulting
                     JavaDStream</p></dd><dt>returns</dt><dd class="cmt"><p>New JavaDStream that is created by the Get to HBase
</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.JavaHBaseContext#streamBulkPut" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="streamBulkPut[T](javaDstream:org.apache.spark.streaming.api.java.JavaDStream[T],tableName:org.apache.hadoop.hbase.TableName,f:org.apache.spark.api.java.function.Function[T,org.apache.hadoop.hbase.client.Put]):Unit"></a>
      <a id="streamBulkPut[T](JavaDStream[T],TableName,Function[T,Put]):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">streamBulkPut</span><span class="tparams">[<span name="T">T</span>]</span><span class="params">(<span name="javaDstream">javaDstream: <span class="extype" name="org.apache.spark.streaming.api.java.JavaDStream">JavaDStream</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.streamBulkPut.T">T</span>]</span>, <span name="tableName">tableName: <span class="extype" name="org.apache.hadoop.hbase.TableName">TableName</span></span>, <span name="f">f: <span class="extype" name="org.apache.spark.api.java.function.Function">Function</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.streamBulkPut.T">T</span>, <span class="extype" name="org.apache.hadoop.hbase.client.Put">Put</span>]</span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple abstraction over the HBaseContext.</p><div class="fullcomment"><div class="comment cmt"><p>A simple abstraction over the HBaseContext.streamMapPartition method.</p><p>It allow addition support for a user to take a JavaDStream and
generate puts and send them to HBase.</p><p>The complexity of managing the Connection is
removed from the developer
</p></div><dl class="paramcmts block"><dt class="param">javaDstream</dt><dd class="cmt"><p>Original DStream with data to iterate over</p></dd><dt class="param">tableName</dt><dd class="cmt"><p>The name of the table to put into</p></dd><dt class="param">f</dt><dd class="cmt"><p>Function to convert a value in
                   the JavaDStream to a HBase Put
</p></dd></dl></div>
    </li><li name="org.apache.hadoop.hbase.spark.JavaHBaseContext#streamMap" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="streamMap[T,U](javaDstream:org.apache.spark.streaming.api.java.JavaDStream[T],mp:org.apache.spark.api.java.function.Function[(Iterator[T],org.apache.hadoop.hbase.client.Connection),Iterator[U]]):org.apache.spark.streaming.api.java.JavaDStream[U]"></a>
      <a id="streamMap[T,U](JavaDStream[T],Function[(Iterator[T],Connection),Iterator[U]]):JavaDStream[U]"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">streamMap</span><span class="tparams">[<span name="T">T</span>, <span name="U">U</span>]</span><span class="params">(<span name="javaDstream">javaDstream: <span class="extype" name="org.apache.spark.streaming.api.java.JavaDStream">JavaDStream</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.streamMap.T">T</span>]</span>, <span name="mp">mp: <span class="extype" name="org.apache.spark.api.java.function.Function">Function</span>[(<span class="extype" name="scala.Iterator">Iterator</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.streamMap.T">T</span>], <span class="extype" name="org.apache.hadoop.hbase.client.Connection">Connection</span>), <span class="extype" name="scala.Iterator">Iterator</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.streamMap.U">U</span>]]</span>)</span><span class="result">: <span class="extype" name="org.apache.spark.streaming.api.java.JavaDStream">JavaDStream</span>[<span class="extype" name="org.apache.hadoop.hbase.spark.JavaHBaseContext.streamMap.U">U</span>]</span>
      </span>
      </h4>
      <p class="shortcomment cmt">A simple enrichment of the traditional Spark Streaming JavaDStream
mapPartition.</p><div class="fullcomment"><div class="comment cmt"><p>A simple enrichment of the traditional Spark Streaming JavaDStream
mapPartition.</p><p>This function differs from the original in that it offers the
developer access to a already connected Connection object</p><p>Note: Do not close the Connection object.  All Connection
management is handled outside this method</p><p>Note: Make sure to partition correctly to avoid memory issue when
getting data from HBase
</p></div><dl class="paramcmts block"><dt class="param">javaDstream</dt><dd class="cmt"><p>Original JavaDStream with data to iterate over</p></dd><dt class="param">mp</dt><dd class="cmt"><p>Function to be given a iterator to iterate through
                   the JavaDStream values and a Connection object to
                   interact with HBase</p></dd><dt>returns</dt><dd class="cmt"><p>Returns a new JavaDStream generated by the user
                   definition function just like normal mapPartition
</p></dd></dl></div>
    </li><li name="scala.AnyRef#synchronized" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="synchronized[T0](x$1:=&gt;T0):T0"></a>
      <a id="synchronized[T0](⇒T0):T0"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">synchronized</span><span class="tparams">[<span name="T0">T0</span>]</span><span class="params">(<span name="arg0">arg0: ⇒ <span class="extype" name="java.lang.AnyRef.synchronized.T0">T0</span></span>)</span><span class="result">: <span class="extype" name="java.lang.AnyRef.synchronized.T0">T0</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd></dl></div>
    </li><li name="scala.AnyRef#toString" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="toString():String"></a>
      <a id="toString():String"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">toString</span><span class="params">()</span><span class="result">: <span class="extype" name="java.lang.String">String</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div>
    </li><li name="scala.AnyRef#wait" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="wait():Unit"></a>
      <a id="wait():Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">wait</span><span class="params">()</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd>
                <span class="name">@throws</span><span class="args">(<span>
      
      <span class="defval" name="classOf[java.lang.InterruptedException]">...</span>
    </span>)</span>
              
        </dd></dl></div>
    </li><li name="scala.AnyRef#wait" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="wait(x$1:Long,x$2:Int):Unit"></a>
      <a id="wait(Long,Int):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">wait</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.Long">Long</span></span>, <span name="arg1">arg1: <span class="extype" name="scala.Int">Int</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd>
                <span class="name">@throws</span><span class="args">(<span>
      
      <span class="defval" name="classOf[java.lang.InterruptedException]">...</span>
    </span>)</span>
              
        </dd></dl></div>
    </li><li name="scala.AnyRef#wait" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="wait(x$1:Long):Unit"></a>
      <a id="wait(Long):Unit"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier">final </span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">wait</span><span class="params">(<span name="arg0">arg0: <span class="extype" name="scala.Long">Long</span></span>)</span><span class="result">: <span class="extype" name="scala.Unit">Unit</span></span>
      </span>
      </h4>
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd>
                <span class="name">@throws</span><span class="args">(<span>
      
      <span class="defval" name="classOf[java.lang.InterruptedException]">...</span>
    </span>)</span>
              
        </dd></dl></div>
    </li></ol>
            </div>

        

        
        </div>

        <div id="inheritedMembers">
        <div class="parent" name="scala.Serializable">
              <h3>Inherited from <span class="extype" name="scala.Serializable">Serializable</span></h3>
            </div><div class="parent" name="java.io.Serializable">
              <h3>Inherited from <span class="extype" name="java.io.Serializable">Serializable</span></h3>
            </div><div class="parent" name="scala.AnyRef">
              <h3>Inherited from <span class="extype" name="scala.AnyRef">AnyRef</span></h3>
            </div><div class="parent" name="scala.Any">
              <h3>Inherited from <span class="extype" name="scala.Any">Any</span></h3>
            </div>
        
        </div>

        <div id="groupedMembers">
        <div class="group" name="Ungrouped">
              <h3>Ungrouped</h3>
              
            </div>
        </div>

      </div>

      <div id="tooltip"></div>

      <div id="footer">  </div>
      <script defer="defer" type="text/javascript" id="jquery-js" src="../../../../../lib/jquery.js"></script><script defer="defer" type="text/javascript" id="jquery-ui-js" src="../../../../../lib/jquery-ui.js"></script><script defer="defer" type="text/javascript" id="tools-tooltip-js" src="../../../../../lib/tools.tooltip.js"></script><script defer="defer" type="text/javascript" id="template-js" src="../../../../../lib/template.js"></script>
    </body>
      </html>